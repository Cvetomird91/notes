=======================================================================================================
-- AWS Cloud API and CLI / -- CLI / -- AWS CLI / -- AWS API / -- API

Cli options:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q62-i1.jpg +
 via - https://docs.aws.amazon.com/cli/latest/topic/config-vars.html#general-options +

 The --dry-run option checks whether you have the required permissions for the action, without actually making the request, and provides an error response. If you have the required permissions, the error response is DryRunOperation, otherwise, it is UnauthorizedOperation.

https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html
https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html

aws ec2 monitor-instances --instance-ids i-1234567890abcdef0 - This enables detailed monitoring for a running instance.
aws ec2 run-instances --image-id ami-09092360 --monitoring Enabled=true - This syntax is used to enable detailed monitoring when launching an instance from AWS CLI.
https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html

EC2 detailed monitoring:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q19-i1.jpg
via - https://docs.aws.amazon.com/cli/latest/reference/ec2/monitor-instances.html
 
-- Trusted Advisor / -- AWS Trusted Advisor / -- Amazon Trusted Advisor
-- AWS Cost Explorer

-- AWS Budgets

AWS Budgets lets customers set custom budgets and receive alerts if their costs or usage exceed (or are forecasted to exceed) their budgeted amount.

AWS requires approximately 5 weeks of usage data to generate budget forecasts - AWS requires approximately 5 weeks of usage data to generate budget forecasts. If you set a budget to alert based on a forecasted amount, this budget alert isn't triggered until you have enough historical usage information.
 If the user account does not have enough privileges, the user will not be able to create the budget at all.
 Stand-alone accounts too can create budgets and being part of an Organization is not mandatory to use AWS Budgets.

 Budget best practices:
 https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-best-practices.html +

If your application creates an AWS client using the default constructor, then the client will search for credentials using the default credentials provider chain, in the following order:
1. In the Java system properties: aws.accessKeyId and aws.secretKey.
2. In system environment variables: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
3. In the default credentials file (the location of this file varies by platform).
4. Credentials delivered through the Amazon EC2 container service if the AWS_CONTAINER_CREDENTIALS_RELATIVE_URI environment variable is set and security manager has permission to access the variable.
5. In the instance profile credentials, which exist within the instance metadata associated with the IAM role for the EC2 instance.
6. Web Identity Token credentials from the environment or container.

The default provider chain and EC2 instance profiles:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q36-i1.jpg +
via - https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-roles.html +

=======================================================================================================
-- IAM and access control

IAM Access Analyzer - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.
You can set the scope for the analyzer to an organization or an AWS account. This is your zone of trust. The analyzer scans all of the supported resources within your zone of trust. When Access Analyzer finds a policy that allows access to a resource from outside of your zone of trust, it generates an active finding.

The following policy types only limit permissions but cannot grant permissions:
AWS Organizations Service Control Policy (SCP) – Use an AWS Organizations Service Control Policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.

Permissions boundary - Permissions boundary is a managed policy that is used for an IAM entity (user or role). The policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions.

Overview of Policy Types:  
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q59-i1.jpg +
via - https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html +

IAM policy principal - You can use the Principal element in a policy to specify the principal that is allowed or denied access to a resource (In IAM, a principal is a person or application that can make a request for an action or operation on an AWS resource. The principal is authenticated as the AWS account root user or an IAM entity to make requests to AWS). You cannot use the Principal element in an IAM identity-based policy. You can use it in the trust policies for IAM roles and in resource-based policies.

IAM policy condition - The Condition element (or Condition block) lets you specify conditions for when a policy is in effect, like so - "Condition" : { "StringEquals" : { "aws:username" : "johndoe" }}.

IAM policy resource - The Resource element specifies the object or objects that the statement covers. You specify a resource using an ARN.

IAM policy variables

Instead of creating individual policies for each user, you can use policy variables and create a single policy that applies to multiple users (a group policy). Policy variables act as placeholders. When you make a request to AWS, the placeholder is replaced by a value from the request when the policy is evaluated.

As an example, the following policy gives each of the users in the group full programmatic access to a user-specific object (their own "home directory") in Amazon S3.
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q40-i1.jpg
via - https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html

Access Advisor feature on IAM console - To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments

In case you want to give permissions for a resource in one account to a resource in another
(the example is with a DynamoDB table and AWS Lambda function in another account)

You can give a Lambda function created in one account ("account A") permissions to assume a role from another account ("account B") to access resources such as DynamoDB or S3 bucket. You need to create an execution role in Account A that gives the Lambda function permission to do its work. Then you need to create a role in account B that the Lambda function in account A assumes to gain access to the cross-account DynamoDB table. Make sure that you modify the trust policy of the role in Account B to allow the execution role of Lambda to assume this role. Finally, update the Lambda function code to add the AssumeRole API call.

Sample use-case to configure a Lambda function to assume a role from another AWS account:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q1-i1.jpg
via - https://aws.amazon.com/premiumsupport/knowledge-center/lambda-function-assume-iam-role/

You cannot attach a resource policy to a DynamoDB

https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html +
https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html +

The development team has just configured and attached the IAM policy needed to access AWS Billing and Cost Management for all users under the Finance department. But, the users are unable to see AWS Billing and Cost Management service in the AWS console. What could be the reason for this issue?

You need to activate IAM user access to the Billing and Cost Management console for all the users who need access - By default, IAM users do not have access to the AWS Billing and Cost Management console. You or your account administrator must grant users access. You can do this by activating IAM user access to the Billing and Cost Management console and attaching an IAM policy to your users. Then, you need to activate IAM user access for IAM policies to take effect. You only need to activate IAM user access once.

You manage access in AWS by creating policies and attaching them to IAM identities (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. Resource-based policies are JSON policy documents that you attach to a resource such as an Amazon S3 bucket. These policies grant the specified principal permission to perform specific actions on that resource and define under what conditions this applies.

Trust policy - Trust policies define which principal entities (accounts, users, roles, and federated users) can assume the role. An IAM role is both an identity and a resource that supports resource-based policies. For this reason, you must attach both a trust policy and an identity-based policy to an IAM role. The IAM service supports only one type of resource-based policy called a role trust policy, which is attached to an IAM role.

AWS Organizations Service Control Policies (SCP) - If you enable all features of AWS organization, then you can apply service control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user. An explicit deny in any of these policies overrides the allow.


Access control list (ACL) - Access control lists (ACLs) are service policies that allow you to control which principals in another account can access a resource. ACLs cannot be used to control access for a principal within the same account. Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs.

Permissions boundary - AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. An entity's permissions boundary allows it to perform only the actions that are allowed by both its identity-based policies and its permissions boundaries.

https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_resource-based
https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html

Access Advisor feature on IAM console- To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments. Additionally, by removing unused roles, you can simplify your monitoring and auditing efforts by focusing only on roles that are in use. This feature can help us identify unused IAM roles and remove them without disrupting any service.

how to enable cross-account resource access (for example, with an S3 bucket in one account and EC2 instance in another):
Create an IAM role with S3 access in Account B and set Account A as a trusted entity. Create another role (instance profile) in Account A and attach it to the EC2 instances in Account A and add an inline policy to this role to assume the role from Account B

https://docs.aws.amazon.com/xray/latest/devguide/security_iam_troubleshoot.html

IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.
Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.
This is the most secure option as the role assigned to EC2 can be used to access S3 without storing any credentials onto the EC2 instance.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html

https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html

=======================================================================================================
Compute and Networking

-- EC2

Burstable performance instances, which are T3, T3a, and T2 instances, are designed to provide a baseline level of CPU performance with the ability to burst to a higher level when required by your workload. Burstable performance instances are the only instance types that use credits for CPU usage.

Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives it the same name as the role to which it corresponds.

When you first configure the CLI you may run "aws configure", afterward you should not need to if you want to obtain credentials to authenticate to other AWS services. An IAM role will receive temporary credentials for you so you can focus on using the CLI to get access to other AWS services if you have the permissions.

0 seconds - AWS states that, if your AWS account is less than 12 months old, you can use a t2.micro instance for free within certain usage limits.
Reference:

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html +

Dedicated Instances - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.

A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.

Differences between Dedicated Hosts and Dedicated Instances:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q21-i1.jpg +
via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances +
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html +

High Level Overview of EC2 Instance Purchase Options:  
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q21-i2.jpg +
via - https://aws.amazon.com/ec2/pricing/ +

Spot Instances - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated.

You can specify that Amazon EC2 should do one of the following when it interrupts a Spot Instance:
Stop the Spot Instance
Hibernate the Spot Instance
Terminate the Spot Instance
The default is to terminate Spot Instances when they are interrupted.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html +

Dedicated Hosts - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.

On-Demand Instances - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.

We can decrease EC2 instance load in case of secure traffic via HTTPS in the following way:
"Configure an SSL/TLS certificate on an Application Load Balancer via AWS Certificate Manager (ACM)"

EC2 reserved instance types:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q19-i1.jpg +
via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html +

High Level Overview of EC2 Instance Purchase Options:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q19-i2.jpg +

Elastic Reserved instances - This option has been added as a distractor. There are no Elastic Reserved Instance types.

Reserved Instances offer significant savings on Amazon EC2 costs compared to On-Demand Instance pricing. A Reserved Instance can be purchased for a one-year or three-year commitment, with the three-year commitment offering a bigger discount. Reserved instances come with two offering classes - Standard or Convertible.

Standard Reserved instances - With Standard Reserved Instances, some attributes, such as instance size, can be modified during the term; however, the instance family cannot be modified. You cannot exchange a Standard Reserved Instance, only modify it

Scheduled Reserved instances - Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance so that you know it is available when you need it.

Convertible Reserved instances - A Convertible Reserved Instance can be exchanged during the term for another Convertible Reserved Instance with new attributes including instance family, instance type, platform, scope, or tenancy. This is the best fit for the current requirement.

Zonal Reserved Instances - A zonal Reserved Instance provides a capacity reservation in the specified Availability Zone. Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations independently from the billing discounts offered by Savings Plans or regional Reserved Instances.

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html +

https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q31-i1.jpg +
via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-scope.html +

Reserved instance types:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q19-i1.jpg +

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html +
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html +

Auto Scaling groups cannot span across multiple Regions.
Auto Scaling groups can span across the availability Zones of a Region.
When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.
An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region.
For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets. Customers can select the subnets for your EC2 instances when you create or update the Auto Scaling group.

Amazon EC2 Auto Scaling Overview:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q27-i1.jpg +
via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html +
https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html +

By default, scripts entered as user data are executed with root user privileges - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.

By default, user data runs only during the boot cycle when you first launch an instance - By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html +

All Reserved Instances provide you with a discount compared to On-Demand pricing.

A Reserved Instance billing benefit can apply to a maximum of 3600 seconds (one hour) of instance usage per clock-hour. You can run multiple instances concurrently, but can only receive the benefit of the Reserved Instance discount for a total of 3600 seconds per clock-hour; instance usage that exceeds 3600 seconds in a clock-hour is billed at the On-Demand rate.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i1.jpg +
via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/reserved-instances-types.html +

High Level Overview of EC2 Instance Purchase Options:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q34-i2.jpg +
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts-reserved-instances-application.html +

You configure monitoring for EC2 instances using a launch configuration or template. Monitoring is enabled whenever an instance is launched, either basic monitoring (5-minute granularity) or detailed monitoring (1-minute granularity).

AWS Management Console might have been used to create the launch configuration - By default, basic monitoring is enabled when you create a launch template or when you use the AWS Management Console to create a launch configuration. This could be the reason behind only the basic monitoring taking place.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q28-i1.jpg
via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/enable-as-instance-metrics.html +
Detailed monitoring is enabled by default when you create a launch configuration using the AWS CLI or an SDK.

ASGAverageCPUUtilization - This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group.

ASGAverageNetworkOut - This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network interfaces by the Auto Scaling group.

ALBRequestCountPerTarget - This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an Application Load Balancer target group.

https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html

It is possible to reuse SSH keys across regions.
Here is the correct way of reusing SSH keys in your AWS Regions:
1. Generate a public SSH key (.pub) file from the private SSH key (.pem) file.
2. Set the AWS Region you wish to import to.
3. Import the public SSH key into the new Region.

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html +
https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html +
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html +

Policy Evaluation explained:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q43-i1.jpg +
via - https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html +

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.html +
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.environments.html +

-- AWS VPC / -- Amazon VPC / Networking & VPC / -- VPC

Which conditions should be met for Internet connectivity to be established in EC2?

The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic - The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity

The route table in the instance’s subnet should have a route to an Internet Gateway - A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.

A subnet can only be associated with one route table at a time.
 A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.

Public subnets have access to the internet via Internet Gateway.

https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html

-- ASG / -- Auto Scaling Group

https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html

How scaling policies work:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q21-i1.jpg
via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html

You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.

Since a minimum capacity of 1 was defined, an instance was launched in only one AZ. This AZ went down, taking the application with it. If the minimum capacity is set to 2. As per Auto Scale AZ configuration, it would have launched 2 instances- one in each AZ, making the architecture disaster-proof and hence highly available.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q65-i1.jpg
via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html +

With target tracking scaling policies, you select a scaling metric and set a target value. You can use predefined customized metrics.

https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html

=======================================================================================================
-- ELB - Elastic Load balancer / -- Elastic Load Balancer

ALB access logs - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Access logging is an optional feature of Elastic Load Balancing that is disabled by default.
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q29-i1.jpg
via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html

Elastic Load Balancers can connect with Auto Scale groups to provide horizontal scaling.
A Load Balancer can target EC2 instances only within an AWS Region.

https://aws.amazon.com/elasticloadbalancing/
https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html

ALB request tracing - You can use request tracing to track HTTP requests. The load balancer adds a header with a trace identifier to each request it receives. Request tracing will not help you to analyze latency specific data.

ALB HTTP status response options:
HTTP 503 - HTTP 503 indicates 'Service unavailable' error. This error in ALB is an indicator of the target groups for the load balancer having no registered targets.

HTTP 500 - HTTP 500 indicates 'Internal server' error. There are several reasons for their error: A client submitted a request without an HTTP protocol, and the load balancer was unable to generate a redirect URL, there was an error executing the web ACL rules.

HTTP 504 - HTTP 504 is 'Gateway timeout' error. Several reasons for this error, to quote a few: The load balancer failed to establish a connection to the target before the connection timeout expired, The load balancer established a connection to the target but the target did not respond before the idle timeout period elapsed.

HTTP 403 - HTTP 403 is 'Forbidden' error. You configured an AWS WAF web access control list (web ACL) to monitor requests to your Application Load Balancer and it blocked a request.

https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html

You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.

Application Load Balancer Configuration for Security Groups and Health Check Routes:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q28-i1.jpg
via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html

Cross-Zone Load Balancing Overview:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q30-i1.jpg
via - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html

X-Forwarded-For - The X-Forwarded-For request header helps you identify the IP address of a client when you use an HTTP or HTTPS load balancer. Because load balancers intercept traffic between clients and servers, your server access logs contain only the IP address of the load balancer. To see the IP address of the client, use the X-Forwarded-For request header.

Incorrect options:

X-Forwarded-Proto - The X-Forwarded-Proto request header helps you identify the protocol (HTTP or HTTPS) that a client used to connect to your load balancer. Your server access logs contain only the protocol used between the server and the load balancer; they contain no information about the protocol used between the client and the load balancer. To determine the protocol used between the client and the load balancer, use the X-Forwarded-Proto request header.

X-Forwarded-Port - The X-Forwarded-Port request header helps you identify the destination port that the client used to connect to the load balancer.

https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html#x-forwarded-for

https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance
https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance

Sticky sessions:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q58-i1.jpg
via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#slow-start-mode

A Classic Load Balancer with HTTP or HTTPS listeners might route more traffic to higher-capacity instance types. This distribution aims to prevent lower-capacity instance types from having too many outstanding requests. It’s a best practice to use similar instance types and configurations to reduce the likelihood of capacity gaps and traffic imbalances.
A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in favor of higher-capacity instance types is desirable.

https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions
https://aws.amazon.com/premiumsupport/knowledge-center/elb-fix-unequal-traffic-routing/
https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones

A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration. Incoming connections remain unmodified, so application software need not support X-Forwarded-For.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q50-i1.jpg
via - https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html

An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply and then selects a target from the target group for the rule action.
One of many benefits of the Application Load Balancer is its support for path-based routing. You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL. For needs relating to network traffic go with Network Load Balancer.

 It is a basic load balancer that distributes traffic. If your account was created before 2013-12-04, your account supports EC2-Classic instances and you will benefit in using this type of load balancer. The classic load balancer can be used regardless of when your account was created and whether you use EC2-Classic or whether your instances are in a VPC but just remember its the basic load balancer AWS offers and not advanced as the others.

"Create an HTTPS listener on the Application Load Balancer with SSL termination"
An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions.
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/images/component_architecture.png +
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html +

To use an HTTPS listener, you must deploy at least one SSL/TLS server certificate on your load balancer. You can create an HTTPS listener, which uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. As the EC2 instances are under heavy CPU load, the load balancer will use the server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the EC2 instances.
https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/ +

=======================================================================================================
Storage

-- Amazon S3 / -- AWS S3 / -- S3

S3 Analytics - By using Amazon S3 analytics Storage Class Analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class.

https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/

S3 security best practices:
https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q8-i1.jpg
 via - https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html

 https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html

https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html

You need to use multi-part upload for large files: In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q30-i1.jpg
 via - https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html
 With a single PUT operation, you can upload objects up to 5 GB in size.

https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html
https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html

When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket. The extra logs about logs might make it harder to find the log that you are looking for. This configuration would drastically increase the size of the S3 bucket.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q57-i1.jpg
 via - https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/
 https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-access-control.html

On Amazon S3, all objects by default are private. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.
You can also use an IAM instance profile to create a pre-signed URL. When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The pre-signed URLs are valid only for the specified duration. So for the given use-case, the object key can be retrieved from the DynamoDB table, and then the application can generate the pre-signed URL using the IAM instance profile.

Please see this note for more details:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q5-i1.jpg
via - https://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html

https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/

which of the following would you suggest to automatically make the S3 bucket owner, also the owner of all objects in the bucket, irrespective of the AWS account used for uploading the objects?
Use S3 Object Ownership to default bucket owner to be the owner of all objects in the bucket
S3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account. With S3 Object Ownership, any new objects that are written by other accounts with the bucket-owner-full-control canned access control list (ACL) automatically become owned by the bucket owner, who then has full control of the objects.
S3 Object Ownership has two settings: 1. Object writer – The uploading account will own the object. 2. Bucket owner preferred – The bucket owner will own the object if the object is uploaded with the bucket-owner-full-control canned ACL. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account.

Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. A bucket ACLs allow you to control access at bucket level.

https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html
https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html

Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. However, S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. Each time S3 Transfer Acceleration is used to upload an object, AWS checks whether S3 Transfer Acceleration is likely to be faster than a regular Amazon S3 transfer. If it finds that S3 Transfer Acceleration might not be significantly faster, AWS shifts back to normal Amazon S3 transfer mode.

Creating a CORS rule for an S3 bucket:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q4-i1.jpg
via - https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html

Which of the following options can be used to control access to data stored on Amazon S3? 
Bucket policies, Identity and Access Management (IAM) policies
Query String Authentication, Access Control Lists (ACLs)
https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html

Allows full S3 access, but explicitly denies access to the Production bucket if the user has not signed in using MFA within the last thirty minutes - This example shows how you might create a policy that allows an Amazon S3 user to access any bucket, including updating, adding, and deleting objects. However, it explicitly denies access to the Production bucket if the user has not signed in using multi-factor authentication (MFA) within the last thirty minutes. This policy grants the permissions necessary to perform this action in the console or programmatically using the AWS CLI or AWS API.
This policy never allows programmatic access to the Production bucket using long-term user access keys. This is accomplished using the aws:MultiFactorAuthAge condition key with the NumericGreaterThanIfExists condition operator. This policy condition returns true if MFA is not present or if the age of the MFA is greater than 30 minutes. In those situations, access is denied. To access the Production bucket programmatically, the S3 user must use temporary credentials that were generated in the last 30 minutes using the GetSessionToken API operation.
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_full-access-except-production.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html
https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html
https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html
https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html

S3 lifecycle actions are not replicated with S3 replication - With S3 Replication (CRR and SRR), you can establish replication rules to make copies of your objects into another storage class, in the same or a different region. Lifecycle actions are not replicated, and if you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both.
Amazon S3 Replication (CRR and SRR) is configured at the S3 bucket level, a shared prefix level, or an object level using S3 object tags. You add a replication configuration on your source bucket by specifying a destination bucket in the same or different AWS region for replication.
https://docs.amazonaws.cn/en_us/AmazonS3/latest/userguide/replication.html

To configure your bucket to allow cross-origin requests, you create a CORS configuration. The CORS configuration is a document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support each origin, and other operation-specific information. You can add up to 100 rules to the configuration. You can add the CORS configuration as the cors subresource to the bucket.
If you are configuring CORS in the S3 console, you must use JSON to create a CORS configuration. The new S3 console only supports JSON CORS configurations.

S3 Access Analyzer - Access Analyzer for S3 is a feature that monitors your access policies, ensuring that the policies provide only the intended access to your S3 resources. Access Analyzer for S3 evaluates your bucket access policies and enables you to discover and swiftly remediate buckets with potentially unintended access.

S3 Access Points - Amazon S3 Access Points simplifies managing data access at scale for applications using shared data sets on S3. With S3 Access Points, you can now easily create hundreds of access points per bucket, representing a new way of provisioning access to shared data sets. Access Points provide a customized path into a bucket, with a unique hostname and access policy that enforces the specific permissions and network controls for any request made through the access point.
https://docs.aws.amazon.com/AmazonS3/latest/userguide/ManageCorsUsing.html
https://aws.amazon.com/s3/faqs/

In S3 IAM roles and resource-based policies delegate access across accounts only within a single partition. For example, assume that you have an account in US West (N. California) in the standard aws partition. You also have an account in China (Beijing) in the aws-cn partition. You can't use an Amazon S3 resource-based policy in your account in China (Beijing) to allow access for users in your standard AWS account.

Amazon S3 ACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP, WRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon S3 groups as a grantee for the Amazon S3 ACL.

If you apply a bucket policy at the bucket level, you can define who can access (Principal element), which objects they can access (Resource element), and how they can access (Action element). Applying a bucket policy at the bucket level allows you to define granular access to different objects inside the bucket by using multiple policies to control access. You can also review the bucket policy to see who can access objects in an S3 bucket.
https://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html
https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/

You can protect data at rest in Amazon S3 by using three different modes of server-side encryption: SSE-S3, SSE-C, or SSE-KMS. SSE-KMS requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS. You can choose a customer managed CMK or the AWS managed CMK for Amazon S3 in your account. If you choose to encrypt your data using the standard features, AWS KMS and Amazon S3 perform the following actions:
1. Amazon S3 requests a plaintext data key and a copy of the key encrypted under the specified CMK.
2. AWS KMS generates a data key, encrypts it under the CMK, and sends both the plaintext data key and the encrypted data key to Amazon S3.
3. Amazon S3 encrypts the data using the data key and removes the plaintext key from memory as soon as possible after use.
4. Amazon S3 stores the encrypted data key as metadata with the encrypted data.
An IAM user or role needing to perform those actions needs the kms:GenerateDataKey action enabled.
https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/
https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html

SSE-S3 Overview:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q21-i1.jpg
via - https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html

https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html
https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html

-- Amazon Elastic Block Store (EBS) / -- EBS / -- AWS EBS / -- Amazon EBS

The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1. So, for a 200 GiB volume size, max IOPS possible is 200*50 = 10000 IOPS.

Overview of Provisioned IOPS SSD (io1) volumes:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q36-i1.jpg
via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html

To create and use an encrypted Amazon Elastic Block Store (EBS) volume, you need permissions to use Amazon EBS. The key policy associated with the CMK would need to include these. The above policy is an example of one such policy.

In this CMK policy, the first statement provides a specified IAM principal the ability to generate a data key and decrypt that data key from the CMK when necessary. These two APIs are necessary to encrypt the EBS volume while it’s attached to an Amazon Elastic Compute Cloud (EC2) instance.
https://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf

The performance of gp2 volumes is tied to volume size, which determines the baseline performance level of the volume and how quickly it accumulates I/O credits; larger volumes have higher baseline performance levels and accumulate I/O credits faster.

General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods of time. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size.

Maximum IOPS vs Volume Size for General Purpose SSD (gp2) volumes:
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/gp2_iops_1.png
via - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html

EBS volumes are AZ locked.
 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html

EBS volumes support both in-flight encryption and encryption at rest using KMS
You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:
- Data at rest inside the volume
- All data moving between the volume and the instance
- All snapshots created from the volume
- All volumes created from those snapshots
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html

-- Amazon Elastic File System (EFS)
-- Amazon Glacier

=======================================================================================================
-- Amazon CloudFront / -- CloudFront / -- AWS CloudFront

CloudFront Overview:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q16-i1.jpg
via - https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/
https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/

=======================================================================================================
Docker in AWS (ECR, ECS)

ECS Overview:
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/images/overview-fargate.png +
https://d1.awsstatic.com/diagrams/product-page-diagrams/product-page-diagram_ECS_1.86ebd8c223ec8b55aa1903c423fbe4e672f3daf7.png +
via - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html +

How ECR works:
https://d1.awsstatic.com/diagrams/product-page-diagrams/Product-Page-Diagram_Amazon-ECR.bf2e7a03447ed3aba97a70e5f4aead46a5e04547.png +
via - https://aws.amazon.com/ecr/ +

Using a single task definition allows the two containers to share memory. 
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q2-i1.jpg
via - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/application_architecture.html +
https://aws.amazon.com/blogs/containers/how-amazon-ecs-manages-cpu-and-memory-resources/ +

https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs +
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html +

ECS_ENABLE_TASK_IAM_ROLE - This configuration item is used to enable IAM roles for tasks for containers with the bridge and default network modes.
ECS_ENGINE_AUTH_DATA - This refers to the authentication data within a Docker configuration file, so this is not the correct option.
ECS_AVAILABLE_LOGGING_DRIVERS - The Amazon ECS container agent running on a container instance must register the logging drivers available on that instance with this variable. This configuration item refers to the logging driver.
ECS_CLUSTER - This refers to the ECS cluster that the ECS agent should check into. This is passed to the container instance at launch through Amazon EC2 user data.
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html +

https://aws.amazon.com/blogs/compute/microservice-delivery-with-amazon-ecs-and-application-load-balancers/ +

If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.

https://aws.amazon.com/premiumsupport/knowledge-center/deregister-ecs-instance/ +
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html +

In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.

Sample config for ECS Container Agent:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q35-i1.jpg +
via - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html +
https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html +

=======================================================================================================
Databases

-- AWS RDS / -- RDS / -- Amazon RDS

RDS MySQL and RDS PostgreSQL work with IAM database authentication.
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html

The following two actions can help to establish a resilience RDS setup:
- use cross-region read replicas
- Enable the automated backup feature of Amazon RDS in a multi-AZ deployment that creates backups in a single AWS Region

https://aws.amazon.com/rds/features/ +
https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/ +

You can enable automatic backups but as of 2020, the retention period is 0 to 35 days.
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CreateSnapshot.html +


-- AWS Aurora
-- AWS ElastiCache / -- ElastiCache

https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/images/ElastiCache-Caching.png +
via - https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html +

Overview of Amazon ElastiCache features:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q3-i1.jpg
via - https://aws.amazon.com/elasticache/features/ +

https://aws.amazon.com/caching/session-management/ +

One can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster mode comes with the primary benefit of horizontal scaling of your Redis cluster, with almost zero impact on the performance of the cluster.

When building production workloads, you should consider using a configuration with replication, unless you can easily recreate your data. Enabling Cluster-Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster-Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.

Redis Cluster config:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q14-i1.jpg +
via - https://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/ +

Redis offers snapshots facility, replication, and supports transactions, which Memcached cannot
https://aws.amazon.com/elasticache/redis-vs-memcached/ +

https://aws.amazon.com/caching/database-caching/ +

Geospatial on Amazon ElastiCache for Redis:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q38-i1.jpg +

You can leverage ElastiCache for Redis with cluster mode enabled to enhance reliability and availability with little change to your existing workload. Cluster Mode comes with the primary benefit of horizontal scaling up and down of your Redis cluster, with almost zero impact on the performance of the cluster.

Enabling Cluster Mode provides a number of additional benefits in scaling your cluster. In short, it allows you to scale in or out the number of shards (horizontal scaling) versus scaling up or down the node type (vertical scaling). This means that Cluster Mode can scale to very large amounts of storage (potentially 100s of terabytes) across up to 90 shards, whereas a single node can only store as much data in memory as the instance type has capacity for.

https://aws.amazon.com/blogs/database/amazon-elasticache-utilizing-redis-geospatial-capabilities/ +

-- Other
-- Amazon DynamoDB / -- DynamoDB / -- AWS DynamoDB

DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique. Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB).

DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key.

Please see these details for the DynamoDB Partition Keys:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i1.jpg +
via - https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/ +

https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q11-i2.jpg +
via - https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/ +

https://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/ +

Use the DynamoDB on-demand backup capability to write to Amazon S3 and download locally
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q58-i1.jpg +
via - https://aws.amazon.com/premiumsupport/knowledge-center/back-up-dynamodb-s3/ +
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/BackupRestore.html +
https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-importexport-ddb-part1.html
https://docs.aws.amazon.com/emr/latest/ReleaseGuide/EMR_Hive_Commands.html#EMR_Hive_Commands_exporting +
https://aws.amazon.com/blogs/big-data/how-to-export-an-amazon-dynamodb-table-to-amazon-s3-using-aws-step-functions-and-aws-glue/ +

https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i1.jpg +
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q44-i2.jpg +
via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html +

DynamoDB Transactions Overview:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q8-i1.jpg +
via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transactions.html +

DynamoDB optionally supports conditional writes for write operations (PutItem, UpdateItem, DeleteItem). A conditional write succeeds only if the item attributes meet one or more expected conditions. Otherwise, it returns an error.
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate +

DynamoDB supports eventually consistent and strongly consistent reads.

Eventually Consistent Reads

When you read data from a DynamoDB table, the response might not reflect the results of a recently completed write operation. The response might include some stale data. If you repeat your read request after a short time, the response should return the latest data.

Strongly Consistent Reads

When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful.

DynamoDB uses eventually consistent reads by default. Read operations (such as GetItem, Query, and Scan) provide a ConsistentRead parameter. If you set this parameter to true, DynamoDB uses strongly consistent reads during the operation. As per the given use-case, to make sure that only the last updated value of any item is used in the application, you should use strongly consistent reads by setting ConsistentRead = true for GetItem operation.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q9-i1.jpg +
via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html +

 a ProjectionExpression: A projection expression is a string that identifies the attributes you want. To retrieve a single attribute, specify its name. For multiple attributes, the names must be comma-separated.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q19-i1.jpg +
 via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html +

A Scan operation in Amazon DynamoDB reads every item in a table or a secondary index. By default, a Scan operation returns all of the data attributes for every item in the table or index. You can also use the ProjectionExpression parameter so that Scan only returns some of the attributes, rather than all of them.

The Query operation in Amazon DynamoDB finds items based on primary key values. You must provide the name of the partition key attribute and a single value for that attribute. The Query returns all items with that partition key value. Optionally, you can provide a sort key attribute and use a comparison operator to refine the search results.

 If you need to further refine the Query results, you can optionally provide a filter expression. A filter expression determines which items within the Query results should be returned to you. All of the other results are discarded. A filter expression is applied after Query finishes, but before the results are returned. Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present. A Query operation can retrieve a maximum of 1 MB of data. This limit applies before the filter expression is evaluated.

 It's not always possible to distribute read and write activity evenly. When data access is imbalanced, a "hot" partition can receive a higher volume of read and write traffic compared to other partitions. To better accommodate uneven access patterns, DynamoDB adaptive capacity enables your application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed your table’s total provisioned capacity or the partition maximum capacity.

ProvisionedThroughputExceededException explained:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i1.jpg +
via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html +

Hot partition explained:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q41-i2.jpg +
via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html +

The most efficient way to truncate a dynamodb table is by deleting it and then recreating it using
the delete-table CLI operation (DeleteTable API operation).
Scanning and deleting each item is a too expensive operation.
https://docs.aws.amazon.com/cli/latest/reference/dynamodb/delete-table.html +

Streams:
https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html +

Differences between GSI and LSI:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q24-i1.jpg +
via - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html +

=======================================================================================================
Route 53 and DNS
-- AWS Route 53 / -- Route 53

the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.

Comparison between CNAME and Alias record:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q34-i1.jpg +
via - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html +

Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record. 3rd party websites do not qualify for these as we have no control over those.
https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html +

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q38-i1.jpg +
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q38-i2.jpg +

https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html +

=======================================================================================================
-- Encryption / -- AWS KMS / -- KMS / -- CloudHSM / -- AWS CloudHSM

KMS stores the CMK, and receives data from the clients, which it encrypts and sends back. This is how
KMS encryption works.

The maximum data size supported by KMS is 4KB.

A customer master key (CMK) is a logical representation of a master key. The CMK includes metadata, such as the key ID, creation date, description, and key state. The CMK also contains the key material used to encrypt and decrypt data. You can generate CMKs in KMS, in an AWS CloudHSM cluster, or import them from your key management infrastructure.

AWS KMS supports symmetric and asymmetric CMKs. A symmetric CMK represents a 256-bit key that is used for encryption and decryption. An asymmetric CMK represents an RSA key pair that is used for encryption and decryption or signing and verification (but not both), or an elliptic curve (ECC) key pair that is used for signing and verification.

AWS KMS supports three types of CMKs: customer-managed CMKs, AWS managed CMKs, and AWS owned CMKs.

Overview of Customer master keys (CMKs):
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q14-i1.jpg
via - https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys

https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys
https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html

The following two services can be used to deploy TLS/SSL certificates in AWS:
- IAM -  IAM is used as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM. IAM securely encrypts your private keys and stores the encrypted version in IAM SSL certificate storage. IAM supports deploying server certificates in all Regions, but you must obtain your certificate from an external provider for use with AWS. You cannot upload an ACM certificate to IAM. Additionally, you cannot manage your certificates from the IAM Console.
- AWS Certificate manager - AWS Certificate Manager (ACM) is the preferred tool to provision, manage, and deploy server certificates. With ACM you can request a certificate or deploy an existing ACM or external certificate to AWS resources. Certificates provided by ACM are free and automatically renew. In a supported Region, you can use ACM to manage server certificates from the console or programmatically.

CloudFront Key pairs can only be created by the root user.
For Amazon CloudFront, you use key pairs to create signed URLs for private content, such as when you want to distribute restricted content that someone paid for.

https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html

While AWS KMS does support sending data up to 4 KB to be encrypted directly, envelope encryption can offer significant performance benefits. When you encrypt data directly with AWS KMS it must be transferred over the network. Envelope encryption reduces the network load since only the request and delivery of the much smaller data key go over the network. The data key is used locally in your application or encrypting AWS service, avoiding the need to send the entire block of data to AWS KMS and suffer network latency.

AWS Lambda environment variables can have a maximum size of 4 KB. Additionally, the direct 'Encrypt' API of KMS also has an upper limit of 4 KB for the data payload. To encrypt data larger than 1 MB and pass it to a lambda function, you need to use the Encryption SDK and pack the encrypted file with the lambda function.

https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html
https://aws.amazon.com/kms/faqs/

The development team at a company wants to encrypt a 111 GB object using AWS KMS.
Which of the following represents the best solution?
Make a GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data - GenerateDataKey API, generates a unique symmetric data key for client-side encryption. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data.
GenerateDataKey returns a unique data key for each request. The bytes in the plaintext key are not related to the caller or the CMK.
To encrypt data outside of AWS KMS:
1. Use the GenerateDataKey operation to get a data key.
2. Use the plaintext data key (in the Plaintext field of the response) to encrypt your data outside of AWS KMS. Then erase the plaintext data key from memory.
3. Store the encrypted data key (in the CiphertextBlob field of the response) with the encrypted data.
To decrypt data outside of AWS KMS:
1. Use the Decrypt operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.
2. Use the plaintext data key to decrypt data outside of AWS KMS, then erase the plaintext data key from memory.

Encrypt API is used to encrypt plaintext into ciphertext by using a customer master key (CMK). The Encrypt operation has two primary use cases:
- To encrypt small amounts of arbitrary data, such as a personal identifier or database password, or other sensitive information.
- To move encrypted data from one AWS Region to another.
https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html
https://docs.aws.amazon.com/kms/latest/APIReference/API_Encrypt.html
https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html

=======================================================================================================
-- Elastic Beanstalk / -- AWS Elastic Beanstalk / -- Amazon Elastic Beanstalk

.ebextensions/<mysettings>.config : You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q64-i1.jpg +
via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html +
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html +
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions-optionsettings.html +

Elastic BeanStalk Key Concepts:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q59-i1.jpg +
via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.html +

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/images/clearbox-flow-00.png +
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html +
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html +


With deployment policies such as 'All at once', AWS Elastic Beanstalk performs an in-place update when you update your application versions and your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then swap CNAMEs (via Route 53) of the two environments to redirect traffic to the new version instantly. In case of any deployment issues, the rollback process is very quick via swapping the URLs for the two environments.

Overview of Elastic Beanstalk Deployment Policies:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q10-i1.jpg
via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html +

'All at once' deployment policy - Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time. 

'Rolling' deployment policy - This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. 

Deploy the new application version using 'Rolling with additional batch' deployment policy - This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment.

Any resources created as part of your .ebextensions is part of your Elastic Beanstalk template and will get deleted if the environment is terminated.

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q23-i1.jpg +
 via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html +

  To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with blue-green deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group.


Using Elastic Beanstalk with Amazon RDS:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q23-i2.jpg +
 via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html +

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/customize-environment-resources-elasticache.html +

The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.

Overview of Elastic Beanstalk Deployment Policies:

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q15-i1.jpg +
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q61-i1.jpg +
via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html +

Deploy using 'Rolling with additional batch' deployment policy - With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment. Launching the extra batch takes time, and ensures that the same bandwidth is retained throughout the deployment. This policy also avoids any reduced availability, although at a cost of an even longer deployment time compared to the Rolling method. Finally, this option is suitable if you must maintain the same bandwidth throughout the deployment.

Deploy using 'Immutable' deployment policy - A slower deployment method, that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails. With this method, Elastic Beanstalk performs an immutable update to deploy your application. In an immutable update, a second Auto Scaling group is launched in your environment and the new version serves traffic alongside the old version until the new instances pass health checks.

Deploy using 'All at once' deployment policy - This is the quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance. Then, the web proxy or application server might need to restart. As a result, your application might be unavailable to users (or have low availability) for a short time.

Deploy using 'Rolling' deployment policy - With this method, your application is deployed to your environment one batch of instances at a time. Most bandwidth is retained throughout the deployment. Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service.

SSM parameter store is still not supported by beanstalk.

https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method +

=======================================================================================================
Deployment as Code

-- AWS CodeDeploy / -- CodeDeploy

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q58-i1.jpg +
 via - https://aws.amazon.com/about-aws/whats-new/2017/01/aws-codedeploy-introduces-blue-green-deployments/ +
https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview-blue-green
https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html +

An AppSpec file must be a YAML-formatted file named appspec.yml and it must be placed in the root of the directory structure of an application's source code.

The AppSpec file is used to:
Map the source files in your application revision to their destinations on the instance.
Specify custom permissions for deployed files.
Specify scripts to be run on each instance at various stages of the deployment process.

During deployment, the CodeDeploy agent looks up the name of the current event in the hooks section of the AppSpec file. If the event is not found, the CodeDeploy agent moves on to the next step. If the event is found, the CodeDeploy agent retrieves the list of scripts to execute. The scripts are run sequentially, in the order in which they appear in the file. The status of each script is logged in the CodeDeploy agent log file on the instance.

If a script runs successfully, it returns an exit code of 0 (zero). If the CodeDeploy agent installed on the operating system doesn't match what's listed in the AppSpec file, the deployment fails.

https://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html +

An EC2/On-Premises deployment hook is executed once per deployment to an instance. You can specify one or more scripts to run in a hook.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q56-i1.jpg +
 via - https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order +

ValidateService: ValidateService is the last deployment lifecycle event. It is used to verify the deployment was completed successfully.
AfterInstall - You can use this deployment lifecycle event for tasks such as configuring your application or changing file permissions
ApplicationStart - You typically use this deployment lifecycle event to restart services that were stopped during ApplicationStop
AllowTraffic - During this deployment lifecycle event, internet traffic is allowed to access instances after a deployment. This event is reserved for the AWS CodeDeploy agent and cannot be used to run scripts

Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.

You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy.

The CodeDeploy agent is a software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments. The CodeDeploy agent archives revisions and log files on instances. The CodeDeploy agent cleans up these artifacts to conserve disk space. You can use the :max_revisions: option in the agent configuration file to specify the number of application revisions to the archive by entering any positive integer. CodeDeploy also archives the log files for those revisions. All others are deleted, except for the log file of the last successful deployment.
More info here:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q48-i1.jpg +
via - https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html +
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AgentReference.html +
https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/

 if the deployment of the new version fails a new deployment of the last known working version
 of the application is deployed with a new deployment ID
https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html +

-- AWS CodePipeline / -- Amazon CodePipeline / -- CodePipeline

https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals-action-add.html +
https://docs.aws.amazon.com/codepipeline/latest/userguide/vpc-support.html +
https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html +

-- AWS CodeBuild / -- CodeBuild

https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html +
https://docs.aws.amazon.com/codebuild/latest/userguide/build-env-ref-compute-types.html +

To prevent a build running for too long, we need to enable CodeBuild timeouts.
CodeBuild scales automatically, the organization does not have to do anything for scaling or for parallel builds

A build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).
The following rules apply when you run multiple builds:
When possible, builds run concurrently. The maximum number of concurrently running builds can vary.
Builds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.
A build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.
By setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.

https://docs.aws.amazon.com/codebuild/latest/userguide/builds-working.html

With the Local Build support for AWS CodeBuild, you just specify the location of your source code, choose your build settings, and CodeBuild runs build scripts for compiling, testing, and packaging your code. You can use the AWS CodeBuild agent to test and debug builds on a local machine.

By building an application on a local machine you can:
Test the integrity and contents of a buildspec file locally.
Test and build an application locally before committing.
Identify and fix errors quickly from your local development environment.

AWS CodeBuild is integrated with AWS CloudTrail.
CloudTrail captures all API calls for CodeBuild as events, including calls from the CodeBuild console and from code calls to the CodeBuild APIs.

https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/ +

During rollbacks codedeploy first deploys to the failed instances.
AWS CodeDeploy rolls back deployments by redeploying a previously deployed revision of an application as a new deployment on the failed instances.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q22-i1.jpg +
via - https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html +

AWS CodeBuild monitors functions on your behalf and reports metrics through Amazon CloudWatch. These metrics include the number of total builds, failed builds, successful builds, and the duration of builds. You can monitor your builds at two levels: Project level, AWS account level. You can export log data from your log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.

https://docs.aws.amazon.com/codebuild/latest/userguide/monitoring-metrics.html +
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html +
https://docs.aws.amazon.com/codebuild/latest/userguide/getting-started-input-bucket-console.html +

Best Practices for Caching Dependencies:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q7-i1.jpg +
via - https://aws.amazon.com/blogs/devops/how-to-enable-caching-for-aws-codebuild/ +

https://docs.aws.amazon.com/codebuild/latest/userguide/create-project.html#create-project-cli

-- AWS CodeCommit / -- CodeCommit

The following can be used for authentication against CodeCommit:
Git credentials - These are IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS.
SSH Keys - Are locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH.
AWS access keys - You can use these keys with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS.

https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html
https://docs.aws.amazon.com/codebuild/latest/userguide/stop-build.html

Data in AWS CodeCommit repositories is encrypted in transit and at rest. When data is pushed into an AWS CodeCommit repository (for example, by calling git push), AWS CodeCommit encrypts the received data as it is stored in the repository.

https://docs.aws.amazon.com/codecommit/latest/userguide/encryption.html

The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS connections. You can also use these same credentials with any third-party tool or individual development environment (IDE) that supports HTTPS authentication using a static user name and password.
An IAM user is an identity within your Amazon Web Services account that has specific custom permissions. For example, an IAM user can have permissions to create and manage Git credentials for accessing CodeCommit repositories. This is the recommended user type for working with CodeCommit. You can use an IAM user name and password to sign in to secure AWS webpages like the AWS Management Console, AWS Discussion Forums, or the AWS Support Center.
Authentication and access control for AWS CodeCommit:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q52-i1.jpg
via - https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html
https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html

=======================================================================================================
Infrastructure as Code

-- AWS CloudFormation / -- CloudFormation

https://aws.amazon.com/cloudformation/

Sample CloudFormation YAML template (print the image when printing):
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q60-i1.jpg
via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html

'Conditions' section of the template - This optional section includes conditions that control whether certain resources are created or whether certain resource properties are assigned a value during stack creation or update. For example, you could conditionally create a resource that depends on whether the stack is for a production or test environment.
The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.
You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your template, you can add an EnvironmentType input parameter, which accepts either prod or test as inputs. For the production environment, you might include Amazon EC2 instances with certain capabilities; however, for the test environment, you want to use reduced capabilities to save money.
Conditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the Resources and Outputs sections of a template.


'Resources' section of the template - This is the only required section and specifies the stack resources and their properties, such as an Amazon Elastic Compute Cloud instance or an Amazon Simple Storage Service bucket. You can refer to resources in the Resources and Outputs sections of the template.

'Parameters' section of the template - This optional section is helpful in passing Values to your template at runtime (when you create or update a stack). You can refer to parameters from the Resources and Outputs sections of the template.
Parameters enable you to input custom values to your CloudFormation template each time you create or update a stack.
This is how we can define a parameter in a template:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q11-i1.jpg
via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html

Outputs - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find. You can associate conditions with the outputs that you want to conditionally create.


You can create a cross-stack reference to export resources from one AWS CloudFormation stack to another. For example, you might have a network stack with a VPC and subnets and a separate public web application stack. To use the security group and subnet from the network stack, you can create a cross-stack reference that allows the web application stack to reference resource outputs from the network stack. With a cross-stack reference, owners of the web application stacks don't need to create or maintain networking rules or assets.

To create a cross-stack reference, use the Export output field to flag the value of a resource output for export. Then, use the Fn::ImportValue intrinsic function to import the value.

You cannot use the Ref intrinsic function to import the value.
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q3-i1.jpg

via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html

Pseudo parameters are parameters that are predefined by AWS CloudFormation. You do not declare them in your template. Use them the same way as you would a parameter, as the argument for the Ref function.

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html

Pseudo parameters:
AWS::Partition
Returns the partition that the resource is in. For standard AWS Regions, the partition is aws. For resources in other partitions, the partition is aws-partitionname. For example, the partition for resources in the China (Beijing and Ningxia) Region is aws-cn and the partition for resources in the AWS GovCloud (US-West) region is aws-us-gov.

AWS::Region
Returns a string representing the Region in which the encompassing resource is being created, such as us-west-2.

AWS::StackId
Returns the ID of the stack as specified with the aws cloudformation create-stack command, such as arn:aws:cloudformation:us-west-2:123456789012:stack/teststack/51af3dc0-da77-11e4-872e-1234567db123.

AWS::StackName
Returns the name of the stack as specified with the aws cloudformation create-stack command, such as teststack.

AWS::URLSuffix
Returns the suffix for a domain. The suffix is typically amazonaws.com, but might differ by Region. For example, the suffix for the China (Beijing) Region is amazonaws.com.cn.

https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_CloudFormation.ad3a4c93b4fdd3366da3da0de4fb084d89a5d761.png
https://aws.amazon.com/cloudformation/

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html

Using CloudFormation, you can create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.

A CloudFormation template has an optional Outputs section which declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe stack calls), or view on the AWS CloudFormation console. For example, you can output the S3 bucket name for a stack to make the bucket easier to find.

You can use the Export Output Values to export the name of the resource output for a cross-stack reference. For each AWS account, export names must be unique within a region. In this case, we would have a conflict within us-east-2.

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q20-i1.jpg
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/outputs-section-structure.html

!FindInMap [ MapName, TopLevelKey, SecondLevelKey ] - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. YAML Syntax for the full function name: Fn::FindInMap: [ MapName, TopLevelKey, SecondLevelKey ]

Short form of the above syntax is : !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]

Where,

MapName - Is the logical name of a mapping declared in the Mappings section that contains the keys and values. TopLevelKey - The top-level key name. Its value is a list of key-value pairs. SecondLevelKey - The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey.

Consider the following YAML template:

Mappings:
  RegionMap:
    us-east-1:
      HVM64: "ami-0ff8a91507f77f867"
      HVMG2: "ami-0a584ac55a7631c0c"
    us-west-1:
      HVM64: "ami-0bdb828fd58c52235"
      HVMG2: "ami-066ee5fd4a9ef77f1"
    eu-west-1:
      HVM64: "ami-047bb4163c506cd98"
      HVMG2: "ami-31c2f645"
    ap-southeast-1:
      HVM64: "ami-08569b978cc4dfa10"
      HVMG2: "ami-0be9df32ae9f92309"
    ap-northeast-1:
      HVM64: "ami-06cd52961ce9f0d85"
      HVMG2: "ami-053cdd503598e4a9d"
Resources:
  myEC2Instance:
    Type: "AWS::EC2::Instance"
    Properties:
      ImageId: !FindInMap
        - RegionMap
        - !Ref 'AWS::Region'
        - HVM64
      InstanceType: m1.small
The example template contains an AWS::EC2::Instance resource whose ImageId property is set by the FindInMap function.

MapName is set to the map of interest, "RegionMap" in this example. TopLevelKey is set to the region where the stack is created, which is determined by using the "AWS::Region" pseudo parameter. SecondLevelKey is set to the desired architecture, "HVM64" for this example.

FindInMap returns the AMI assigned to FindInMap. For a HVM64 instance in us-east-1, FindInMap would return "ami-0ff8a91507f77f867".

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-findinmap.html

The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, presence of "Transform" section indicates, the document is a SAM template.


Sample CloudFormation YAML template:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q63-i1.jpg
via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html

Parameters in YAML:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q63-i2.jpg
via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html

The intrinsic function Ref returns the value of the specified parameter or resource. When you specify a parameter's logical name, it returns the value of the parameter, when you specify a resource's logical name, it returns a value that you can typically use to refer to that resource such as a physical ID.

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html


All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you must delete Stack B as well as Stack C, before you delete Stack A.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q23-i1.jpg
 via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html

Parameter types enable CloudFormation to validate inputs earlier in the stack creation process.

CloudFormation currently supports the following parameter types:

String – A literal string
Number – An integer or float
List<Number> – An array of integers or floats
CommaDelimitedList – An array of literal strings that are separated by commas
AWS::EC2::KeyPair::KeyName – An Amazon EC2 key pair name
AWS::EC2::SecurityGroup::Id – A security group ID
AWS::EC2::Subnet::Id – A subnet ID
AWS::EC2::VPC::Id – A VPC ID
List<AWS::EC2::VPC::Id> – An array of VPC IDs
List<AWS::EC2::SecurityGroup::Id> – An array of security group IDs
List<AWS::EC2::Subnet::Id> – An array of subnet IDs

https://aws.amazon.com/blogs/devops/using-the-new-cloudformation-parameter-types/

-- SAM / -- AWS SAM / -- Amazon SAM

SAM supports the following resource types:

AWS::Serverless::Api
AWS::Serverless::Application
AWS::Serverless::Function
AWS::Serverless::HttpApi
AWS::Serverless::LayerVersion
AWS::Serverless::SimpleTable
AWS::Serverless::StateMachine
https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-resources-and-properties.html
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/transform-aws-serverless.html

Serverless Application Model (SAM) Templates include several major sections. Transform and Resources are the only required sections.

Please review this note for more details:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q5-i1.jpg
via - https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-template-anatomy.html

https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html

In order to use the output value of a template into another stack the following must be done:
Use 'Export' field in the Output section of the stack's template
To share information between stacks, export a stack's output values. Other stacks that are in the same AWS account and region can import the exported values.
To export a stack's output value, use the Export field in the Output section of the stack's template. To import those values, use the Fn::ImportValue function in the template for the other stacks.
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html



=======================================================================================================
Configuration as Code

-- AWS OpsWorks / -- OpsWorks / -- Amazon OpsWorks
-- AWS Config / -- Config
-- AWS Secrets Manager

AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the need to hardcode sensitive information in plain text. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.

Benefits of Secrets Manager:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q25-i1.jpg
via - https://aws.amazon.com/secrets-manager/

=======================================================================================================
Authentication and Authorization

-- MFA / -- Amazon MFA / -- AWS MFA

SMS-based MFA is available only for IAM users, you cannot use this type of MFA with the AWS account root user.
Hardware MFA device - This hardware device generates a six-digit numeric code. The user must type this code from the device on a second webpage during sign-in. Each MFA device assigned to a user must be unique. A user cannot type a code from another user's device to be authenticated. Can be used for root user authentication.

U2F security key - A device that you plug into a USB port on your computer. U2F is an open authentication standard hosted by the FIDO Alliance. When you enable a U2F security key, you sign in by entering your credentials and then tapping the device instead of manually entering a code.

Virtual MFA devices - A software app that runs on a phone or other device and emulates a physical device. The device generates a six-digit numeric code. The user must type a valid code from the device on a second webpage during sign-in. Each virtual MFA device assigned to a user must be unique. A user cannot type a code from another user's virtual MFA device to authenticate.

https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html

AWS will soon end support for SMS multi-factor authentication (MFA).

https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_sms.html

-- Amazon STS / -- AWS STS / -- STS

AWS Security Token Service (STS) - AWS Security Token Service (AWS STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users). However, it is not supported by API Gateway.
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html

Use decode-authorization-message to decode additional information about the authorization status of a request from an encoded message returned in response to an AWS request. If a user is not authorized to perform an action that was requested, the request returns a Client.UnauthorizedOperation response (an HTTP 403 response). The message is encoded because the details of the authorization status can constitute privileged information that the user who requested the operation should not see. To decode an authorization status message, a user must be granted permissions via an IAM policy to request the DecodeAuthorizationMessage (sts:DecodeAuthorizationMessage) action.
https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html

-- Cognito

Cognito Overview:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q58-i1.jpg
via - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html

After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own server-side resources, or to the Amazon API Gateway.

Amazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.

The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q53-i1.jpg
via - https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-with-identity-providers.html

"Cognito Identity Pools" - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.

"Cognito Sync" - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend. The client libraries cache data locally so your app can read and write data regardless of device connectivity status. When the device is online, you can synchronize data, and if you set up push sync, notify other devices immediately that an update is available.

https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-sync.html

Cognito User Pools - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.

We can use Cognito Authentication via Cognito User Pools with Application Load Balancer

Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules. The authenticate-cognito and authenticate-oidc action types are supported only with HTTPS listeners.
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i1.jpg

Auth when using ALB with CloudFront:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q4-i2.jpg
via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html

Difference between Cognito User Pools and Cognito Identity Pools:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q53-i2.jpg
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q10-i1.jpg
via - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html

https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html
https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html
https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/

Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.

https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor-view-data.html

Amazon Cognito identity pools (federated identities) enable you to create unique identities for your users and federate them with identity providers. With an identity pool, you can obtain temporary, limited-privilege AWS credentials to access other AWS services. Amazon Cognito identity pools support the following identity providers:

Public providers: Login with Amazon (Identity Pools), Facebook (Identity Pools), Google (Identity Pools), Sign in with Apple (Identity Pools).

Amazon Cognito User Pools
Open ID Connect Providers (Identity Pools)
SAML Identity Providers (Identity Pools)
Developer Authenticated Identities (Identity Pools)
Exam Alert:
Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q10-i1.jpg
via - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html

Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.

A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK.

Cognito is fully managed by AWS and works out of the box so it meets the requirements for the given use-case.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q2-i1.jpg
 via - https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html

 https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q17-i1.jpg
 via - https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-from-instance.html
https://aws.amazon.com/ec2/autoscaling/faqs/

As an alternative to using IAM roles and policies or Lambda authorizers, you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway. To use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO_USER_POOLS type and then configure an API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access token for the user, and then call the API method with one of the tokens, which are typically set to the request's Authorization header. The API call succeeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn't authorized to make the call because the client did not have credentials that could be authorized.
https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html

-- Other

Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. For security, most requests to AWS must be signed with an access key, which consists of an access key ID and secret access key. These two keys are commonly referred to as your security credentials
https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html

=======================================================================================================
Integration

-- Amazon SQS / -- SQS

Amazon SQS uses a visibility timeout to prevent other consumers from receiving and processing the same message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours.

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i1.jpg

For example, you have a message with a visibility timeout of 5 minutes. After 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes. You can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time. If you try to extend the visibility timeout beyond the maximum, your request is rejected. So, for the given use-case, the application can set the initial visibility timeout to 1 minute and then continue to update the ChangeMessageVisibility value if required.

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q5-i2.jpg
via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html

https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html

There are no message limits for storing in SQS, but 'in-flight messages' do have limits. Make sure to delete messages after you have processed them. There can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-limits.html

https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q59-i1.jpg
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html

ApproximateNumberOfMessagesVisible - This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. Hence, this metric does not work for target tracking.


Delay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q4-i1.jpg
 via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html

 SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.

 Quotas:
 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-queues.html
 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-messages.html
 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-policies.html

To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application requires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for storing your data.
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html

Amazon SQS leverages the AWS cloud to dynamically scale, based on demand. SQS scales elastically with your application so you don't have to worry about capacity planning and pre-provisioning. For most standard queues (depending on queue traffic and message backlog), there can be a maximum of approximately 120,000 inflight messages (received from a queue by a consumer, but not yet deleted from the queue).
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html

 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html
 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html
 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-queues.html
 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-messages.html
 https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/quotas-policies.html

https://aws.amazon.com/sqs/
https://aws.amazon.com/kinesis/data-firehose/

Good practise to minimize SQS costs:
Amazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.

Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds.

Exam Alert:

Please review the differences between Short Polling vs Long Polling:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i1.jpg
via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html

Consuming messages using long polling:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q1-i2.jpg
via - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html

Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. 

You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes.

Server-side encryption (SSE) lets you transmit sensitive data in encrypted queues. SSE protects the contents of messages in queues using keys managed in AWS Key Management Service (AWS KMS).
AWS KMS combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use Amazon SQS with AWS KMS, the data keys that encrypt your message data are also encrypted and stored with the data they protect.
You can choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS).
https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-server-side-encryption.html

Sample SNS-SQS Fanout message:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q35-i1.jpg +
via - https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html +

-- Amazon SNS / -- SNS


Amazon SNS enables message filtering and fanout to a large number of subscribers, including serverless functions, queues, and distributed systems. Additionally, Amazon SNS fans out notifications to end users via mobile push messages, SMS, and email.

How SNS Works:
https://d1.awsstatic.com/product-marketing/SNS/product-page-diagram_SNS_how-it-works_1.53a464980bf0d5a868b141e9a8b2acf12abc503f.png +
via - https://aws.amazon.com/sns/

https://docs.aws.amazon.com/sns/latest/dg/sns-lambda-as-subscriber.html +
https://docs.aws.amazon.com/sns/latest/dg/sns-firehose-as-subscriber.html +

-- Amazon Kinesis / -- Kinesis

AWS Kinesis Data Streams

Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.

Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering). Amazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.

For example, you have a billing application and an audit application that runs a few hours behind the billing application. By default, records of a stream are accessible for up to 24 hours from the time they are added to the stream. You can raise this limit to up to 7 days by enabling extended data retention or up to 365 days by enabling long-term data retention. For the given use-case, Amazon Kinesis Data Streams can be configured to store data for up to 7 days and you can run the audit application up to 7 days behind the billing application.

KDS provides the ability to consume records in the same order a few hours later
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q6-i1.jpg
via - https://aws.amazon.com/kinesis/data-streams/faqs/

KDS provides the ability for multiple applications to consume the same stream concurrently
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q9-i1.jpg
via - https://aws.amazon.com/kinesis/data-streams/faqs/
https://aws.amazon.com/kinesis/data-firehose/faqs/
https://aws.amazon.com/kinesis/data-analytics/faqs/
https://aws.amazon.com/kinesis/data-streams/
https://aws.amazon.com/kinesis/data-firehose/

Amazon Kinesis Data Streams is useful for rapidly moving data off data producers and then continuously processing the data, be it to transform the data before emitting to a data store, run real-time metrics and analytics, or derive more complex data streams for further processing. Kinesis data streams can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.

Kinesis Data stream overview:
https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png

The partition key is used by Kinesis Data Streams to distribute data across shards. Kinesis Data Streams segregates the data records that belong to a 
stream into multiple shards, using the partition key associated with each data record to determine the shard to which a given data record belongs.
You can also use metrics to determine which are your "hot" or "cold" shards, that is, shards that are receiving much more data, or much less data, than expected. You could then selectively split the hot shards to increase capacity for the hash keys that target those shards. Similarly, you could merge cold shards to make better use of their unused capacity.
https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html +
https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html +
https://docs.aws.amazon.com/streams/latest/dev/writing-with-agents.html +

Which of the following is the most optimal way of sending log data from the EC2 instances to KDS?
Install and configure Kinesis Agent on each of the instances

Kinesis Agent is a stand-alone Java software application that offers an easy way to collect and send data to Kinesis Data Streams. The agent continuously monitors a set of files and sends new data to your stream. The agent handles file rotation, checkpointing, and retry upon failures. It delivers all of your data in a reliable, timely, and simple manner. It also emits Amazon CloudWatch metrics to help you better monitor and troubleshoot the streaming process.
You can install the agent on Linux-based server environments such as web servers, log servers, and database servers. After installing the agent, configure it by specifying the files to monitor and the stream for the data. After the agent is configured, it durably collects data from the files and reliably sends it to the stream.
The agent can also pre-process the records parsed from monitored files before sending them to your stream. You can enable this feature by adding the dataProcessingOptions configuration setting to your file flow. One or more processing options can be added and they will be performed in the specified order.

AWS Kinesis Data Firehose - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.

Firehose overview:
https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png +

Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk. With Kinesis Data Firehose, you don't need to write applications or manage resources. You configure your data producers to send data to Kinesis Data Firehose, and it automatically delivers the data to the destination that you specified.

Amazon Elasticsearch Service (Amazon ES) with optionally backing up data to Amazon S3 - Amazon ES is a supported destination type for Kinesis Firehose. Streaming data is delivered to your Amazon ES cluster, and can optionally be backed up to your S3 bucket concurrently.

Data Flow for ES:
https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-es.png +
via - https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html +

Amazon Simple Storage Service (Amazon S3) as a direct Firehose destination - For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.

Data Flow for S3:
https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-s3.png +

Amazon Redshift with Amazon S3 - For Amazon Redshift destinations, streaming data is delivered to your S3 bucket first. Kinesis Data Firehose then issues an Amazon Redshift COPY command to load data from your S3 bucket to your Amazon Redshift cluster. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.

Data Flow for Redshift:
https://docs.aws.amazon.com/firehose/latest/dev/images/fh-flow-rs.png +

AWS Kinesis Data Analytics - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time. You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data.

Amazon SQS - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.

Exam alert:

Please remember that Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications.

Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it's retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. Also, the HTTPS protocol ensures that data inflight is encrypted as well.

When a ProvisionedThroughputExceeded exception is thrown we can do the following to fix it:
- increase the number of shards within your data streams to provide enough capacity
- configure the data producer to retry with an exponential backoff

The capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception.

https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q6-i1.jpg

SNS and SQS can be used to create a fanout messaging scenario in which messages are "pushed" to multiple subscribers, which eliminates the need to periodically check or poll for updates and enables parallel asynchronous processing of the message by the subscribers. SQS can allow for later re-processing and dead letter queues. This is called the fan-out pattern.
https://aws.amazon.com/getting-started/hands-on/send-fanout-event-notifications/

Stream data to an RDS with Kinesis:
https://aws.amazon.com/blogs/database/streaming-changes-in-a-database-with-amazon-kinesis/

-- CloudFront / -- AWS CloudFront


=======================================================================================================
Serverless Compute

https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png
https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png

-- AWS Lambda / -- Lambda / -- Amazon Lambda

When a Lambda function is exceeding its concurrency limit we need to set up reserved concurrency limit
so that it throttles if it goes above a certain concurrency limit.
Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.

To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.

How lambda concurrency works:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q6-i1.jpg
via - https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved

You should use provisioned concurrency to enable your function to scale without fluctuations in latency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency. Provisioned concurrency is not used to limit the maximum concurrency for a given Lambda function

https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html#configuration-concurrency-reserved
https://aws.amazon.com/blogs/networking-and-content-delivery/lambda-functions-as-targets-for-application-load-balancers/

You can use versions to manage the deployment of your AWS Lambda functions. For example, you can publish a new version of a function for beta testing without affecting users of the stable production version. You can change the function code and settings only on the unpublished version of a function. When you publish a version, the code and most of the settings are locked to ensure a consistent experience for users of that version.

You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. You can use routing configuration on an alias to send a portion of traffic to a Lambda function version. For example, you can reduce the risk of deploying a new version by configuring the alias to send most of the traffic to the existing version, and only a small percentage of traffic to the new version.

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i1.jpg
 via - https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html

 https://media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i2.jpg

https://media.datacumulus.com/aws-dva-pt/assets/pt1-q13-i3.jpg
via - https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html

API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.

https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html
https://docs.aws.amazon.com/apigateway/latest/developerguide/stage-variables.html

To set up lambda environments you need:
- lambda aliases
- stage variables

Stage Variables
Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates. With deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.
For example, your API can pass a GET request as an HTTP proxy to the backend web host (for example, http://example.com). In this case, the backend web host is configured in a stage variable so that when developers call your production endpoint, API Gateway calls example.com. When you call your beta endpoint, API Gateway uses the value configured in the stage variable for the beta stage and calls a different web host (for example, beta.example.com).

Lambda Aliases
A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN.
Lambda Aliases allow you to create a "mutable" Lambda version that points to whatever version you want in the backend. This allows you to have a "dev", "test", prod" Lambda alias that can remain stable over time.

https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-override-request-response-parameters.html

In the AWS Lambda resource model, you choose the amount of memory you want for your function which allocates proportional CPU power and other resources. This means you will have access to more compute power when you choose one of the new larger settings. You can set your memory in 64MB increments from 128MB to 3008MB. You access these settings when you create a function or update its configuration. The settings are available using the AWS Management Console, AWS CLI, or SDKs.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q15-i1.jpg
 via - https://docs.aws.amazon.com/lambda/latest/dg/configuration-console.html

Concurrency is the number of requests that a Lambda function is serving at any given time. If a Lambda function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency.

Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, use provisioned concurrency. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.

You can configure Application Auto Scaling to manage provisioned concurrency on a schedule or based on utilization. Use scheduled scaling to increase provisioned concurrency in anticipation of peak traffic. To increase provisioned concurrency automatically as needed, use the Application Auto Scaling API to register a target and create a scaling policy.

Please see this note for more details on provisioned concurrency:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q3-i1.jpg +
via - https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html +

Configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule - To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency. When a function has reserved concurrency, no other function can use that concurrency. More importantly, reserved concurrency also limits the maximum concurrency for the function, and applies to the function as a whole, including versions and aliases.

You cannot configure Application Auto Scaling to manage Lambda reserved concurrency on a schedule.
https://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/

Lambdas with scheduled events:
https://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html

A Lambda alias is like a pointer to a specific Lambda function version. You can create one or more aliases for your AWS Lambda function. Users can access the function version using the alias ARN. An alias can only point to a function version, not to another alias. You can update an alias to point to a new version of the function. Event sources such as Amazon S3 invoke your Lambda function. These event sources maintain a mapping that identifies the function to invoke when events occur. If you specify a Lambda function alias in the mapping configuration, you don't need to update the mapping when the function version changes. This is the right choice for the current requirement.

https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html
https://docs.aws.amazon.com/lambda/latest/dg/configuration-tags.html

The Lambda runtime makes environment variables available to your code and sets additional environment variables that contain information about the function and invocation request. The total size of all environment variables doesn't exceed 4 KB. There is no limit defined on the number of variables that can be used.
https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html

 AWS best practices for Lambda suggest taking advantage of execution context reuse to improve the performance of your functions. Initialize SDK clients and database connections outside of the function handler, and cache static assets locally in the /tmp directory. Subsequent invocations processed by the same instance of your function can reuse these resources. This saves execution time and cost. To avoid potential data leaks across invocations, don’t use the execution context to store user data, events, or other information with security implications.

When you invoke a Lambda function, two types of error can occur. Invocation errors occur when the invocation request is rejected before your function receives it. Function errors occur when your function's code or runtime returns an error. Depending on the type of error, the type of invocation, and the client or service that invokes the function, the retry behavior, and the strategy for managing errors varies.
Lambda function failures are commonly caused by:
Permissions issues Code issues Network issues Throttling Invoke API 500 and 502 errors
You can insert logging statements into your code to help you validate that your code is working as expected. Lambda automatically integrates with CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function, which is named /aws/lambda/<function name>.
Please see this note for more details:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q9-i1.jpg
via - https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html

Which of the following should you implement to connect AWS Lambda function to an RDS instance?
Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS - You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration. This is the right way of giving RDS access to Lambda.

Lambda VPC Config:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q39-i1.jpg
via - https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html

You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. 

https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html
https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html
https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html
https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/

which are the use cases, wherein AWS Lambda will add a message into a DLQ after being processed?
1. The Lambda function invocation is asynchronous - When an asynchronous invocation event exceeds the maximum age or fails all retry attempts, Lambda discards it. Or sends it to dead-letter queue if you have configured one.
2. The event fails all processing attempt - A dead-letter queue acts the same as an on-failure destination in that it is used when an event fails all processing attempts or expires without being processed.
https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html

https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html
https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html

-- Amazon Step Functions / -- Step Functions

How step functions work:
https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png
via - https://aws.amazon.com/step-functions/

Please see this note for a simple example of a State Machine:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q8-i1.jpg
via - https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-state-machine-structure.html

Standard Workflows on AWS Step Functions are more suitable for long-running, durable, and auditable workflows where repeating workflow steps is expensive (e.g., restarting a long-running media transcode) or harmful (e.g., charging a credit card twice). Example workloads include training and deploying machine learning models, report generation, billing, credit card processing, and ordering and fulfillment processes. Step functions also support any human approval steps.

You should use Express Workflows for workloads with high event rates and short durations. Express Workflows support event rates of more than 100,000 per second.

https://aws.amazon.com/step-functions/features/ +
https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/ +

-- AppSync - managed service that uses GraphQL.
-- API Gateway / -- AWS API Gateway / -- Amazon API Gateway

https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html

API Gateway REST APIs: A collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods that have unique HTTP verbs supported by API Gateway.
API Gateway HTTP API: A collection of routes and methods that are integrated with backend HTTP endpoints or Lambda functions. You can deploy this collection in one or more stages. Each route can expose one or more API methods that have unique HTTP verbs supported by API Gateway.
AWS Lambda, AWS Identity and Access Management (IAM) and Amazon Cognito services are all available as HTTP APIs. AWS Web Application Firewall (AWS WAF) however, is only available in REST APIs.

Choosing between HTTP API or REST API:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt6-q26-i1.jpg
via - https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html

A usage plan specifies who can access one or more deployed API stages and methods—and also how much and how fast they can access them. The plan uses API keys to identify API clients and meters access to the associated API stages for each key.

You can configure usage plans and API keys to allow customers to access selected APIs at agreed-upon request rates and quotas that meet their business requirements and budget constraints.


Overview of API Gateway Usage Plans and API keys:
https://media.datacumulus.com/aws-dva-pt/assets/pt1-q18-i1.jpg
via - https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html

A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. Every time you update an API, you must redeploy the API to an existing stage or to a new stage. Updating an API includes modifying routes, methods, integrations, authorizers, and anything else other than stage settings.

https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html
https://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html
https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html +

API Gateway supports the following mechanisms for authentication and authorization:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt2-q55-i1.jpg +
via - https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html +
https://docs.aws.amazon.com/STS/latest/APIReference/welcome.html +

Mock - This type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the integration setup without incurring charges for using the backend and to enable collaborative development of an API.
It is also used to return CORS-related headers to ensure that the API method permits CORS access. In fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock integration.

AWS_PROXY - This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined integration setup. This integration relies on direct interactions between the client and the integrated Lambda function.

HTTP_PROXY - The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not set the integration request or the integration response. API Gateway passes the incoming request from the client to the HTTP endpoint and passes the outgoing response from the HTTP endpoint to the client.

HTTP - This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and integration response. You must set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html +

After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages are identified by the API ID and stage name. They're included in the URL that you use to invoke the API. Each stage is a named reference to a deployment of the API and is made available for client applications to call.

Stages enable robust version control of your API. In our current use-case, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done by redeploying the API to the prod stage or updating a stage variable value from the stage name of test to that of prod.
 it is not possible to deploy an API without choosing a stage.

For each stage, you can optimize API performance by adjusting the default account-level request throttling limits and enabling API caching. And these settings can be changed/updated at any time.
https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html +

You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.

https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html +

 A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping templates help format/structure the data in a way that it is easily readable, unlike a server response that might always be easy to ready. Mapping Templates do not help in latency issues of the APIs.

Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. When your API's resources receive requests from a domain other than the API's own domain and you want to restrict servicing these requests, you must disable cross-origin resource sharing (CORS) for selected methods on the resource.

https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-protect.html +
https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-throttling.html +
https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-mutual-tls.html +
https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#invalidate-method-caching +

ALB routing options:
Path based

You can create a listener with rules to forward requests based on the URL path. This is known as path-based routing. If you are running microservices, you can route traffic to multiple back-end services using path-based routing. For example, you can route general requests to one target group and request to render images to another target group.

This path-based routing allows you to route requests to, for example, /api to one set of servers (also known as target groups) and /mobile to another set. Segmenting your traffic in this way gives you the ability to control the processing environment for each category of requests. Perhaps /api requests are best processed on Compute Optimized instances, while /mobile requests are best handled by Memory Optimized instances.

Host based

You can create Application Load Balancer rules that route incoming traffic based on the domain name specified in the Host header. Requests to api.example.com can be sent to one target group, requests to mobile.example.com to another, and all others (by way of a default rule) can be sent to a third. You can also create rules that combine host-based routing and path-based routing. This would allow you to route requests to api.example.com/production and api.example.com/sandbox to distinct target groups.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q54-i1.jpg
via - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#rule-condition-types
https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/

=======================================================================================================
Serverless Applications

=======================================================================================================
Serverless Application Patterns

=======================================================================================================
Monitoring and Troubleshooting

https://aws.amazon.com/config/faq/ +
https://aws.amazon.com/cloudwatch/
https://aws.amazon.com/cloudtrail/

-- AWS CloudTrail / -- CloudTrail
How cloudtrail works:
https://d1.awsstatic.com/product-marketing/CloudTrail/Product-Page-Diagram-AWSX-CloudTrail_How-it-Works.d2f51f6e3ec3ea3b33d0c48d472f0e0b59b46e59.png +
via - https://aws.amazon.com/cloudtrail/
https://aws.amazon.com/cloudtrail/faqs/ +

Exam Alert:
You may see scenario-based questions asking you to select one of CloudWatch vs CloudTrail vs Config. Just remember this thumb rule -
Think resource performance monitoring, events, and alerts; think CloudWatch.
Think account-specific activity and audit; think CloudTrail.
Think resource-specific history, audit, and compliance; think Config.

AWS CloudTrail can be used to track who and when retrieved a var from SSM parameter store.

Organization trail:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q32-i1.jpg +
via - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html +
Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket.
AWS CloudTrail supports Amazon S3 Data Events, apart from bucket Events. You can record all API actions on S3 Objects and receive detailed information such as the AWS account of the caller, IAM user role of the caller, time of the API call, IP address of the API, and other details. All events are delivered to an S3 bucket and CloudWatch Events, allowing you to take programmatic actions on the events.
Organization trails must be created in the master account, and when specified as applying to an organization, are automatically applied to all member accounts in the organization. Member accounts will be able to see the organization trail, but cannot modify or delete it. By default, member accounts will not have access to the log files for the organization trail in the Amazon S3 bucket.

https://aws.amazon.com/about-aws/whats-new/2016/11/aws-cloudtrail-supports-s3-data-events/ +

By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE).

For CloudTrail to work with S3 buckets and the bucket owner to receive logs, he must be object owner, as well.
Otherwise, the bucket owner must get permissions, through the object ACL, for the same object API to get the same object-access API logs.
https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html#cloudtrail-object-level-crossaccount

-- AWS CloudWatch / -- CloudWatch / -- Amazon CloudWatch

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch-new.html +
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html +

As part of basic monitoring, Amazon EC2 sends metric data to CloudWatch in 5-minute periods. To send metric data for your instance to CloudWatch in 1-minute periods, you can enable detailed monitoring on the instance, however, this comes at an additional cost.
The basic monitoring data is available automatically in a 5-minute interval and detailed monitoring data is available in a 1-minute interval.

You can publish your own metrics to CloudWatch using the AWS CLI or an API. Metrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.

High-resolution metrics can give you more immediate insight into your application's sub-minute activity. But, every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges.
https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html

Using high-resolution custom metric, your applications can publish metrics to CloudWatch with 1-second resolution. You can watch the metrics scroll across your screen seconds after they are published and you can set up high-resolution CloudWatch Alarms that evaluate as frequently as every 10 seconds. You can alert with High-Resolution Alarms, as frequently as 10-second periods. High-Resolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1-minute alarms.
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q11-i1.jpg +
via - https://aws.amazon.com/blogs/aws/new-high-resolution-custom-metrics-and-alarms-for-amazon-cloudwatch/ +

The Amazon CloudWatch Monitoring Scripts for Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instances demonstrate how to produce and consume Amazon CloudWatch custom metrics. These Perl scripts comprise a fully functional example that reports memory, swap, and disk space utilization metrics for a Linux instance. You can set a cron schedule for metrics reported to CloudWatch and report memory utilization to CloudWatch every x minutes.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html +

Encrypt logs using KMS:
https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html +
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q10-i1.jpg +

By default, your instance is enabled for basic monitoring and Amazon EC2 sends metric data to CloudWatch in 5-minute periods. You can optionally enable detailed monitoring. After you enable detailed monitoring, the Amazon EC2 console displays monitoring graphs with a 1-minute period for the instance.
The following describes the data interval and charge for basic and detailed monitoring for instances.
- Basic monitoring Data is available automatically in 5-minute periods at no charge.
- Detailed monitoring Data is available in 1-minute periods for an additional charge.
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-cloudwatch.html

-- AWS CloudWatch metrics and Amazon EventBridge
-- Amazon X-Ray / -- X-ray / -- AWS XRay / -- Amazon XRay

AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.

How X-Ray Works:
https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png +
via - https://aws.amazon.com/xray/

You can also customize the X-Ray sampling rules:  
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i1.jpg +
via - https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html +

Filter expression syntax:
https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt3-q13-i2.jpg +
via - https://docs.aws.amazon.com/xray/latest/devguide/xray-console-filters.html +

You can use AWS X-Ray to visualize the components of your application, identify performance bottlenecks, and troubleshoot requests that resulted in an error. Your Lambda functions send trace data to X-Ray, and X-Ray processes the data to generate a service map and searchable trace summaries. This is a useful tool for troubleshooting. But, for the current use-case, we already know the bottleneck that needs to be fixed and that is the context reuse.

X-Ray overview:
https://docs.aws.amazon.com/xray/latest/devguide/images/architecture-dataflow.png +
via - https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html +

EC2 X-Ray Daemon - The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon logs could help with figuring out the problem.
The X-Ray daemon uses the AWS SDK to upload trace data to X-Ray, and it needs AWS credentials with permission to do that. On Amazon EC2, the daemon uses the instance's instance profile role automatically. Eliminates API permission issues (in case the role doesn't have IAM permissions to write data to the X-Ray service)

You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.

By customizing sampling rules, you can control the amount of data that you record, and modify sampling behavior on the fly without modifying or redeploying your code. Sampling rules tell the X-Ray SDK how many requests to record for a set of criteria. By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.

https://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html +
https://docs.aws.amazon.com/xray/latest/devguide/xray-console-deeplinks.html +

=======================================================================================================
Optimization

=======================================================================================================
Other

-- Amazon Systems Manager

AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources.
https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html +


-- AWS SSM Parameter Store / -- SSM / -- Parameter store / -- SSM Parameter store / -- Systems Manager Parameter store

With AWS Systems Manager Parameter Store, you can create SecureString parameters, which are parameters that have a plaintext parameter name and an encrypted parameter value. Parameter Store uses AWS KMS to encrypt and decrypt the parameter values of Secure String parameters. Also, if you are using customer-managed CMKs, you can use IAM policies and key policies to manage to encrypt and decrypt permissions. To retrieve the decrypted value you only need to do one API call.

AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values.

AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. For the given use-case, as the DevOps team does not want to re-deploy the application every time there are configuration changes, so they can use the SSM Parameter Store to store the configuration externally.

https://assets-pt.media.datacumulus.com/aws-dva-pt/assets/pt4-q47-i1.jpg +
 via - https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html +

-- AWS IoT Core service

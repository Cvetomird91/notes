AWS Cloud API

=======================================================================================================
Compute and Networking

Regions - a set of datacenters
Availability zone - each region has a set of availability zones (from 2 to 6, the usual is 3)

ap-southeast-2 - Syndney region
Sydney region availability zones:
ap-southeast-2a
ap-southeast-2b
ap-southeast-2c

Availability zones are a discrete data center with redundant power, networking and connectivity.
They are separate from each other, so that they're isolated from disasters.

IAM federation - big enterprises intergrate their own repository of users with IAM

Basic good practices for IAM roles:
- One IAM User per physical role
- One IAM Role per Application
- IAM credentials should never be shared
- Never write IAM credentials in code
- Never use ROOT IAM credentials in an application

SSH troubleshooting:
1) There's a connection timeout
This is a security group issue. Any timeout (not just for SSH) is related to security groups or a
firewall. Ensure your security group looks like this and correctly assigned to your EC2 instance.

Security groups can be reused and assigned to multiple EC2 instances.
They're locked down to a region/VPC combination.

A security group misconfiguration is usually responsible for time-outs.
A "connection refused" error is usually caused by an application error - the security group
in this case is allowing traffic properly.

All inbound traffic is blocked by default.
All outbound traffic is allowed by default.

We can reference security groups within other security groups.

Types of IPs in EC2:
- public IP
- private IP
- elastic IP

When you stop and then start an EC2 instance, it can change its public IP.
If you need to have a fixed public IP for your instance, you need an Elastic IP.
You can attach 1 elastic IP at a time.

You can have at most 5 elastic ips. This can be increased after reaching out to AWS support.

Overusing elastic IPs is bad architecture. It's best to use random public IPs and remap a DNS record
to then.

We can mask the failure of an instance by switching the elastic ip to another instance but
overusing that pattern is bad practice.

EC2 instance launch types:
- on demand instances: short workload, predictable pricing - billing per second, after the
	first minute. Highest cost but no upfront payment, no long term commitment. Recommended
	for short-term and un-interrupted workloads, where you can't predict how the application will
	behave
- reserved (minimum 1 year) - suitable for a workload we'll be sure we'll use for a minimal amount of
time - can have up to 75% discount compared to On-demand. You need to pay upfront for what you use
	with long term commitment. Reservation period can be from 1 to 3 years. You reserve a specific
	instance type. A server for database is a proper example for this type of instance
	- reserved instances - long workloads
	- convertible reserved instance - long workloads with flexible instances. We can change the
		EC2 instance type here. Up to 54% discount.
	- scheduled reserved instances: example - every Thursday between 3 and 6 pm. They can get
	launched periodically within a time period we specify.
- spot instances - short workload, cheap, losing the instance isn't fatal (less reliable)
	Can get a discount of up to 90% compared to on-demand.
	instances you can use any time if the max price you can pay from them is less than the
	current spot time.
	Useful if your workload is resilient to failure like:
	- batch jobs
	- data analysis
	- image processing
	Not recommended for critical purposes
- dedicated instances - no other customers will share the physical hardware
- dedicated hosts - book an entire physical server, control instance placement. Allocated for
your account for a 3 year period reservation

You do not pay for an instance if the instance is stopped.

ENI (Elastic Network interface):
- can have primary private IPv4, one or more secondary IPv4
- can have one elastic IP (IPv4) per private IPv4
- can have one public IPv4
- can have one or more security groups
- can have a MAC address
- we can create ENIs independently and move them inbetween EC2 instaces for failover (in the same
availability zone)
- ENIs are bound to a specific availability zone

AMIs are built for a specific AWS region!

AMIs have the following characteristics:
- RAM
- CPU
- I/O (disk performance)
- network
- gpu

=======================================================================================================
ELB - Elastic Load balancer

ELB seamlessly handles failures of downstream instances.
It does regular health checks on your EC2 instances.
It provides SSL termination for your instances.
It enforces stickiness with cookies.
High availability across zones.
It separates public from private traffic.

There are 4 types of load balancers:
- classic load balancer (v1) - http, https, tcp
- application load balancer (v2 - new generation) - http, https, websocket, http/2
- network load balancer - tcp, tls, udp
- gateway load balancer (not on the exam)

ELBs can be internal or external.

Load balancer 503 - load balancer is at max capacity or no target is registered

ALB supports routing based on different target groups.
ALBs can stand in front of lambda functions

NLBs are used when you want extreme performance and you want to use a protocol different than HTTP(S)
It can also be useful when you want a static IP assigned.

Load balancer stickiness - the same client is always redirected to the same actual instance behind
the load balancer. This feature is available for CLB and ALB.
The "cookie" used for stickiness has an expiration date you control.
Stickiness may bring imbalance to load balancing overloading some of the downstream instances.
Stickiness for ALB is configured at the target group level.
For CLBs stickiness is set at the load balancer level.

With cross-zone load balancing each load balance instance distributes evenly the calls across all
registered instances in all AZs.
Otherwise, it'll distribute them only to the instances in its own availability zone.
This is disabled by default in CLB. No additional charges for it if enabled.
For ALB this is the default behaviour and cannot be disabled.
For NLB this is disabled by default. You'll pay additional charges if enabled.

Load balancers use an X.509 certificate
ACM - AWS certificate manager - this is where we manage SSL certificates

SNI - ability to have multiple SSL certificates on the same host. This is achieved by the
client specifying which hostname of a target server it wants to reach.
SNI works with Cloudfront, ALB and NLB. Does not work with CLB.

Connection draining (aka Deregistration delay for target groups in ALBs and NLBs) - this is the
time to complete "in-flight requests" while the instance is de-registering or unhealthy.
The ELB stops sending new requests to the instance being drained.
The ELB won't start new connections to an instance being drained and it'll give it 300 seconds by
default to complete the requests
It can be between 1 and 3600 seconds.
Can be disabled by setting it to 1 second.

Auto Scaling Group parameters:
- minimum size
- actual size / desired capacity
- maximum size - how many instances can be added at scale out
scale out - add instances
scale in - remove instances
Auto Scaling Alarms - they determine when to trigger a scale in / out. It is possible to scale
an ASG based on CloudWatch alarms. CloudWatch metrics are computed overall for all ASG instances.
We can also use metrics directly from the EC2 instances.
We can also use custom metrics sent to CloudWatch from our EC2 instances.
ASGs use Launch configurations or Launch Templates (newer)
To Update an ASG, you must provide a new launch configuration / launch template.
IAM roles attached to an ASG will get assigned to EC2 instances
ASG are free. You pay for the underlying resources being launched.

ASG scaling policies:
- target tracking scaling
- simple / step scaling policy
- scheduled actions

ASG Cooldowns - the cooldown period ensures that your ASG doesn't scale in or out additional instances
before the previous scaling takes effect

=======================================================================================================
Storage

-- Amazon S3
Versioning is a means of keeping multiple variants of an object in the same bucket. You can use
versioning to preserve, retrieve, and restore every version of every object stored in your Amazon
S3 bucket. With versioning you can easily recover from both unintended user actions and application
failures. The following are version states of buckets:
- Versioning suspended
- Versioning disabled
- Versioning enabled

The following are a deciding factor when choosing an AWS Region for your bucket:
- Cost - prices are different between regions
- Latency
- Regulatory requirements - different regions may have different legal requirements

Pre-signed URLs allow you to grant time-limited permission to download objects from an Amazon S3
bucket.

If data must be encrypted before being sent to Amazon S3, client-side encryption must be used.

Amazon S3 supports server access logging for buckets that tracks access time and who accessed the
bucket.

Amazon S3 bucket policies can specify a request IP range, an AWS account and a prefix for objects
that can be accessed.

Instance store = ephemeral storage - it gets destroyed when the instance is destroyed.
It is physically attached to the EC2 instance, while EBS is a network drive.
It has better performance and good for cache / scratch data.
It's not an option if you want to persist data inbetween instance terminations
it survives reboots but not instance terminations
It cannot increase in size, as opposed to EBS
There's a risk of data loss in instance store if the physical hard disk fails.

-- Amazon Elastic Block Store (EBS)

Amazon snapshot EBS data is loaded lazily.
EBS volumes are scoped within a specific availability zone. You cannot attach an EBS volume from
us-east-1a to us-east-1b
An EBS volume can be accessed by a single EC2 instance, as opposed to EFS volumes.
To use an EBS volume in another availability zone we need to create a snapshot of it
and restore it in another AZ.

EBS backups use IO and you shouldn't run them while your application is handling a lot
of traffic.

EBS volume types:
- GP2 (SSD) - recommended for most workloads, system boot volumes and virtual desktops,
	low-latency interactive apps, development and test environments
	Size is between 1GiB to 16TiB
	A small gp2 volume can burst IOPS to 3000
	Max IOPS is 16k
- IO1 (SSD) - critical business applications that require more than 16000 IOPS per volume
	Suitable for large database workloads (Oracle SQL, MS SQL etc)
	4GiB - 16TiB
	Max IOPS are 32000
- ST1 (HDD) - good for big data, data warehouses, log processing
	good for streaming workloads that require fast throughput at a low price
	Cannot be a boot volume
	500 GiB - 16 TiB
	Max IOPS is 500
- SC1 (HDD) - the cheapest option. Good for less frequently accessed data
	500GiB - 16TiB
	Max IOPS is 250
	Max throughput is 250 MiB/s
EBS volume types are characterized by Size and Throughput (I/O Ops per second)
Only GP2 and IO1 can be used as boot volumes

-- Amazon Elastic File System (EFS)

A managed NFS that could be mounted on many EC2 instances at the same time
across many different availability zones (EBS is availability zone dependent).
It's pay per use and is 3x more expencive than gp2 ebs volumes.

To use EFS you need security groups.
It works with Linux-based AMIs only.

You can use KMS keys to encrypt the file system.
It scales automatically. Users don't plan the capacity.

Amazon EFS supports one to thousands of Amazon EC2 instances conneting to a file system concurrently.

The Max I/O performance mode of EFS is optimized for applications where tens, hundreds, or
thousands of EC2 instances are accessing the file system.

Performance modes:
- general purpose
- max I/O - good for big data

Therea are different storage tiers in EFS - lifecycle management. We can move files
between EFS volumes after a number of days:
- standard
- infrequent access (EFS-IA): cost to retrieve files, lower price to store

-- Amazon Glacier
Glacier can be used as a standalone service and as an Amazon S3 storage class.


=======================================================================================================
Databases

-- AWS RDS

1 RCU - One strongly consistent read per second of 4KB.
In order to determine the number of RCUs for a number of reads per seconds we need to determine the
number of complete chunks
and then multiply the number of chunks with the count of complete reads.
E.g. to calculate the RCUs of 25 strongly consistent reads per seconds of 15 KB (those are 4 complete
chunks of 4KB - 4 x 4 = 16KB),
we need 25 x 4 = 100 RCUs

To calclulate eventually consistent reads we need to divide the final result by 2:
(25 x 4) / 2 = 50 RCUs

WCU - write capacity unit
1 WCU = 1 write per second of 1KB (1024 bytes).
How to calculate for example for 100 writes per second of 512 bytes:
512 bytes uses one complete chunk of 1 KB
(512/1024 = 0.5, rounded up to 1).

Amazon RDS manages for the developer the following things out of the box:
- database software installation and patching
- hardware provisioning
- backups
You are still responsible for managing the database settings reuired by the application.

An existing Amazon RDS instance is deleted if the environment is deleted. There is no auto-retention
of the database instance. You must create a snapshot to retain the data and to restore the database.

You cannot SSH into the EC2 instances hosting AWS RDS instances. This is a managed service
RDS EC2 instance storage is backed by EBS (gp2 or io1)

RDS backups are enabled by default. There are daily full backups in a maintenance window.

Transaction logs are backed-up by RDS every 5 minutes

RDS DB snapshots differ from backups with being triggered by the user.

We can create up to 5 read replicas in the same or other AZ or other region.
Async replication is performed when we enable read replicas.
This means that the reads are "eventually consistent".

Each replica can be promoted to a stand-alone DB instance.

Applications need to update their connection string to leverage read replicas.

In AWS There's a network cost when data goes from one AZ to another.
To reduce the network cost you can have a read replica in the same AZ (if the usecase allows it).
It'll be free in such case.

A disaster recovery instance is a database "copy" that receives sync replication from the
main database. When the main database goes down the replcia starts to handle
read and write queries. The DNS record gets automatically redirected to it.

Read replicas can be used in such a way as well for disaster recovery.

AWS RDS supports IAM authentication - an IAM user could be created as a database user (it
works for MySQL and PostgreSQL).

AWS RDS supports encryption with AWS KMS - AES-256 encryption.
It needs to be defined at launch time.

If the master is not encrypted, the read replicas cannot be encrypted.

it also supports in-flight encryption using SSL.

To enforce SSL:
in postgresql: rds.force_ssl=1 in the AWS RDS console (Parameter groups)
in mysql:
GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SSL;

Snapshots of un-encrypted RDS databases are un-encrypted.
Snapshots of encrypted RDS databases are encrypted.

To encrypt an un-encrypted RDS database:
1. create a snapshot of the un-encrypted database
2. copy the snapshot and enable encryption for the snapshot
3. restore the database from the encrypted snapshot
4. migrate applications to the new database, and delete the old one

RDS databases are usually deployed within a private subnet.
RDS leverages security groups to expose the database to the internet.

Transparent Data Encryption (TDE) - available only for MS SQL and Oracle SQL.
This feature encrypts data at rest - db files both on the hard drive and backup.

-- AWS Aurora

AWS Aurora is compatible with both PostgreSQL and MySQL clients.
AWS claim 5x times improvement over MySQL performance and 3x over PostgreSQL.

The Aurora size can be between 10GB and come up to 64GB.

Aurora can have up to 15 replicas.
Aurora is HA native.
Aurora costs 20% more than other RDS engines.

Aurora stores copies of your data as read replicas as twice as the AZs in which
they're placed.
E.g. 6 copies of your data across 3 AZ.
4 copies out of 6 are needed for writing.
3 copies out of 6 are needed for reads.
Aurora does some peer to peer self-healing.
Storage is striped across 100s of volumes.
One Aurora instance makes writes (master).
If the master is down, a new one is promoted in less than 30 seconds.
Aurora supports cross-region replication.
Aurora provides a "writer endpoint" - a DNS record always pointing to the master.
Aurora provides a "reader endpoint" - a DNS record pointing to all read replicas.
Aurora shared storage volumes are auto expanding from 10G to 64TB.

Aurora backtrack - a feature which allows you to restore the database to any point in time.

Aurora serverless - automated database instantiation and auto-scaling based on actual usage

Aurora global database - 1 primary region and up to 5 secondary read-only regions.

up to 16 read replicas per secondary region.

-- AWS ElastiCache

ElastiCache for Memcached doesn't support a high-availability configuration.
Only ElastiCache Redis could run for in a high-availaiblity configuration.
ElastiCache Redis supports data durability using AOF persistence (append only file).

ElastiCache Memcached - multi-node for partitioning of data (sharding)
It's non-persistent (no AOF).

ElastiCache Redis supports backup and restore with AOF, opposed to ElastiCache memcached.
Memcached has multi-threaded architecture.

Caching strategies:
- Lazy loading / Cache aside / Lazy population - update cache on retrieval if an item is missing there
- Write-through cache - update the cache after the database is updated

Both are usually combined.

Cache churn - a data is present in cache that will never be accessed

Evicting cache:
- explicitly delete the item
- item is evicted because there's not enough space in cache (LRU)
- the TTL of the item expires

If too many evictions happen due to memory, you should scale up or out

Setting a TTL is good practice, except when you're using write-through

-- Other

Amazon Redshift - managed data warehouse solution
	S3DistCp - Redshift feature that allows it to copy data from S3 to the Redshift cluster

Amazon Neptune - NoSQL graph database for highly connected datasets.

-- Amazon DynamoDB

In DynamoDB scans are less efficient than queries.
In DynamoDB you can create local secondary indexes only when you are creating the table.
Server-side encryption provided by the DynamoDB service is a recommended way to protect data at rest
stored in DynamoDB.
Each record in DynamoDB Streams appears only once in a stream.
400KB - this is the maximum DynamoDB item size limit.
QueryTable is the DynamoDB operation used to find items based on primary key.
UpdateTable is the DynamoDB operation used to modify the provisioned throughput settings, global
secondary indexes, or DynamoDB Streams settings for a given table.
Scan is the DynamoDB operation used to read every item in a table.
The lifetime of data in a DynamoDB stream is 24 hours.

These are the following three operations used to read data in DynamoDB:
- GetItem
- Scan
- Query

Amazon DynamoDB Accelerator (DAX) - write-through and read-through caching service for DynamoDB.
=======================================================================================================
Route 53 and DNS

Alias - hostname mapped to an AWS resource

Route 53 supports load balancing through DNS - client load balancing
Route 53 supports health checks.

Route 53 routing policies:
- simple - use when you need to redirect to a single resource. You can't attach health checks to
	simple routing policies. If multiple values are returned a random one is chosen by the client
- failover
- geolocation
- latency
- weighted
- multi-value

You pay $0.50 per month per hosted zone

Route 53 is a global service, it's not region and AZ dependant.

Route 53 TTL - the time for which the browser will cache the DNS response
High TTL - 24 hours - less traffic on DNS but more possible outdated records
Low TTL - 60 seconds - more traffic on DNS but record data on the client side will be more
up to date
TTL is mandatory for each TTL record

CNAME records vs aliases:
A CNAME points a hostname to any other hostname
They work only for non-root domains (something.mydomain.com, not mydomain.com)
Alias records point a hostname to an AWS Resource (app.mydomain.com => lb1-1234.us-east2.elb.amazonaws.com)
Aliases work for both root and subdomains.
Aliases are free of charge and include a health check.

=======================================================================================================
Encryption

KMI - key management infrastructure
KMI has the following two components:
- management layer - responsible for allowing authorized users to access the stored keys
- storage layer - responsible for storing encryption keys
AWS KMS - key management service
AWS KMS uses AES-256 as its encryption algorithm.
AWS KMS provides a centralized key management dashboard.
AWS KMS does not currently support asymetric key encryption.
AWS KMS provides the simplest solution with little development time to implement encryption on an
Amazon EBS volume.
Some of the services AWS KMS can integrate with are the following:
- Amazon EBS
- Amazon Redshift
- Amazon S3
AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily
AWS CloudHSM supports both symetric and asymetric key encryption.
generate and use your own encryption keys on the AWS Cloud.
The following are methods for AWS to provide KMI:
- you controll the encryption method and KMI
- AWS controls the encryption method and the entire KMI
- you control the encryption method and key management, and AWS provides the storage component of
the KMI

Custom key stores - this feature of Amazon KMS allows you to use a cluster of AWS CloudHSM
instances for the storage of your encryption keys.

SSE-S3 - S3 feature that allows each file deployed to S3 to be encrypted by AWS. This feature is
useful when the developer doesn't want to get involved and encryption and key storing. It automatically
encrypts the files and rotates and stores keys.

The following components are required in an encryption system:
- a cryptographic algorithm
- data to encrypt
- a method to encrypt data

=======================================================================================================
Deployment Strategies

Amazon Elastic Beanstalk deploys application code and the architecture to support an environment for
the application to run.
Elastic Beanstalk runs at no additional charge. You incur charges only for services deployed.
Elastic Beanstalk creates a service role to access AWS services and an instance role to access
instances.
The following accounts are billed for user-accessed AWS resources allocated by AWS Elastic Beanstalk:
- the account running the services
- the cross-account able to access the shared services
- the cross-account with the Amazon Simple Storage Service bucket holding a downloaded copy of
the code artifact

AWS services for CI/CD:
AWS CodeCommit - hosted git
AWS CodeBuild
AWS CodeDeploy
AWS CodePipeline

Amazon ECS task placement policies determine how tasks are placed within a cluster.
The spread policy makes sure tasks are distributed as much within a single cluster.

=======================================================================================================
Deployment as Code

In an immutable update, a new Auto Scaling group is created and registered with the load balancer.
Once health checks pass, the existing Auto Scaling group is terminated.

-- AWS CodeDeploy

If an AWS CodeDeploy configuration contains a file which already exists on the server,
the deployment will fail by default.
The CodeDeploy agent sends progress reports to the CodeDeploy service. The service does not attempt
to query instances directly, and the Amazon EC2 API does not interact with instances at the
operating system level.

-- AWS CodePipeline

The minimum number of stages required by a pipeline in AWS CodePipeline is 2.
In the Invoke pipeline action we can execute Lambda functions.

=======================================================================================================
Infrastructure as Code


=======================================================================================================
Configuration as Code

-- AWS OpsWorks
AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.

AWS OpsWorks Stacks is an application and server management service. With OpsWorks Stacks, you can
model your application as a stack containing different layers, such as load balancing,
database, and application server.
In AWS OpsWorks Stacks a custom cookbook repository location is configured for a stack. When
instances in the stack are first launched, they will download cookbooks from this location and run
them as part of lifecycle events.

AWS OpsWorks Stacks instance types:
- 24/7
- On demand
- Load-based
- Time-based - start and stop on specific time

AWS OpsWorks doesn't have a concept of cookbook caching.
OpsWorks lifecycle events don't allow you to specify cookbook versions.
After modifying cookbooks you must first run the Update Custom Cookbooks command.

A single OpsWorks task definition can describe up to 10 containers to launch at a time.
Task definitions should group containers by similar purpose, lifecycle, or resource requirements.

OpsWorks stacks instances are registered when they come online and deregistered when they are
moved to a different state.

=======================================================================================================
Authentication and Authorization

The AWS SDK relies on access keys, not passwords. The best practice is to use AWS Identity and Access
Management (IAM) credentials and not the AWS account credentials. Comparing IAM users or IAM roles,
only IAM users can have long-term security credentials.

We can use identity federation and IAM roles to establish a trust relationship between an
external Active Directory and AWS.

When you use identity federation to assume a role, The AWS security token service (AWS STS)
generates the access key ID, secret access key and session token.

The IAM trust policy defines the principals who can request role credentials from the AWS STS.
The long-term credentials are not limited to a single AWS Region. IAM is a global service, and IAM
user credentials are valid across different AWS Regions.

IAM policies are global in scope, so you do not need a custom one per AWS Region.

You can add IAM users to IAM groups but not IAM roles. Instead, roles must be assumed for short-term
sessions.

DynamoDBReadOnlyAccess policy - built-in policy that applies to the resource * wildcard, which means
that it applies to any and all DynamoDB tables accessible from the account regardless of when those
tables were created.

AWS Security Token Service (AWS STS) supports a number of different tokens:
- AssumeRole - short lived; 60 minutes by default, can be extended to 720 minutes
- GetSessionToken
- GetFederationToken

AWS AD Connector - AD Connector is a directory gateway with which you can redirect directory requests
to your on-premises Microsoft Active Directory without caching any information in the cloud.
For example, you can integrate your RADIUS-based MFA infrastructure that you already have on-premise.
It gives you the ability to configure changes to Active Directory on your existing Acitve
Directory console.

Using AWS as an identity provider (IdP) to access non-AWS resources allows you to use AWS
CloudTrail to audit who is using the service.

To use AWS Single Sign-On (AWS SSO), you must set up AWS Organizations Service and enable
all the features.
AWS SSO uses MS AD (either AWS Managed Microsoft Acitve Directory or Active Directory
Connector [AD Connector] but not Simple Active Directory).
AWS SSO does not support Amazon Cognito.
AWS SSO does not use SAML.

-- Cognito
Amazon Cognito supports device remembering and tracking
Amazon Cognito supports MFA authentication
Amazon Cognito supports Google SSO and MS AD and custom identity providers.You need SAML 2.0 based
identity provider to integrate with Amazon Cognito.

=======================================================================================================
Refactor to Microservices

-- Amazon SQS
Amazon SNS supports the same attributes and parameters as Amazon SQS.
To send a message larger than 256KB, you use Amazon SQS to save the file in Amazon S3 and then
send a link to the file on Amazon SQS. In this way you can send files larger than 256KB as S3 links
through SQS.
Amazon SQS queues do not support subscriptions.
Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages
that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your
application or messaging system because they let you isolate problematic messages to determine
why their processing doesn't succeed.

A queue without consumers deletes all the messages in it after the retention period has passed.

-- Amazon SNS

-- Amazon Step Functions

Amazon Step functions states:
- Pass
- Task
- Choice
- Wait
- Succeed
- Fail
- Parallel
- Map

-- Amazon Kinesis

Some of the Kinesis options
- Amazon Kinesis agent - a stand-alone Java agent The agent continuously monitors a set of files
and sends new data to your Kinesis Data Firehose delivery stream.
- Amazon Kinesis Producer Library (KPL) -
	https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html
- Amazon Kinesis Data Streams API

In Kinesis there is no limit in the number of consumers as long as they stay within the capacity
of the stream, which is based on the number of shards. For a single shard, the capacity is 2MB of
read or five transactions per second.

Amazon Kinesis Data Streams is a service for ingesting large amounts of data in real time and
for performing real-time analytics on the data.

=======================================================================================================
Serverless Compute

The default timeout value for an AWS Lambda function is 3 seconds.
The default limit for concurrent executions with Lambda is set to 1000. This is a soft limit that
can be raised. To do this, you must open a case through the AWS support center page and send a Server
Limit increase request.
The maximum execution time for a Lambda function is 300 seconds (5 minutes).

There are two types of policies with Lambda:
- function policy - defines which AWS resources are allowed to invoke your function
- execution policy - defines which AWS resources your function can access.

Ruby is not supported by AWS lambda functions.

We can send event payloads to an Amazon SQS queue or Amazon SNS topic if the lambda function fails
by enabling the dead-letter queue configuration option for the lambda.

The maximum amount of memory you can assign to an AWS Lambda function is 3008MB.

=======================================================================================================
Serverless Applications

Serverless applications contain both static and dynamic data

-- SAM
AWS SAM (Serverless application model) - it's based on Amazon Cloudformation and is optimized for
deploying serverless resources.
Properties for AWS Serverless Application Model:
- context - context object for a Lambda function
- handler - function handler for a Lambda function
- events - the events property allows you to assign a Lambda to an event source in Amazon API Gateway
- runtime - the language in which your AWS Lambda runs as


-- API Gateway
Amazon API gateway only supports HTTPS endpoints.
Without enabling CORS we're not going to be able to use the Amazon API Gateway service. You use a
stage to deploy your API, and a resource is typed object that is part of your API's domain. Each
resource may have an associated data model and relationships to other resourses and can respond to
different methods.

=======================================================================================================
Serverless Application Patterns

You can add a maximum of 20 nodes to an Amazon ElastiCache Redis cluster.

=======================================================================================================
Monitoring and Troubleshooting

The following are considered management events by AWS CloudTrail:
- modifying an Amazon S3 bucket policy
- creating an Amazon RDS instance

AWS CloudTrail events contain the following information:
- what request is being made
- when the request was made
- which resource was acted on
- who made the request

CloudTrail stores event history for 90 days back. If you would like to store this information
permanently, you can create a CloudTrail trail, which stores the logs in Amazon S3.

In Amazon CloudWatch data points with a period of 300 seconds are stored for 63 days.
Logging to Amazon CloudWatch from a EC2 instance requires installing the CloudWatch Logs agent.

CloudWatch alarms support triggering actions in Amazon EC2, EC2 Auto Scaling, and Amazon SNS. We can
trigger AWS Lambda functions from an alarm, but only by first sending the alarm notification to an
Amazon SNS topic.

The following EC2 metrics are available in CloudWatch:
- cpu utilization
- disk I/O
- network traffic - in/out
RAM cannot be tracked by CloudWatch because memory is allocated in a single block to an instance
and is managed by the guest OS, the underlying EC2 instance host doesn't have visibility
into consumption.
CloudWatch metrics support dashboards
The LookupEvents API action can be used to query event data.

With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon EKS clusters,
Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and
troubleshooting, implement pre-approved change work flows, and audit operational changes
for your groups of resources.

=======================================================================================================
Optimization

Multi availability zone is used for fault tolerance.
In case of slow reads from AWS RDS we can create a read replica and routing calls to it.

In AWS autoscaling group the instance user data is the space where you can place optional
scripts that can run after bootstrapping the application. This is used to perform.

By launching instances in multiple Availability Zones in the same region, you help protect your
applications from a single point of failure.

Provisioned IOPS volumes in the Amazon EBS are designed to deliver the expected performance only
when they are attached to an Amazon EBS optimized instance.

In AWS Cloudfront, some headers such as Date or User-Agent, significantly reduce the cache hit ration
(the proportion of requests that are served from a CloudFront edge cache).

In AWS S3, a multipart upload is supported and we can use it to upload big objects to S3.

One of the recommended best practices for AWS Lambda is to avoid using recursive code.

=======================================================================================================

Resources:
https://github.com/mostafac0des/certified-aws-developer-associate-notes
https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/
https://instances.vantage.sh/
https://aws.amazon.com/ec2/instance-types/
https://aws.amazon.com/caching/best-practices/
https://aws.amazon.com/builders-library/caching-challenges-and-strategies/

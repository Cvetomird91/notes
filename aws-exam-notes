AWS Cloud API and CLI

The AWS CLI creates the following two folders after configuration

~/.aws/config
~/.aws/credentials

CLI command to deserialize authorization message:
aws sts decode-authorization-message --encoded-message <message-string>

To use MFA with AWS CLI, you must create a temporary session.
To do so, you must run the STS GetSessionToken API call:
$ aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code \
code-from-token --duration-seconds 3600

Offical SDKs for AWS are present in the following languages:
- Java
- .NET
- Nodejs
- PHP
- Python (named boto3 / botocore)
- Go
- Ruby
- C++

We're using the Python SDK when we use the AWS CLI.

us-east-1 - default region when configuring AWS CLI

API Rate limits - how many times you can call an AWS API in a row.
for example:
DescribeInstances API for EC2 has a limit of 100 calls per second
GetObject on S3 has a limit of 5500 GET per second per prefix

In case we hit the API rate limits:
- for intermittent errors - implement Exponential Backoff
- for consistent errors - request an API throttling limit increase

Service Quotas (Service Limits) - we can increase them on demand if we hit the limit
Service Quotas API - we can use it to request it programatically

ThrottlingException - happens intermittently, use exponential backoff - it retries calls with a
longer waiting interval. Thus, you're slowing down the load on your system.

The CLI will look for credentials in this order:
1. Command line options - --region, --output and --profile
2. Environment variables - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN
3. CLI credentials file - ~/.aws/credentials or C:\Users\user\.aws\credentials
4. CLI configuration file - ~/.aws/config or C:\Usersuser\.aws\config
5. Container credentials - for ECS tasks
6. Instance profile credentials - for EC2 instance profiles

The Java AWS SDK will look for credentials in this order:
1. Environment variables - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
2. java system properties - aws.accessKeyId and aws.secretKey
3. default creds profile file - ~/.aws/credentials
4. Container credentials - for ECS tasks
5. Instance profile credentials - for EC2 instance profiles

When you call the AWS HTTP API, you need to sign the request.
If you're making direct request to the AWS APIs you need to sign them
yourself using Signature v4 (SigV4)

=======================================================================================================
Compute and Networking

Regions - a set of datacenters
Availability zone - each region has a set of availability zones (from 2 to 6, the usual is 3)

ARN - Amazon Resource name

ap-southeast-2 - Syndney region
Sydney region availability zones:
ap-southeast-2a
ap-southeast-2b
ap-southeast-2c

Availability zones are a discrete data center with redundant power, networking and connectivity.
They are separate from each other, so that they're isolated from disasters.

IAM federation - big enterprises intergrate their own repository of users with IAM

Basic good practices for IAM roles:
- One IAM User per physical role
- One IAM Role per Application
- IAM credentials should never be shared
- Never write IAM credentials in code
- Never use ROOT IAM credentials in an application

Putting your security credentials on an AWS machine is bad practice.

SSH troubleshooting:
1) There's a connection timeout
This is a security group issue. Any timeout (not just for SSH) is related to security groups or a
firewall. Ensure your security group looks like this and correctly assigned to your EC2 instance.

Security groups can be reused and assigned to multiple EC2 instances.
They're locked down to a region/VPC combination.

A security group misconfiguration is usually responsible for time-outs.
A "connection refused" error is usually caused by an application error - the security group
in this case is allowing traffic properly.

All inbound traffic is blocked by default.
All outbound traffic is allowed by default.

We can reference security groups within other security groups.

Types of IPs in EC2:
- public IP
- private IP
- elastic IP

When you stop and then start an EC2 instance, it can change its public IP.
If you need to have a fixed public IP for your instance, you need an Elastic IP.
You can attach 1 elastic IP at a time.

You can have at most 5 elastic ips. This can be increased after reaching out to AWS support.

Overusing elastic IPs is bad architecture. It's best to use random public IPs and remap a DNS record
to then.

We can mask the failure of an instance by switching the elastic ip to another instance but
overusing that pattern is bad practice.

EC2 instance metadata allows instances to "learn" about each other without additional IAM roles.
The URL for EC2 metadata is:
http://169.254.169.254/latest/meta-data
This URL will work only from an EC2 instance.
Using it you can retrieve the IAM role name from the metadata, but you CANNOT retrieve the IAM policy.

EC2 instance launch types:
- on demand instances: short workload, predictable pricing - billing per second, after the
	first minute. Highest cost but no upfront payment, no long term commitment. Recommended
	for short-term and un-interrupted workloads, where you can't predict how the application will
	behave
- reserved (minimum 1 year) - suitable for a workload we'll be sure we'll use for a minimal amount of
time - can have up to 75% discount compared to On-demand. You need to pay upfront for what you use
	with long term commitment. Reservation period can be from 1 to 3 years. You reserve a specific
	instance type. A server for database is a proper example for this type of instance
	- reserved instances - long workloads
	- convertible reserved instance - long workloads with flexible instances. We can change the
		EC2 instance type here. Up to 54% discount.
	- scheduled reserved instances: example - every Thursday between 3 and 6 pm. They can get
	launched periodically within a time period we specify.
- spot instances - short workload, cheap, losing the instance isn't fatal (less reliable)
	Can get a discount of up to 90% compared to on-demand.
	instances you can use any time if the max price you can pay from them is less than the
	current spot time.
	Useful if your workload is resilient to failure like:
	- batch jobs
	- data analysis
	- image processing
	Not recommended for critical purposes
- dedicated instances - no other customers will share the physical hardware
- dedicated hosts - book an entire physical server, control instance placement. Allocated for
your account for a 3 year period reservation

You do not pay for an instance if the instance is stopped.

ENI (Elastic Network interface):
- can have primary private IPv4, one or more secondary IPv4
- can have one elastic IP (IPv4) per private IPv4
- can have one public IPv4
- can have one or more security groups
- can have a MAC address
- we can create ENIs independently and move them inbetween EC2 instaces for failover (in the same
availability zone)
- ENIs are bound to a specific availability zone

AMIs are built for a specific AWS region!

AMIs have the following characteristics:
- RAM
- CPU
- I/O (disk performance)
- network
- gpu

IAM Roles can be attached to EC2 instances.
IAM Roles can come with a policy authorizing exactly what the EC2 instance should be able to do

-- Networking & VPC

VPC - private network to deploy your resources (regional resource)
Subnets - they allow you to partition your network inside your VPC (Availability Zone resourse). They
are defined at the availability zone level.
A public subnet is a subnet that is accessible from the internet.
A private subnet is a subnet that is not accessible from the internet.
To define access to the internet and between subnets, we use RouteTables.
Internet gateway - helps our VPC instances connect to the internet
Public subnets have a route to the internet gateway. This is what makes a subnet a public subnet.
NAT gateways (AWS-managed) and NAT instances (self-managed) allow your instances in your Private
Subnets to access the internet while remaining private.

NACL (Network ACL) - firewall that controlls traffic from and to subnet
It can have allow and deny rules. Those rules are attached at the subnet level. Those rules only
include IP addresses.
The default NACL allows everything and everything out

Security groups - a firewall that controlls traffic to and form an ENI / an EC2 instance. It can only
have allow rules
Rules include IP addresses, ports and other security groups

https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison

VPC flow logs - captures info about traffic going into your interfaces:
- vpc flow logs
- subnet flow logs
- ENI flow logs
- all AWS managed interfaces - ELB, RDS, ElastiCache etc
Helps to monitor and troubleshoot connectivity issues:
- subnets to internet
- subnets to subnets
- internet to subnets

VPC Peering - connect two VPC, privately using AWS' network. This makes them behave as if they were
in the same network. For this to work we must not have overlapping CIDR (IP address range)
VPC peering is not transitive - it needs to be established for each VPC that need to communicate
with one another. E.g.
if VPC A is connected to both VPC B and VPC C, then VPC B and VPC C cannot communicate with one
another.

Endpoints allow you to connect to AWS Services using a private network instead of the public www
network. This gives you enhanced security and lower latency to access AWS resources.

VPC endpoint gateway - allows access to external resources from the private network
VPC endpoint interface - the rest of the service (only used within your VPC)
We can create a VPC endpoint interface (ENI) and from it we can have private access to a public
service

VPC endpoint is the usual way referred on the exam when we need to connect a private AWS service
to the external world.

There are two ways to connect an on-premise private network to a VPC:
- site to site VPN
- Direct Connect - physical connection. It's private, doesn't go over the public internet and is
fast. It takes atleast a month to establish

Both Direct Connect and Site to Site VPN cannot access VPC endpoints

=======================================================================================================
ELB - Elastic Load balancer

ELB seamlessly handles failures of downstream instances.
It does regular health checks on your EC2 instances.
It provides SSL termination for your instances.
It enforces stickiness with cookies.
High availability across zones.
It separates public from private traffic.

There are 4 types of load balancers:
- classic load balancer (v1) - http, https, tcp
- application load balancer (v2 - new generation) - http, https, websocket, http/2
- network load balancer - tcp, tls, udp
- gateway load balancer (not on the exam)

ELBs can be internal or external.

Load balancer 503 - load balancer is at max capacity or no target is registered

ALB supports routing based on different target groups.
ALBs can stand in front of lambda functions

Dynamic port forwarding - useful in ECS setups

NLBs are used when you want extreme performance and you want to use a protocol different than HTTP(S)
It can also be useful when you want a static IP assigned.

Load balancer stickiness - the same client is always redirected to the same actual instance behind
the load balancer. This feature is available for CLB and ALB.
The "cookie" used for stickiness has an expiration date you control.
Stickiness may bring imbalance to load balancing overloading some of the downstream instances.
Stickiness for ALB is configured at the target group level.
For CLBs stickiness is set at the load balancer level.

With cross-zone load balancing each load balance instance distributes evenly the calls across all
registered instances in all AZs.
Otherwise, it'll distribute them only to the instances in its own availability zone.
This is disabled by default in CLB. No additional charges for it if enabled.
For ALB this is the default behaviour and cannot be disabled.
For NLB this is disabled by default. You'll pay additional charges if enabled.

Load balancers use an X.509 certificate
ACM - AWS certificate manager - this is where we manage SSL certificates

SNI - ability to have multiple SSL certificates on the same host. This is achieved by the
client specifying which hostname of a target server it wants to reach.
SNI works with Cloudfront, ALB and NLB. Does not work with CLB.

Connection draining (aka Deregistration delay for target groups in ALBs and NLBs) - this is the
time to complete "in-flight requests" while the instance is de-registering or unhealthy.
The ELB stops sending new requests to the instance being drained.
The ELB won't start new connections to an instance being drained and it'll give it 300 seconds by
default to complete the requests
It can be between 1 and 3600 seconds.
Can be disabled by setting it to 1 second.

Auto Scaling Group parameters:
- minimum size
- actual size / desired capacity
- maximum size - how many instances can be added at scale out
scale out - add instances
scale in - remove instances
Auto Scaling Alarms - they determine when to trigger a scale in / out. It is possible to scale
an ASG based on CloudWatch alarms. CloudWatch metrics are computed overall for all ASG instances.
We can also use metrics directly from the EC2 instances.
We can also use custom metrics sent to CloudWatch from our EC2 instances.
ASGs use Launch configurations or Launch Templates (newer)
To Update an ASG, you must provide a new launch configuration / launch template.
IAM roles attached to an ASG will get assigned to EC2 instances
ASG are free. You pay for the underlying resources being launched.

ASG scaling policies:
- target tracking scaling
- simple / step scaling policy
- scheduled actions

ASG Cooldowns - the cooldown period ensures that your ASG doesn't scale in or out additional instances
before the previous scaling takes effect

=======================================================================================================
Storage

-- Amazon AWS S3

Advertised as "infinitely scaling" storage.

Versioning is a means of keeping multiple variants of an object in the same bucket. You can use
versioning to preserve, retrieve, and restore every version of every object stored in your Amazon
S3 bucket. With versioning you can easily recover from both unintended user actions and application
failures. The following are version states of buckets:
- Versioning suspended
- Versioning disabled
- Versioning enabled

Any file that is not versioned prior to enabling versioning will have version "null".
Versioning is enabled at the bucket level.
Suspending versioning in your bucket doesn't delete the previous versions.

There are 4 ways to encrypt objects in Amazon S3:
- SSE-S3 - encrypts S3 objects using keys handled & managed by AWS - encryption using keys handled
	and managed by Amazon S3. The object is encrypted server side. AES-256 is the type of encryption
	To use it you must set a header
	"x-amz-server-side-encryption":"AES256"

- SSE-KMS - leverage AWS Key Management Service to manage encryption keys. This gives us user control
	and audit trail. Those are the advantages over SSE-S3. The object is encrypted server side. To
	use it we must set a header:
	"x-amz-server-size-encryption":"aws:kms"

- SSE-C - when you want to manage your own encryption keys. In this case Amazon does not store the
	key you provide. HTTPS must be used to use this method. You'll need the same key to decrypt the
	files manually.

- Client-side encryption - keys and encryption are performed by the user. Some libraries like
	S3 encryption client can help with this. Client should encrypt and decrypt the data
	themselves.

Amazon S3 can use both HTTP or HTTPS.

Each S3 bucket must have a globally unique name. S3 is a global service but buckets are
defined at the region level.

S3 bucket websites don't support HTTPS.

Naming convention:
- no uppercase
- no underscore
- 3-63 characters long

S3 objects (files) have a key - the key is the full path to the file:
s3://my-bucket/my_file.txt

The key is composed of:
uri + bucket name + prefix + object name

There's no concept of directories in s3, but the UI can trick you to think there are.
There are just long names with keys that contain "/".

An object value is the content of the file.

Max object size is 5000GB (5TB).

If you upload more than 5GB, you need a multi-part upload.

Each object has metadata - list of key / value pairs.
It also has tags (they're something separate).

The following are a deciding factor when choosing an AWS Region for your bucket:
- Cost - prices are different between regions
- Latency
- Regulatory requirements - different regions may have different legal requirements

Pre-signed URLs allow you to grant time-limited permission to download objects from an Amazon S3
bucket.
They can be generated with the CLI or SDK.
A user given a pre-signed URL will temporarily have the permissions of the IAM role that
generated the URL.

aws s3 presign s3://next-level-bucket/coffee.jpg --region eu-central-1

In case of issues with encrypted files:
aws configure set default.s3.signature_version s3v4

If data must be encrypted before being sent to Amazon S3, client-side encryption must be used.

Amazon S3 supports server access logging for buckets that tracks access time and who accessed the
bucket.

Amazon S3 bucket policies can specify a request IP range, an AWS account and a prefix for objects
that can be accessed.

S3 Security types:
- user-based - based on IAM policies
- resource-based
	- bucket policies - bucket-level rules (JSON based)
	- Object ACL - finer grain at the object level
	- bucket ACL - less common

S3 storage types:
- Standard - General Purpose
	High durability of objects across multiple AZ
	Use cases: big data analytics, mobile & gaming apps, content distribution
- Standard-infrequent access (IA) - suitable for data that is less frequently accessed, but
	requires rapid access when needed
	It has a lower cost than S3 Standard
- One Zone-infrequent access - same as IA but stored in multiple AZs
	99.5% availability
	Supports SSL for data at transit and encryption at rest
	Low cost compared to IA (by 20%)
	Use cases: secondary backup or storing data you can recreate
- Intelligent Tiering
	small monthly monitoring and auto-tiering fee
	Automatically moves objects between access tiers based on changing access patterns
- Glacier - low cost object storage meant for archiving and backups
	Each item in Glacier is called an archive. Max archive size is 40TB
	Arcyives are stored in vaults, not buckets
- Glacier Deep archives
- Reduced Redundancy Storage

An IAM principal can access an S3 object if the user IAM permissions allow it OR
the resource policy ALLOWS it.

S3 supports VPC endpoints for instances in VPC without www internet.
S3 Access logs can be stored in other S3 buckets.
S3 API calls ban be logged in AWS CloudTrail.

Amazon S3 access logs: https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html
Setting the monitored bucket to be where you store your access logs is bad practice.

S3 supports replication:
- CRR - cross-region replication
- SRR - same region replication
To enable it we must enable versioning in source and destionation buckets.
The buckets can be in different accounts.
Copying is asynchronous.
Proper IAM permissions must be granted.
It's not retroactive - only the files added after enabling it will be replicated.
Any delete operation is not replicated.
You cannot "chain" replication. If bucket 1 has replication in bucket 2 and bucket 2
has replication in bucket 3, then objects created in bucket 1 aren't replicated in 3.

S3 supports MFA delete to prevent accidental deletion of objects.
Only the bucket owner (root account) can enable/disable MFA-Delete
This can be enabled only using CLI or AWS API calls.

aws s3api put-bucket-versioning --bucket mfa-delete-lab --versioning-configuration Status=Enabled,MFADelete=Enabled \
--mfa "mfa-device-arn mfa-device-code" --profile mfa-delete-lab

aws s3api put-bucket-versioning --bucket mfa-delete-lab --versioning-configuration Status=Enabled,MFADelete=Disabled \
--mfa "mfa-device-arn mfa-device-code" --profile mfa-delete-lab

S3 consistency model:
- read after write consistency for PUTS of new objects - as soon as a new object is
written, we can retrieve it (PUT 200 => GET 200)
- this is true unless we've queried the bucket before uploading the file:
(GET 404 => PUT 200 => GET 404) - eventually consistent
- eventual consistency for DELETES and PUTS of existing objects:
if we read an object after updating, we might get the older version
ex: (PUT 200 => PUT 200 => GET 200 (might be older version))
if we delete an object we might still be able to retrieve it for a short period
ex: (DELETE 200 => GET 200)

There's no way to request "strong consistency" in Amazon S3!

S3 Cors - Cross-Origin Resource Sharing

Access-Control-Allow-Origin

An origin is a scheme (protocol), host (domain) and port:
e.g. https://www.example.com

If a client does a cross-origin request on our S3 bucket, we need to enable the
correct CORS headers

Lifecycle rules

transition actions - it defines when objects are transitioned to another storage
class
expiration actions - delete an object after some time

Lifecycle rules can be applied for a certain prefix - s3://mybucket/mp3/*

The prefix of a file is the part of the name between the bucket name and filename

S3 Select & Glacier Select - perform simple SQL-like queries against files in a bucket

We can use Amazon SQS and Amazon SNS to send messages on events like
S3:ObjectCreated, S3:Objectremoved, S3:ObjectRestore, S3:Replication etc.

You need to enable versioning to notify on every successful write.

S3 Athena supports ODBC and JDBC
In S3 Athena you get charged per query and amount of data scanned.
S3 Athena is based on Presto (https://prestodb.io/)

For the exam, anytime you need to analyze data directly on S3, usually Athena is the solution.

S3 Object Lock / Glacier Vault Lock
WORM model - Write Once, Read Many - block object deletion for a specific amount of time

-- Amazon Elastic Block Store (EBS)

Amazon snapshot EBS data is loaded lazily.
EBS volumes are scoped within a specific availability zone. You cannot attach an EBS volume from
us-east-1a to us-east-1b
An EBS volume can be accessed by a single EC2 instance, as opposed to EFS volumes.
To use an EBS volume in another availability zone we need to create a snapshot of it
and restore it in another AZ.

Depending on the volume and instance types, you can use Multi-Attach to mount a volume to
multiple instances at the same time.

EBS backups use IO and you shouldn't run them while your application is handling a lot
of traffic.

EBS volume types:
- GP2 (SSD) - recommended for most workloads, system boot volumes and virtual desktops,
	low-latency interactive apps, development and test environments
	Size is between 1GiB to 16TiB
	A small gp2 volume can burst IOPS to 3000
	Max IOPS is 16k
- IO1 (SSD) - critical business applications that require more than 16000 IOPS per volume
	Suitable for large database workloads (Oracle SQL, MS SQL etc)
	4GiB - 16TiB
	Max IOPS are 32000
- ST1 (HDD) - good for big data, data warehouses, log processing
	good for streaming workloads that require fast throughput at a low price
	Cannot be a boot volume
	500 GiB - 16 TiB
	Max IOPS is 500
- SC1 (HDD) - the cheapest option. Good for less frequently accessed data
	500GiB - 16TiB
	Max IOPS is 250
	Max throughput is 250 MiB/s
EBS volume types are characterized by Size and Throughput (I/O Ops per second)
Only GP2 and IO1 can be used as boot volumes

Instance store = ephemeral storage - it gets destroyed when the instance is destroyed.
It is physically attached to the EC2 instance, while EBS is a network drive.
It has better performance and good for cache / scratch data.
It's not an option if you want to persist data inbetween instance terminations
it survives reboots but not instance terminations
It cannot increase in size, as opposed to EBS
There's a risk of data loss in instance store if the physical hard disk fails.


-- Amazon Elastic File System (EFS)

A managed NFS that could be mounted on many EC2 instances at the same time
across many different availability zones (EBS is availability zone dependent).
It's pay per use and is 3x more expencive than gp2 ebs volumes.

To use EFS you need security groups.
It works with Linux-based AMIs only.

You can use KMS keys to encrypt the file system.
It scales automatically. Users don't plan the capacity.

Amazon EFS supports one to thousands of Amazon EC2 instances conneting to a file system concurrently.

The Max I/O performance mode of EFS is optimized for applications where tens, hundreds, or
thousands of EC2 instances are accessing the file system.

Performance modes:
- general purpose
- max I/O - good for big data

Therea are different storage tiers in EFS - lifecycle management. We can move files
between EFS volumes after a number of days:
- standard
- infrequent access (EFS-IA): cost to retrieve files, lower price to store

-- Amazon Glacier
Glacier can be used as a standalone service and as an Amazon S3 storage class.

=======================================================================================================

Amazon CloudFront

Caches content at edge locations (216 point of presence globally - edge locations).
It also adds DDoS protection, integration with AWS Shield and AWS WAF

https://aws.amazon.com/cloudfront/features/

CloudFront Origin Access Identity
CloudFront can be used as an ingress to S3 (upload files from anywhere in the world).
Any http backend can be used with CloudFront.

ALB or EC2 when used as origins require their security groups to allow CloudFront.

Geo Restriction - whitelist or blacklist users only from a specific region. It's using a
third party GeoIP database for this purpose.

In CloudFront we can cache based on multiple things:
- Headers
- Session Cookies
- Query String parameters

The cache lives at each CloudFront Edge Location.

TTL can be from 0 seconds to 1 year. It can be set by the origin using the Cache Control header
or Expires header

You can invalidate part of the cache using the CreateInvalidation API

Viewer protocol policy - enables SSL between your client and your edge location. We can
redirect HTTP to HTTPS or enable HTTPS only.

Origin protocol policy - the HTTP(S) policy for the target. It can be HTTPS only or match
the viewer's protocol policy.

S3 bucket websites don't support HTTPS.

To distribute private content with CloudFront we can use a Signed URL / Signed Cookie. We can
attach a policy with:
- Includes URL expiration
- Includes IP ranges to access the data from
- Trusted signers (which AWS accounts can create signed URLs)

A signed URL gives access to individual files. The difference with S3 pre-signed URLs is that
CloudFront signed URLs give access to a path no matter the origin.
S3 pre-signed URLs issue a request as the person who pre-signed the URL.

Two types of signers:
- trusted key group
- aws account that contains a CloudFront key pair (not recommended because only root
accounts can be used)

Origin groups - increase high-availability and do failover. This is a group of EC2 instances.
If the primary origin fails, the second one is used.

Field level encryption

=======================================================================================================
Docker in AWS (ECR, ECS)

Task Definition - metadata in JSON which defines how ECS runs a docker container or a set of docker
containers. Similar to K8S pods. Max number of containers defiend in a task definition could be 10.

Task Role - optional IAM role that allows a container to talk to AWS services. Often, on the exam,
if a container is not allowed to do something it lacks a proper Task Role. It is defined in the
task definition.

ecsInstanceRole - this role gets attached to each ECS Task
ecsServiceRole - this role gets attached to each ECS Service and allows it to register and
deregister instances as ELB targets

ECS Services define how much tasks shall we run and how should they run.
They can be linked to ELBs if needed.

EC2 Instance profile - used by the ECS agent to make API calls to the ECS service and send container
logs to CloudWatch and also pull images from ECR.

We can only add a load-balancer to an ECS service at service creation. We cannot add it by editing
a service.
Only ALB load balancer supports dynamic port mapping for ECS services.

Access to ECR is controlled through IAM policies

There are two ways to authenticate against ECR
- AWS CLI v1 login command:
	$(aws ecr get-login --no-include-mail --region eu-west-1)
- AWS CLI v2 login command:
	aws ecr get-login-password --region eu-west-1 | docker login --username AWS -- password-stdin | \
	123.dkr.ecr.eu-west-1.amazonaws.com

When a task of type EC2 is launched, ECS must determine where to place it, with the constraints
of CPU, memory and available port. To define this we need to define a task placement strategy
and task placement constraints. This doesn't work on Fargate yet.

ECS Task placement strategies:
- binpack - place task based on the least available amount of CPU or memory. This minimizes
	the number of EC2 instances in use
- random - places the task randomly
- spread - place the tasks evenly based on the specified criteria
Task placement strategies can be mixed together

ECS Task placement constraints:
- distinctInstance: place each task on a different EC2 instance
- memberOf: places task on instances that satisfy an expression

Cluster Query Language

ECS - Service auto scaling:
- target tracking
- step scaling
- scheduled scaling

Cluster capacity provider - determines the infrastructure that the task runs on. It scales
the EC2 cluster we run ECS on.

bind mounts - sharing data between containers in a task. Can be used to share ephemeral storage
between application container and sidecar. The sidecar can be used to send metrics/logs to other
destinations (separation of concerns).

/etc/ecs/ecs.config - EC2 confing file for the ECS agent

ECS security groups work at the EC2 instance level.

ECS does provide integration with AWS CloudWatch Logs.
You need to setup logging at the task definition level.
Each container will have a different log stream.

ECS_ENABLE_TASK_IAM_ROLE - a variable defined in /etc/ecs/ecs.config that can be used for adding
additional IAM roles to an EC2 ECS instance

=======================================================================================================
Databases

-- AWS RDS

1 RCU - One strongly consistent read per second of 4KB.
In order to determine the number of RCUs for a number of reads per seconds we need to determine the
number of complete chunks
and then multiply the number of chunks with the count of complete reads.
E.g. to calculate the RCUs of 25 strongly consistent reads per seconds of 15 KB (those are 4 complete
chunks of 4KB - 4 x 4 = 16KB),
we need 25 x 4 = 100 RCUs

To calclulate eventually consistent reads we need to divide the final result by 2:
(25 x 4) / 2 = 50 RCUs

WCU - write capacity unit
1 WCU = 1 write per second of 1KB (1024 bytes).
How to calculate for example for 100 writes per second of 512 bytes:
512 bytes uses one complete chunk of 1 KB
(512/1024 = 0.5, rounded up to 1).

Amazon RDS manages for the developer the following things out of the box:
- database software installation and patching
- hardware provisioning
- backups
You are still responsible for managing the database settings reuired by the application.

An existing Amazon RDS instance is deleted if the environment is deleted. There is no auto-retention
of the database instance. You must create a snapshot to retain the data and to restore the database.

You cannot SSH into the EC2 instances hosting AWS RDS instances. This is a managed service
RDS EC2 instance storage is backed by EBS (gp2 or io1)

RDS backups are enabled by default. There are daily full backups in a maintenance window.

Transaction logs are backed-up by RDS every 5 minutes

RDS DB snapshots differ from backups with being triggered by the user.

We can create up to 5 read replicas in the same or other AZ or other region.
Async replication is performed when we enable read replicas.
This means that the reads are "eventually consistent".

Each replica can be promoted to a stand-alone DB instance.

Applications need to update their connection string to leverage read replicas.

In AWS There's a network cost when data goes from one AZ to another.
To reduce the network cost you can have a read replica in the same AZ (if the usecase allows it).
It'll be free in such case.

A disaster recovery instance is a database "copy" that receives sync replication from the
main database. When the main database goes down the replcia starts to handle
read and write queries. The DNS record gets automatically redirected to it.

Read replicas can be used in such a way as well for disaster recovery.

AWS RDS supports IAM authentication - an IAM user could be created as a database user (it
works for MySQL and PostgreSQL).

AWS RDS supports encryption with AWS KMS - AES-256 encryption.
It needs to be defined at launch time.

If the master is not encrypted, the read replicas cannot be encrypted.

it also supports in-flight encryption using SSL.

To enforce SSL:
in postgresql: rds.force_ssl=1 in the AWS RDS console (Parameter groups)
in mysql:
GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SSL;

Snapshots of un-encrypted RDS databases are un-encrypted.
Snapshots of encrypted RDS databases are encrypted.

To encrypt an un-encrypted RDS database:
1. create a snapshot of the un-encrypted database
2. copy the snapshot and enable encryption for the snapshot
3. restore the database from the encrypted snapshot
4. migrate applications to the new database, and delete the old one

RDS databases are usually deployed within a private subnet.
RDS leverages security groups to expose the database to the internet.

Transparent Data Encryption (TDE) - available only for MS SQL and Oracle SQL.
This feature encrypts data at rest - db files both on the hard drive and backup.

-- AWS Aurora

AWS Aurora is compatible with both PostgreSQL and MySQL clients.
AWS claim 5x times improvement over MySQL performance and 3x over PostgreSQL.

The Aurora size can be between 10GB and come up to 64GB.

Aurora can have up to 15 replicas.
Aurora is HA native.
Aurora costs 20% more than other RDS engines.

Aurora stores copies of your data as read replicas as twice as the AZs in which
they're placed.
E.g. 6 copies of your data across 3 AZ.
4 copies out of 6 are needed for writing.
3 copies out of 6 are needed for reads.
Aurora does some peer to peer self-healing.
Storage is striped across 100s of volumes.
One Aurora instance makes writes (master).
If the master is down, a new one is promoted in less than 30 seconds.
Aurora supports cross-region replication.
Aurora provides a "writer endpoint" - a DNS record always pointing to the master.
Aurora provides a "reader endpoint" - a DNS record pointing to all read replicas.
Aurora shared storage volumes are auto expanding from 10G to 64TB.

Aurora backtrack - a feature which allows you to restore the database to any point in time.

Aurora serverless - automated database instantiation and auto-scaling based on actual usage

Aurora global database - 1 primary region and up to 5 secondary read-only regions.

up to 16 read replicas per secondary region.

-- AWS ElastiCache

ElastiCache for Memcached doesn't support a high-availability configuration.
Only ElastiCache Redis could run for in a high-availaiblity configuration.
ElastiCache Redis supports data durability using AOF persistence (append only file).

ElastiCache Memcached - multi-node for partitioning of data (sharding)
It's non-persistent (no AOF).

ElastiCache Redis supports backup and restore with AOF, opposed to ElastiCache memcached.
Memcached has multi-threaded architecture.

Caching strategies:
- Lazy loading / Cache aside / Lazy population - update cache on retrieval if an item is missing there
- Write-through cache - update the cache after the database is updated

Both are usually combined.

Cache churn - a data is present in cache that will never be accessed

Evicting cache:
- explicitly delete the item
- item is evicted because there's not enough space in cache (LRU)
- the TTL of the item expires

If too many evictions happen due to memory, you should scale up or out

Setting a TTL is good practice, except when you're using write-through

-- Other

Amazon Redshift - managed data warehouse solution
	S3DistCp - Redshift feature that allows it to copy data from S3 to the Redshift cluster

Amazon Neptune - NoSQL graph database for highly connected datasets.

-- Amazon DynamoDB

In DynamoDB scans are less efficient than queries.
In DynamoDB you can create local secondary indexes only when you are creating the table.
Server-side encryption provided by the DynamoDB service is a recommended way to protect data at rest
stored in DynamoDB.
Each record in DynamoDB Streams appears only once in a stream.
400KB - this is the maximum DynamoDB item size limit.
QueryTable is the DynamoDB operation used to find items based on primary key.
UpdateTable is the DynamoDB operation used to modify the provisioned throughput settings, global
secondary indexes, or DynamoDB Streams settings for a given table.
Scan is the DynamoDB operation used to read every item in a table.
The lifetime of data in a DynamoDB stream is 24 hours.

These are the following three operations used to read data in DynamoDB:
- GetItem
- Scan
- Query

Amazon DynamoDB Accelerator (DAX) - write-through and read-through caching service for DynamoDB.
=======================================================================================================
Route 53 and DNS

Alias - hostname mapped to an AWS resource

Route 53 supports load balancing through DNS - client load balancing
Route 53 supports health checks.

Route 53 routing policies:
- simple - use when you need to redirect to a single resource. You can't attach health checks to
	simple routing policies. If multiple values are returned a random one is chosen by the client
- failover - this one is based on health checks. We have primary and secondar failover records.
	If the health check for the AWS service behind the primary check starts to fail the DNS records
	starts pointing to the secondary one.
- geolocation - routing based on user location. E.g. traffic from the UK should go to this specific IP
	We should also define a "default" policy in case there's no match for a location
- latency - redirect to the server that has the least latency at the moment close to us
- weighted - controls the percentage of each endpoint that is going to be reached. This is helpful
	to route a % of traffic to a new applicatoin versoin. Supports health checks. Helpful to
	split traffic between regions
- multi-value - route traffic to multiple resources. Want to associate a route 53 health
	check with records. Up to 8 healthy records are returned for each Multi Value query. Multi
	value is not a substitute for having an ELB.

You pay $0.50 per month per hosted zone

Route 53 is a global service, it's not region and AZ dependant.

Route 53 TTL - the time for which the browser will cache the DNS response
High TTL - 24 hours - less traffic on DNS but more possible outdated records
Low TTL - 60 seconds - more traffic on DNS but record data on the client side will be more
up to date
TTL is mandatory for each TTL record

CNAME records vs aliases:
A CNAME points a hostname to any other hostname
They work only for non-root domains (something.mydomain.com, not mydomain.com)
Alias records point a hostname to an AWS Resource (app.mydomain.com => lb1-1234.us-east2.elb.amazonaws.com)
Aliases work for both root and subdomains.
Aliases are free of charge and include a health check.

Default Route 53 health checks are 3 in route 53 before a status changes.
Default health check interval is 30 seconds. The other option is 10s - it costs more.
Health checks cost more when they're not non-aws endpoints.

=======================================================================================================
Encryption

KMI - key management infrastructure
KMI has the following two components:
- management layer - responsible for allowing authorized users to access the stored keys
- storage layer - responsible for storing encryption keys
AWS KMS - key management service
AWS KMS uses AES-256 as its encryption algorithm.
AWS KMS provides a centralized key management dashboard.
AWS KMS does not currently support asymetric key encryption.
AWS KMS provides the simplest solution with little development time to implement encryption on an
Amazon EBS volume.
Some of the services AWS KMS can integrate with are the following:
- Amazon EBS
- Amazon Redshift
- Amazon S3
AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily
AWS CloudHSM supports both symetric and asymetric key encryption.
generate and use your own encryption keys on the AWS Cloud.
The following are methods for AWS to provide KMI:
- you controll the encryption method and KMI
- AWS controls the encryption method and the entire KMI
- you control the encryption method and key management, and AWS provides the storage component of
the KMI

Custom key stores - this feature of Amazon KMS allows you to use a cluster of AWS CloudHSM
instances for the storage of your encryption keys.

SSE-S3 - S3 feature that allows each file deployed to S3 to be encrypted by AWS. This feature is
useful when the developer doesn't want to get involved and encryption and key storing. It automatically
encrypts the files and rotates and stores keys.

The following components are required in an encryption system:
- a cryptographic algorithm
- data to encrypt
- a method to encrypt data

=======================================================================================================
Deployment Strategies

Amazon Elastic Beanstalk deploys application code and the architecture to support an environment for
the application to run.
Elastic Beanstalk runs at no additional charge. You incur charges only for services deployed.
Elastic Beanstalk creates a service role to access AWS services and an instance role to access
instances.
The following accounts are billed for user-accessed AWS resources allocated by AWS Elastic Beanstalk:
- the account running the services
- the cross-account able to access the shared services
- the cross-account with the Amazon Simple Storage Service bucket holding a downloaded copy of
the code artifact

AWS services for CI/CD:
AWS CodeCommit - hosted git
AWS CodeBuild
AWS CodeDeploy
AWS CodePipeline

Amazon ECS task placement policies determine how tasks are placed within a cluster.
The spread policy makes sure tasks are distributed as much within a single cluster.

=======================================================================================================
Deployment as Code

In an immutable update, a new Auto Scaling group is created and registered with the load balancer.
Once health checks pass, the existing Auto Scaling group is terminated.

-- AWS CodeDeploy

If an AWS CodeDeploy configuration contains a file which already exists on the server,
the deployment will fail by default.
The CodeDeploy agent sends progress reports to the CodeDeploy service. The service does not attempt
to query instances directly, and the Amazon EC2 API does not interact with instances at the
operating system level.

-- AWS CodePipeline

The minimum number of stages required by a pipeline in AWS CodePipeline is 2.
In the Invoke pipeline action we can execute Lambda functions.

=======================================================================================================
Infrastructure as Code


=======================================================================================================
Configuration as Code

-- AWS OpsWorks
AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.

AWS OpsWorks Stacks is an application and server management service. With OpsWorks Stacks, you can
model your application as a stack containing different layers, such as load balancing,
database, and application server.
In AWS OpsWorks Stacks a custom cookbook repository location is configured for a stack. When
instances in the stack are first launched, they will download cookbooks from this location and run
them as part of lifecycle events.

AWS OpsWorks Stacks instance types:
- 24/7
- On demand
- Load-based
- Time-based - start and stop on specific time

AWS OpsWorks doesn't have a concept of cookbook caching.
OpsWorks lifecycle events don't allow you to specify cookbook versions.
After modifying cookbooks you must first run the Update Custom Cookbooks command.

A single OpsWorks task definition can describe up to 10 containers to launch at a time.
Task definitions should group containers by similar purpose, lifecycle, or resource requirements.

OpsWorks stacks instances are registered when they come online and deregistered when they are
moved to a different state.

=======================================================================================================
Authentication and Authorization

The AWS SDK relies on access keys, not passwords. The best practice is to use AWS Identity and Access
Management (IAM) credentials and not the AWS account credentials. Comparing IAM users or IAM roles,
only IAM users can have long-term security credentials.

We can use identity federation and IAM roles to establish a trust relationship between an
external Active Directory and AWS.

When you use identity federation to assume a role, The AWS security token service (AWS STS)
generates the access key ID, secret access key and session token.

The IAM trust policy defines the principals who can request role credentials from the AWS STS.
The long-term credentials are not limited to a single AWS Region. IAM is a global service, and IAM
user credentials are valid across different AWS Regions.

IAM policies are global in scope, so you do not need a custom one per AWS Region.

You can add IAM users to IAM groups but not IAM roles. Instead, roles must be assumed for short-term
sessions.

DynamoDBReadOnlyAccess policy - built-in policy that applies to the resource * wildcard, which means
that it applies to any and all DynamoDB tables accessible from the account regardless of when those
tables were created.

AWS Security Token Service (AWS STS) supports a number of different tokens:
- AssumeRole - short lived; 60 minutes by default, can be extended to 720 minutes
- GetSessionToken
- GetFederationToken

AWS AD Connector - AD Connector is a directory gateway with which you can redirect directory requests
to your on-premises Microsoft Active Directory without caching any information in the cloud.
For example, you can integrate your RADIUS-based MFA infrastructure that you already have on-premise.
It gives you the ability to configure changes to Active Directory on your existing Acitve
Directory console.

Using AWS as an identity provider (IdP) to access non-AWS resources allows you to use AWS
CloudTrail to audit who is using the service.

To use AWS Single Sign-On (AWS SSO), you must set up AWS Organizations Service and enable
all the features.
AWS SSO uses MS AD (either AWS Managed Microsoft Acitve Directory or Active Directory
Connector [AD Connector] but not Simple Active Directory).
AWS SSO does not support Amazon Cognito.
AWS SSO does not use SAML.

-- Cognito
Amazon Cognito supports device remembering and tracking
Amazon Cognito supports MFA authentication
Amazon Cognito supports Google SSO and MS AD and custom identity providers.You need SAML 2.0 based
identity provider to integrate with Amazon Cognito.

=======================================================================================================
Refactor to Microservices

-- Amazon SQS
Amazon SNS supports the same attributes and parameters as Amazon SQS.
To send a message larger than 256KB, you use Amazon SQS to save the file in Amazon S3 and then
send a link to the file on Amazon SQS. In this way you can send files larger than 256KB as S3 links
through SQS.
Amazon SQS queues do not support subscriptions.
Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages
that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your
application or messaging system because they let you isolate problematic messages to determine
why their processing doesn't succeed.

A queue without consumers deletes all the messages in it after the retention period has passed.

-- Amazon SNS

-- Amazon Step Functions

Amazon Step functions states:
- Pass
- Task
- Choice
- Wait
- Succeed
- Fail
- Parallel
- Map

-- Amazon Kinesis

Some of the Kinesis options
- Amazon Kinesis agent - a stand-alone Java agent The agent continuously monitors a set of files
and sends new data to your Kinesis Data Firehose delivery stream.
- Amazon Kinesis Producer Library (KPL) -
	https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html
- Amazon Kinesis Data Streams API

In Kinesis there is no limit in the number of consumers as long as they stay within the capacity
of the stream, which is based on the number of shards. For a single shard, the capacity is 2MB of
read or five transactions per second.

Amazon Kinesis Data Streams is a service for ingesting large amounts of data in real time and
for performing real-time analytics on the data.

=======================================================================================================
Serverless Compute

The default timeout value for an AWS Lambda function is 3 seconds.
The default limit for concurrent executions with Lambda is set to 1000. This is a soft limit that
can be raised. To do this, you must open a case through the AWS support center page and send a Server
Limit increase request.
The maximum execution time for a Lambda function is 300 seconds (5 minutes).

There are two types of policies with Lambda:
- function policy - defines which AWS resources are allowed to invoke your function
- execution policy - defines which AWS resources your function can access.

Ruby is not supported by AWS lambda functions.

We can send event payloads to an Amazon SQS queue or Amazon SNS topic if the lambda function fails
by enabling the dead-letter queue configuration option for the lambda.

The maximum amount of memory you can assign to an AWS Lambda function is 3008MB.

=======================================================================================================
Serverless Applications

Serverless applications contain both static and dynamic data

-- SAM
AWS SAM (Serverless application model) - it's based on Amazon Cloudformation and is optimized for
deploying serverless resources.
Properties for AWS Serverless Application Model:
- context - context object for a Lambda function
- handler - function handler for a Lambda function
- events - the events property allows you to assign a Lambda to an event source in Amazon API Gateway
- runtime - the language in which your AWS Lambda runs as


-- API Gateway
Amazon API gateway only supports HTTPS endpoints.
Without enabling CORS we're not going to be able to use the Amazon API Gateway service. You use a
stage to deploy your API, and a resource is typed object that is part of your API's domain. Each
resource may have an associated data model and relationships to other resourses and can respond to
different methods.

=======================================================================================================
Serverless Application Patterns

You can add a maximum of 20 nodes to an Amazon ElastiCache Redis cluster.

=======================================================================================================
Monitoring and Troubleshooting

The following are considered management events by AWS CloudTrail:
- modifying an Amazon S3 bucket policy
- creating an Amazon RDS instance

AWS CloudTrail events contain the following information:
- what request is being made
- when the request was made
- which resource was acted on
- who made the request

CloudTrail stores event history for 90 days back. If you would like to store this information
permanently, you can create a CloudTrail trail, which stores the logs in Amazon S3.

In Amazon CloudWatch data points with a period of 300 seconds are stored for 63 days.
Logging to Amazon CloudWatch from a EC2 instance requires installing the CloudWatch Logs agent.

CloudWatch alarms support triggering actions in Amazon EC2, EC2 Auto Scaling, and Amazon SNS. We can
trigger AWS Lambda functions from an alarm, but only by first sending the alarm notification to an
Amazon SNS topic.

The following EC2 metrics are available in CloudWatch:
- cpu utilization
- disk I/O
- network traffic - in/out
RAM cannot be tracked by CloudWatch because memory is allocated in a single block to an instance
and is managed by the guest OS, the underlying EC2 instance host doesn't have visibility
into consumption.
CloudWatch metrics support dashboards
The LookupEvents API action can be used to query event data.

With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon EKS clusters,
Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and
troubleshooting, implement pre-approved change work flows, and audit operational changes
for your groups of resources.

=======================================================================================================
Optimization

Multi availability zone is used for fault tolerance.
In case of slow reads from AWS RDS we can create a read replica and routing calls to it.

In AWS autoscaling group the instance user data is the space where you can place optional
scripts that can run after bootstrapping the application. This is used to perform.

By launching instances in multiple Availability Zones in the same region, you help protect your
applications from a single point of failure.

Provisioned IOPS volumes in the Amazon EBS are designed to deliver the expected performance only
when they are attached to an Amazon EBS optimized instance.

In AWS Cloudfront, some headers such as Date or User-Agent, significantly reduce the cache hit ration
(the proportion of requests that are served from a CloudFront edge cache).

In AWS S3, a multipart upload is supported and we can use it to upload big objects to S3.

One of the recommended best practices for AWS Lambda is to avoid using recursive code.

If you use SSE-KMS in S3 you may be impacted by the KMS limits.

S3 multipart upload is recommended for files >100MB.
It must be used for files >5GB.

S3 Transfer acceleration - increase transfer speed by transferring file to an AWS edge location
which will forward the data to the S3 bucket in the target region.
It's compatible with multipart upload

S3 byte-range fetches - parallelize GETs by requesting specific byte ranges

=======================================================================================================

Resources:
https://github.com/mostafac0des/certified-aws-developer-associate-notes
https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/
https://instances.vantage.sh/
https://aws.amazon.com/ec2/instance-types/
https://aws.amazon.com/caching/best-practices/
https://aws.amazon.com/builders-library/caching-challenges-and-strategies/

QWikLabs:
https://amazon.qwiklabs.com/catalog?keywords=introduction%20to&ransack=true&per_page=50

AWS Policy generator:
https://awspolicygen.s3.amazonaws.com/policygen.html
https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html
https://aws.amazon.com/cloudfront/features/

AWS ECS hands-on lab:
https://ecsworkshop.com/introduction/

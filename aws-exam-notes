AWS Cloud API and CLI

The AWS CLI creates the following two folders after configuration

~/.aws/config
~/.aws/credentials

CLI command to deserialize authorization message:
aws sts decode-authorization-message --encoded-message <message-string>

To use MFA with AWS CLI, you must create a temporary session.
To do so, you must run the STS GetSessionToken API call:
$ aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code \
code-from-token --duration-seconds 3600

Offical SDKs for AWS are present in the following languages:
- Java
- .NET
- Nodejs
- PHP
- Python (named boto3 / botocore)
- Go
- Ruby
- C++

Most AWS API services are regional in scope. The service is running
and replicating your data across multiple Availability zones within an AWS
region. You choose a regional API endpoint either from your default configuration
or by explicitly setting a location for your API client.

There are three types of AWS access keys:
- Console access
- Programmatic access (for AWS API calls)
- Temporary access

We're using the Python SDK when we use the AWS CLI.

us-east-1 - default region when configuring AWS CLI

API Rate limits - how many times you can call an AWS API in a row.
for example:
DescribeInstances API for EC2 has a limit of 100 calls per second
GetObject on S3 has a limit of 5500 GET per second per prefix

In case we hit the API rate limits:
- for intermittent errors - implement Exponential Backoff
- for consistent errors - request an API throttling limit increase

Service Quotas (Service Limits) - we can increase them on demand if we hit the limit
Service Quotas API - we can use it to request it programatically

ThrottlingException - happens intermittently, use exponential backoff - it retries calls with a
longer waiting interval. Thus, you're slowing down the load on your system.

The CLI will look for credentials in this order:
1. Command line options - --region, --output and --profile
2. Environment variables - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN
3. CLI credentials file - ~/.aws/credentials or C:\Users\user\.aws\credentials
4. CLI configuration file - ~/.aws/config or C:\Usersuser\.aws\config
5. Container credentials - for ECS tasks
6. Instance profile credentials - for EC2 instance profiles

The Java AWS SDK will look for credentials in this order:
1. Environment variables - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
2. java system properties - aws.accessKeyId and aws.secretKey
3. default creds profile file - ~/.aws/credentials
4. Container credentials - for ECS tasks
5. Instance profile credentials - for EC2 instance profiles

When you call the AWS HTTP API, you need to sign the request.
If you're making direct request to the AWS APIs you need to sign them
yourself using Signature v4 (SigV4)

The AWS Cost and Usage report contains the most comprehensive set of AWS cost and usage data
available, including additional metadata about AWS services, pricing and reservations (for example,
Amazon Elastic Compute Cloud (EC2) Reserved instances).

You can configure this report to present the data hourly or daily. It is the source of truth for the
billing pipeline.

In EC2 you can use tags to control permissions. Using IAM policies you can use the tag to gain precise
control over access to resources, ownership, and accurate cost allocation.

-- Trusted Advisor / -- AWS Trusted Advisor / -- Amazon Trusted Advisor

Trusted Advisor is an online tool that provides you with real-time guidance to help you provision your
resources by following AWS best practices.

Some cost-saving best practices:
- terminating your AWS Elastic Beanstalk environment before you terminate resources that
Elastic Beanstalk has created
- releasing Elastic IP addresses that are attracted to an instance that you terminated
- terminating your load balancer before you delete the Amazon EC2 instances that are registered with
it

Here are part of the checks AWS Trusted Advisor performs for us:
- unassociated Elastic IP addresses
- Amazon EBS provisioned IOPS (SSD) volume attachment configuration
- Amazon CloudFront header forwarding and cache hit ratio

-- AWS Cost Explorer

AWS Cost explorer reflects the cost and usage of Amazon Elastic Compute Cloud (Amazon EC2) instances
over the most recent 13 monthsand forecasts potential spending for the next 3 months. By using cost
explorer, you can examine patterns on how much you spend on AWS resources over time, identify
areas that need further inquiry, and view trends that help you understand your costs.
In addition, you can specify time ranges for the data and view time data by day or by month.

You can access your recommendations for Amazon EC2 Reserved instance purchases programatically
using the AWS Cost Explorer API. RI purchase recommendations are calculated based on your past use
and indicate opportunities for potential cost savings.
To tailor your recommendations, you can adjust the RI parameters (for example, return only 
recommendations for partial upfront convertible RIs with a one-year term) and the historical time
period of use over which your recommendations are calculated (for example, last 60 days).
You can access the AWS EC2 RI instances through the RI purchase console.
After you have identified the optimal mix of RIs to address your business needs, you can access them
from the Amazon EC2 RI purchase console or use the AWS SDK to act on your insights and purchase RIs.

=======================================================================================================
-- IAM and access control

Dynamic policy - a policy with a variable
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html

Inline vs Managed policies vs Customer Managed
Inline - strict one to one relation between principal and policy. If you delete the IAM principle
the policy will be deleted.
When we create an inline policy the max size of permissions assigned in total is 2048 bytes.
Managed policies are maintained by AWS - good for power users and administrators. Updated in case of
new services and new APIs
Customer Managed policy - re-usable, under version control

Trust policy
A JSON policy document in which you define the principals that you trust to assume the role.
A role trust policy is a required resource-based policy that is attached to a role in IAM.
The principals that you can specify in the trust policy include users, roles, accounts, and services.

IAM:PassRole - granting a user permissions to pass a role to an AWS service. A trust policy
for the role that allows the service to assume the role

In AD all the objects are organized into a tree. A group of trees is a forest.

AWS Directory services:
- AWS Managed Microsoft AD - it allows you to establish a "trust" connection with an on-prem AD setup
You cannot access the EC2 instances running an AWS Managed Microsoft AD setup, because it is a
managed service. It supports monitoring, daily snapshots and ability to sync with on-premises
Active Directory
- AD Connector - Directory Gateway (proxy) to an on-prem AD. Actual users are managed on-prem
- Simple AD - AD-compatible managed directory on AWS (not based on MS AD). It cannot be joined with
on-premise MS AD. It can be used to authenticate users against Amazon WorkDocs, Amazon Workspaces
and Amazon WorkMail

AWS access keys are typically used to sign API calls.

=======================================================================================================
Compute and Networking

-- EC2 

Amazon Elastic Compute Cloud (EC2) is an example of an architecture that includes a data plane and a
control plane. The data plane consists of physical servers where customersâ€™ Amazon EC2 instances run.
The control plane consists of a number of services that interact with the data plane, performing
functions such as these:

- Telling each server about the EC2 instances that it needs to run.
- Keeping running EC2 instances up to date with Amazon Virtual Private Cloud (VPC) configuration.
- Receiving metering data, logs, and metrics emitted by the servers.
- Deploying new software to the servers.

Although the actual names and responsibilities can vary across similar architectures,
these systems have two things in common:

- The data plane and the control plane need to stay in sync with each other. The data plane needs to
receive configuration updates from the control plane, and the control plane needs to receive operational
state from the data plane.
- The size of the data plane fleet exceeds the size of the control plane fleet, frequently by a factor
of 100 or more.

Regions - a set of datacenters
Availability zone - each region has a set of availability zones (from 2 to 6, the usual is 3)

Public IP addresses of EC2 instances change on each restart. Elastic IPs can be used, so that the same
IP is assigned on reach reboot.

Amazon EC2 instances can be resized but need to be stopped first.

When we create a new EC2 instance, a availability zone is the most specific place where you can place it.

ARN - Amazon Resource name

ap-southeast-2 - Syndney region
Sydney region availability zones:
ap-southeast-2a
ap-southeast-2b
ap-southeast-2c

Availability zones are a discrete data center with redundant power, networking and connectivity.
They are separate from each other, so that they're isolated from disasters.

AWS doesn't define the concept of disaster recovery zones. If a service is down, AWS designs and
builds Availability Zones to be fault tolerant.

IAM federation - big enterprises intergrate their own repository of users with IAM

Basic good practices for IAM roles:
- One IAM User per physical role
- One IAM Role per Application
- IAM credentials should never be shared
- Never write IAM credentials in code
- Never use ROOT IAM credentials in an application

Putting your security credentials on an AWS machine is bad practice.

SSH troubleshooting:
1) There's a connection timeout
This is a security group issue. Any timeout (not just for SSH) is related to security groups or a
firewall. Ensure your security group looks like this and correctly assigned to your EC2 instance.

Security groups can be reused and assigned to multiple EC2 instances.
They're locked down to a region/VPC combination.

A security group misconfiguration is usually responsible for time-outs.
A "connection refused" error is usually caused by an application error - the security group
in this case is allowing traffic properly.

All inbound traffic is blocked by default.
All outbound traffic is allowed by default.

We can reference security groups within other security groups.

Types of IPs in EC2:
- public IP
- private IP
- elastic IP

when you start and stop an EC2 instance it gets moved to a new physical host

When you stop and then start an EC2 instance, it can change its public IP.
If you need to have a fixed public IP for your instance, you need an Elastic IP.
You can attach 1 elastic IP at a time.

You can have at most 5 elastic ips. This can be increased after reaching out to AWS support.

Overusing elastic IPs is bad architecture. It's best to use random public IPs and remap a DNS record
to then.

We can mask the failure of an instance by switching the elastic ip to another instance but
overusing that pattern is bad practice.

EC2 instance metadata allows instances to "learn" about each other without additional IAM roles.
The URL for EC2 metadata is:
http://169.254.169.254/latest/meta-data
This URL will work only from an EC2 instance.
Using it you can retrieve the IAM role name from the metadata, but you CANNOT retrieve the IAM policy.

http://169.254.169.254/latest/meta-data/spot/termination-time

EC2 instance launch types:
- on demand instances: short workload, predictable pricing - billing per second, after the
	first minute. Highest cost but no upfront payment, no long term commitment. Recommended
	for short-term and un-interrupted workloads, where you can't predict how the application will
	behave
- reserved (minimum 1 year) - suitable for a workload we'll be sure we'll use for a minimal amount of
time - can have up to 75% discount compared to On-demand. You need to pay upfront for what you use
	with long term commitment. Reservation period can be from 1 to 3 years. You reserve a specific
	instance type. A server for database is a proper example for this type of instance
	- reserved instances - long workloads
	- convertible reserved instance - long workloads with flexible instances. We can change the
		EC2 instance type here. Up to 54% discount.
	- scheduled reserved instances: example - every Thursday between 3 and 6 pm. They can get
	launched periodically within a time period we specify.
- spot instances - short workload, cheap, losing the instance isn't fatal (less reliable)
	Can get a discount of up to 90% compared to on-demand.
	instances you can use any time if the max price you can pay from them is less than the
	current spot time.
	Useful if your workload is resilient to failure like:
	- batch jobs
	- data analysis
	- image processing
	Not recommended for critical purposes
- dedicated instances - no other customers will share the physical hardware
- dedicated hosts - book an entire physical server, control instance placement. Allocated for
your account for a 3 year period reservation

Valid spot instance behaviours on interruption:
- Stop interrupted spot instances
- Hibernate interrupted spot instances
- Terminate interrupted spot instances

EC2 payment options:
- Pay-as-you-go
- All Upfront
- On-Demand instance pricing
- Partial Upfront
- No upfront

You do not pay for an instance if the instance is stopped.

The amount of network throughput that can be handled is a property of the instance type.
If we want to icrease this for an EC2 instance, we need to stop it and change the instance type.

To ensure your application is prepared for spot instance interruption:
- use termination notices to monitor the status - you can monitor the status with them at 5-second
	intervals and then use the next 2 minutes to complete any needed processing before the
	instance is terminated
- split up the work using Amazon SQS to monitor and track the completion status - this can help
	resume work at a later point
- use the Gold Image of the AMI so that instances are ready to go as soon as a spot request is fulfilled

ENI (Elastic Network interface):
- can have primary private IPv4, one or more secondary IPv4
- can have one elastic IP (IPv4) per private IPv4
- can have one public IPv4
- can have one or more security groups
- can have a MAC address
- we can create ENIs independently and move them inbetween EC2 instaces for failover (in the same
availability zone)
- ENIs are bound to a specific availability zone

Adding additional ENIs to an instance doesn't increase the network throughput that can
be handled by it.

AMIs are built for a specific AWS region!

AMIs have the following characteristics:
- RAM
- CPU
- I/O (disk performance)
- network
- gpu

IAM Roles can be attached to EC2 instances.
IAM Roles can come with a policy authorizing exactly what the EC2 instance should be able to do

-- AWS VPC / -- Amazon VPC / Networking & VPC

VPC - private network to deploy your resources (regional resource)
Subnets - they allow you to partition your network inside your VPC (Availability Zone resourse). They
are defined at the availability zone level.
A public subnet is a subnet that is accessible from the internet.
A private subnet is a subnet that is not accessible from the internet.
To define access to the internet and between subnets, we use RouteTables.
Internet gateway - helps our VPC instances connect to the internet
Public subnets have a route to the internet gateway. This is what makes a subnet a public subnet.
NAT gateways (AWS-managed) and NAT instances (self-managed) allow your instances in your Private
Subnets to access the internet while remaining private.

NACL (Network ACL) - firewall that controlls traffic from and to subnet
It can have allow and deny rules. Those rules are attached at the subnet level. Those rules only
include IP addresses.
The default NACL allows everything and everything out

Security groups - a firewall that controlls traffic to and form an ENI / an EC2 instance. It can only
have allow rules
Rules include IP addresses, ports and other security groups

https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison

VPC flow logs - captures info about traffic going into your interfaces:
- vpc flow logs
- subnet flow logs
- ENI flow logs
- all AWS managed interfaces - ELB, RDS, ElastiCache etc
Helps to monitor and troubleshoot connectivity issues:
- subnets to internet
- subnets to subnets
- internet to subnets

VPC Peering - connect two VPC, privately using AWS' network. This makes them behave as if they were
in the same network. For this to work we must not have overlapping CIDR (IP address range)
VPC peering is not transitive - it needs to be established for each VPC that need to communicate
with one another. E.g.
if VPC A is connected to both VPC B and VPC C, then VPC B and VPC C cannot communicate with one
another.

Endpoints allow you to connect to AWS Services using a private network instead of the public www
network. This gives you enhanced security and lower latency to access AWS resources.

VPC endpoint gateway - allows access to external resources from the private network
VPC endpoint interface - the rest of the service (only used within your VPC)
We can create a VPC endpoint interface (ENI) and from it we can have private access to a public
service

VPC endpoint is the usual way referred on the exam when we need to connect a private AWS service
to the external world.

There are two ways to connect an on-premise private network to a VPC:
- site to site VPN
- Direct Connect - physical connection. It's private, doesn't go over the public internet and is
fast. It takes atleast a month to establish

Both Direct Connect and Site to Site VPN cannot access VPC endpoints

Even though each instance in an Amazon VPC has a unique private IP address, you could assign the same
private IP address ranges to multiple VPCs. Therefore, two EC2 instances in different VPCs could have
the same IP addresses.

All EC2 instances connecting through a private subnet share the public IP address of the NAT.

=======================================================================================================
ELB - Elastic Load balancer

ELB seamlessly handles failures of downstream instances.
It does regular health checks on your EC2 instances.
It provides SSL termination for your instances.
It enforces stickiness with cookies.
High availability across zones.
It separates public from private traffic.

There are 4 types of load balancers:
- classic load balancer (v1) - http, https, tcp
- application load balancer (v2 - new generation) - http, https, websocket, http/2
- network load balancer - tcp, tls, udp
- gateway load balancer (not on the exam)

ELBs can be internal or external.

Load balancer 503 - load balancer is at max capacity or no target is registered

ALB supports routing based on different target groups.
ALBs can stand in front of lambda functions
ALB supports multi-header values. They're useful when working with lambdas.
They're syntax contains of multiple GET parameters with the same key. They're
converted as an array in the lambda context.

Dynamic port forwarding - useful in ECS setups

NLBs are used when you want extreme performance and you want to use a protocol different than HTTP(S)
It can also be useful when you want a static IP assigned.

Load balancer stickiness - the same client is always redirected to the same actual instance behind
the load balancer. This feature is available for CLB and ALB.
The "cookie" used for stickiness has an expiration date you control.
Stickiness may bring imbalance to load balancing overloading some of the downstream instances.
Stickiness for ALB is configured at the target group level.
For CLBs stickiness is set at the load balancer level.

With cross-zone load balancing each load balance instance distributes evenly the calls across all
registered instances in all AZs.
Otherwise, it'll distribute them only to the instances in its own availability zone.
This is disabled by default in CLB. No additional charges for it if enabled.
For ALB this is the default behaviour and cannot be disabled.
For NLB this is disabled by default. You'll pay additional charges if enabled.

Load balancers use an X.509 certificate
ACM - AWS certificate manager - this is where we manage SSL certificates

SNI - ability to have multiple SSL certificates on the same host. This is achieved by the
client specifying which hostname of a target server it wants to reach.
SNI works with Cloudfront, ALB and NLB. Does not work with CLB.

Connection draining (aka Deregistration delay for target groups in ALBs and NLBs) - this is the
time to complete "in-flight requests" while the instance is de-registering or unhealthy.
The ELB stops sending new requests to the instance being drained.
The ELB won't start new connections to an instance being drained and it'll give it 300 seconds by
default to complete the requests
It can be between 1 and 3600 seconds.
Can be disabled by setting it to 1 second.

Auto Scaling Group parameters:
- minimum size
- actual size / desired capacity
- maximum size - how many instances can be added at scale out
scale out - add instances
scale in - remove instances
Auto Scaling Alarms - they determine when to trigger a scale in / out. It is possible to scale
an ASG based on CloudWatch alarms. CloudWatch metrics are computed overall for all ASG instances.
We can also use metrics directly from the EC2 instances.
We can also use custom metrics sent to CloudWatch from our EC2 instances.
ASGs use Launch configurations or Launch Templates (newer)
To Update an ASG, you must provide a new launch configuration / launch template.
IAM roles attached to an ASG will get assigned to EC2 instances
ASG are free. You pay for the underlying resources being launched.

ASG scaling policies:
- target tracking scaling
- simple / step scaling policy
- scheduled actions

ASG Cooldowns - the cooldown period ensures that your ASG doesn't scale in or out additional instances
before the previous scaling takes effect

=======================================================================================================
Storage

When you need to move massive amounts of data from on-prem to AWS you can use AWS Snowball.

-- Amazon S3 / -- AWS S3 / -- S3

Advertised as "infinitely scaling" storage.

Versioning is a means of keeping multiple variants of an object in the same bucket. You can use
versioning to preserve, retrieve, and restore every version of every object stored in your Amazon
S3 bucket. With versioning you can easily recover from both unintended user actions and application
failures. The following are version states of buckets:
- Versioning suspended
- Versioning disabled
- Versioning enabled

S3 bucket names are globally unique and cannot be changed after a bucket is created.
Thus, when you change the Name field of a AWS::S3::Bucket resource, a replacement bucket is
created when changing this property.

Data in S3 is automatically replicated within a region.

Any file that is not versioned prior to enabling versioning will have version "null".
Versioning is enabled at the bucket level.
Suspending versioning in your bucket doesn't delete the previous versions.

We can use cost allocation S3 bucket tags and S3 analytics to review the storage classes
and access patterns usage to help reduce costs.

S3 costs may vary between regions.

There are 4 ways to encrypt objects in Amazon S3:
- SSE-S3 - encrypts S3 objects using keys handled & managed by AWS - encryption using keys handled
	and managed by Amazon S3. The object is encrypted server side. AES-256 is the type of encryption
	To use it you must set a header
	"x-amz-server-side-encryption":"AES256"

- SSE-KMS - leverage AWS Key Management Service to manage encryption keys. This gives us user control
	and audit trail. Those are the advantages over SSE-S3. The object is encrypted server side. To
	use it we must set a header:
	"x-amz-server-size-encryption":"aws:kms"

- SSE-C - when you want to manage your own encryption keys. In this case Amazon does not store the
	key you provide. HTTPS must be used to use this method. You'll need the same key to decrypt the
	files manually.

- Client-side encryption - keys and encryption are performed by the user. Some libraries like
	S3 encryption client can help with this. Client should encrypt and decrypt the data
	themselves.

Amazon S3 can use both HTTP or HTTPS.

Amazon S3 policies can restrict access based on the following:
- IP address range
- AWS account
- objects with a specific prefix

Each S3 bucket must have a globally unique name. S3 is a global service but buckets are
defined at the region level.

S3 bucket websites don't support HTTPS.

Naming convention:
- no uppercase
- no underscore
- 3-63 characters long

S3 objects (files) have a key - the key is the full path to the file:
s3://my-bucket/my_file.txt

The key is composed of:
uri + bucket name + prefix + object name

There's no concept of directories in s3, but the UI can trick you to think there are.
There are just long names with keys that contain "/".

An object value is the content of the file.

Max object size is 5000GB (5TB).

If you upload more than 5GB, you need a multi-part upload.

Each object has metadata - list of key / value pairs.
It also has tags (they're something separate).

The following are a deciding factor when choosing an AWS Region for your bucket:
- Cost - prices are different between regions
- Latency
- Regulatory requirements - different regions may have different legal requirements

Pre-signed URLs allow you to grant time-limited permission to download objects from an Amazon S3
bucket.
They can be generated with the CLI or SDK.
A user given a pre-signed URL will temporarily have the permissions of the IAM role that
generated the URL.

aws s3 presign s3://next-level-bucket/coffee.jpg --region eu-central-1

In case of issues with encrypted files:
aws configure set default.s3.signature_version s3v4

If data must be encrypted before being sent to Amazon S3, client-side encryption must be used.

Amazon S3 supports server access logging for buckets that tracks access time and who accessed the
bucket.

Amazon S3 bucket policies can specify a request IP range, an AWS account and a prefix for objects
that can be accessed.

S3 Security types:
- user-based - based on IAM policies
- resource-based
	- bucket policies - bucket-level rules (JSON based)
	- Object ACL - finer grain at the object level
	- bucket ACL - less common

S3 storage types:
- Standard - General Purpose
	High durability of objects across multiple AZ
	Use cases: big data analytics, mobile & gaming apps, content distribution
- Standard-infrequent access (IA) - suitable for data that is less frequently accessed, but
	requires rapid access when needed
	It has a lower cost than S3 Standard
- One Zone-infrequent access - same as IA but stored in multiple AZs
	99.5% availability
	Supports SSL for data at transit and encryption at rest
	Low cost compared to IA (by 20%)
	Use cases: secondary backup or storing data you can recreate
- Intelligent Tiering - useful when the access pattern is unknown
	small monthly monitoring and auto-tiering fee
	Automatically moves objects between access tiers based on changing access patterns
	Designed for customers who want to optimize storage costs automatically when data
	access patterns change, without performance impact or operational overhead.
	It delivers automatic cost savings by moving data between two tiers - frequent
	access and infrequent access - when access patterns change
- Glacier - low cost object storage meant for archiving and backups
	Each item in Glacier is called an archive. Max archive size is 40TB
	Arcyives are stored in vaults, not buckets
- Glacier Deep archives
- Reduced Redundancy Storage

An IAM principal can access an S3 object if the user IAM permissions allow it OR
the resource policy ALLOWS it.

S3 bucket policies are attached to buckets.
When evaluating if an IAM Principal can perform an operation X on a bucket,
the union of its assigned IAM Policies and S3 Bucket Policies will be evaluated.
The permissions from the S3 bucket policy have higher priority when they're
explicit DENY policies.

S3 supports VPC endpoints for instances in VPC without www internet.
S3 Access logs can be stored in other S3 buckets.
S3 API calls ban be logged in AWS CloudTrail.

Amazon S3 access logs: https://docs.aws.amazon.com/AmazonS3/latest/userguide/LogFormat.html
Setting the monitored bucket to be where you store your access logs is bad practice.

Amazon S3 server logs can be used to record the following fields

- Bucket Owner
- Bucket
- Time
- Remote IP
- Requestor
- Request ID
- Operation
- Key
- Request-URI
- HTTP Status
- Error Code
- Bytes Sent
- Object Size
- Total Time
- Turn-Around Time
- Referer
- User-Agent
- Version Id
- Host Id
- Signature Version
- Cipher Suite
- Authentication Type
- Host Header
- TLS version

For creating a S3 Batch operations job the following need to be specified:
- Operation
- Manifest
- Priority
- RoleArn
- Report
- Tags
- Description

Amazon S3 access points can be created to provide access to a large number of objects
in Amazon S3 buckets. Access points ARN has format as below:
- arn:aws:s3:region:account-id:accesspoint/access-point-name/object/resource - object
is keyword here

S3 supports replication:
- CRR - cross-region replication
To enable it we need the source and destination buckets to have versioning enabled. It
also needs an AWS Identity and Access Management (IAM) role in the account that
grands Amazon S3 permission to replicate objects on your behalf

	Here are some reasons to enable cross-region replication:
	- minimize latency when your customers are in two geographical zones
	- your compliance requirements dictate that you store data at an even further distance
	than Availability Zones, which are tens of miles apart
	- you have compute clusters in two different AWS regions that analyze the same set of objects

- SRR - same region replication
To enable it we must enable versioning in source and destionation buckets.
The buckets can be in different accounts.
Copying is asynchronous.
Proper IAM permissions must be granted.
It's not retroactive - only the files added after enabling it will be replicated.
Any delete operation is not replicated.
You cannot "chain" replication. If bucket 1 has replication in bucket 2 and bucket 2
has replication in bucket 3, then objects created in bucket 1 aren't replicated in 3.

For enabling replication for an Amazon S3 bucket, replication configuration is required to be
added. While creating replication configuration, multiple rules can be added based upon the subset
of objects to be matched. Filter for these rules can be based upon matching objects tag
or object key prefixes or any combination of both.

S3 supports MFA delete to prevent accidental deletion of objects.
Only the bucket owner (root account) can enable/disable MFA-Delete
This can be enabled only using CLI or AWS API calls.

aws s3api put-bucket-versioning --bucket mfa-delete-lab --versioning-configuration Status=Enabled,MFADelete=Enabled \
--mfa "mfa-device-arn mfa-device-code" --profile mfa-delete-lab

aws s3api put-bucket-versioning --bucket mfa-delete-lab --versioning-configuration Status=Enabled,MFADelete=Disabled \
--mfa "mfa-device-arn mfa-device-code" --profile mfa-delete-lab

The s3 cli supports also pagination with --page-size

S3 consistency model:
- read after write consistency for PUTS of new objects - as soon as a new object is
written, we can retrieve it (PUT 200 => GET 200)
- this is true unless we've queried the bucket before uploading the file:
(GET 404 => PUT 200 => GET 404) - eventually consistent
- eventual consistency for DELETES and PUTS of existing objects:
if we read an object after updating, we might get the older version
ex: (PUT 200 => PUT 200 => GET 200 (might be older version))
if we delete an object we might still be able to retrieve it for a short period
ex: (DELETE 200 => GET 200)

There's no way to request "strong consistency" in Amazon S3!

S3 Cors - Cross-Origin Resource Sharing

Access-Control-Allow-Origin

An origin is a scheme (protocol), host (domain) and port:
e.g. https://www.example.com

If a client does a cross-origin request on our S3 bucket, we need to enable the
correct CORS headers

Lifecycle rules

transition actions - it defines when objects are transitioned to another storage
class
expiration actions - delete an object after some time

Lifecycle rules can be applied for a certain prefix - s3://mybucket/mp3/*

The prefix of a file is the part of the name between the bucket name and filename

S3 Select & Glacier Select - perform simple SQL-like queries against files in a bucket

We can use Amazon SQS and Amazon SNS to send messages on events like
S3:ObjectCreated, S3:Objectremoved, S3:ObjectRestore, S3:Replication etc.

You need to enable versioning to notify on every successful write.

S3 Athena supports ODBC and JDBC
In S3 Athena you get charged per query and amount of data scanned.
S3 Athena is based on Presto (https://prestodb.io/)

For the exam, anytime you need to analyze data directly on S3, usually Athena is the solution.

S3 Object Lock / Glacier Vault Lock - creates a write once-read many (WORM) object type
You can use WORM protection for scenarios where it is imperative that data is not changed
or deleted after it has been written.
Object lock is applied to a specific version of an object. So, there are multiple
versions of an object, users can overwrite objects that do not have an object lock applied
to them
WORM model - Write Once, Read Many - block object change/deletion for a specific amount of time

S3 can be used for session state cache but it's not meant for small objects
because there are latency costs.

Here are some reasons to enable cross-origin replication on an Amazon S3 bucket:
- minimize latency when your customers are in two geographic regions
- your compliance requirements dictate that you store data at an even further
distance than Availability Zones, which are tens of miles away.
- you have compute clusters in two different AWS Regions that analyze the
same set of objects

When you need to grant a user outside your AWS account access to an object in S3
the best way to grant access is to have him create a user ID using a third-party
identity provider (idP) and based on that idP assign a policy that permits
access.

Retrieval option 	| S3 Glacier 	| S3 Glacier Deep Archive |
____________________|_______________|_________________________|
Expedited			| 1-5 min		| Not supported           |
Standard 			| 3-5 hours 	| 12 hours 				  |
Bulk 				| 5-12 hours 	| 48 hours 				  |

Replication configuration:
https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-add-config.html

-- Amazon Elastic Block Store (EBS) / -- EBS / -- AWS EBS / -- Amazon EBS

Amazon snapshot EBS data is loaded lazily.
EBS volumes are scoped within a specific availability zone. You cannot attach an EBS volume from
us-east-1a to us-east-1b
An EBS volume can be accessed only by a single EC2 instance, as opposed to EFS volumes.
To use an EBS volume in another availability zone we need to create a snapshot of it
and restore it in another AZ.

Depending on the volume and instance types, you can use Multi-Attach to mount a volume to
multiple instances at the same time.

EBS backups use IO and you shouldn't run them while your application is handling a lot
of traffic.

EBS volume types:
- GP2 (SSD) - recommended for most workloads, system boot volumes and virtual desktops,
	low-latency interactive apps, development and test environments
	Size is between 1GiB to 16TiB
	A small gp2 volume can burst IOPS to 3000
	Max IOPS is 16k
- IO1 (SSD) - critical business applications that require more than 16000 IOPS per volume
	Suitable for large database workloads (Oracle SQL, MS SQL etc)
	Designed for transactional, IOPS-intensive database workloads, boot volumes
	4GiB - 16TiB
	Max IOPS are 32000
- ST1 (HDD) - good for big data, data warehouses, log processing
	good for streaming workloads that require fast throughput at a low price
	Cannot be a boot volume
	500 GiB - 16 TiB
	Max IOPS is 500
- SC1 (HDD) - the cheapest option. Good for less frequently accessed data
	500GiB - 16TiB
	Max IOPS is 250
	Max throughput is 250 MiB/s
EBS volume types are characterized by Size and Throughput (I/O Ops per second)
Only GP2 and IO1 can be used as boot volumes

Instance store = ephemeral storage - it gets destroyed when the instance is destroyed.
It is physically attached to the EC2 instance, while EBS is a network drive.
It has better performance and good for cache / scratch data.
It's not an option if you want to persist data inbetween instance terminations
it survives reboots but not instance terminations
It cannot increase in size, as opposed to EBS
There's a risk of data loss in instance store if the physical hard disk fails.

The VolumeQueueLength metric provides the number of read and write operation
requests waiting to be completed in a specified period of time. In other
words, it monitors the number of requests sent for processing. The
queue depth is the number of pending I/O requests from your application to
your volume.

Encryption operations occur on the servers that host EC2 instances, 
ensuring the security of both data-at-rest and data-in-transit between an 
instance and its attached EBS storage.

-- Amazon Elastic File System (EFS)

A managed NFS that could be mounted on many EC2 instances at the same time
across many different availability zones (EBS is availability zone dependent).
It's pay per use and is 3x more expencive than gp2 ebs volumes.

To use EFS you need security groups.
It works with Linux-based AMIs only.

You can use KMS keys to encrypt the file system.
It scales automatically. Users don't plan the capacity.

Amazon EFS supports one to thousands of Amazon EC2 instances conneting to a file system concurrently.

Performance modes:
- general purpose - appropriate for most use cases and selected by default
- max I/O - good for big data
The Max I/O performance mode of EFS is optimized for applications where tens, hundreds, or
thousands of EC2 instances are accessing the file system. It scales to higher levels of
aggregate throughput and operations per second with a trade-off of slightly higher latencies
for file operations.

Setting up RAID 0 with EFS shares can increse overall I/O.

Therea are different storage tiers in EFS - lifecycle management. We can move files
between EFS volumes after a number of days:
- standard
- infrequent access (EFS-IA): cost to retrieve files, lower price to store

EFS is also used for session cache.

-- Amazon Glacier
Glacier can be used as a standalone service and as an Amazon S3 storage class.

=======================================================================================================

-- Amazon CloudFront

Caches content at edge locations (216 point of presence globally - edge locations).
It also adds DDoS protection, integration with AWS Shield and AWS WAF

https://aws.amazon.com/cloudfront/features/

CloudFront Origin Access Identity
CloudFront can be used as an ingress to S3 (upload files from anywhere in the world).
Any http backend can be used with CloudFront.

ALB or EC2 when used as origins require their security groups to allow CloudFront.

Geo Restriction - whitelist or blacklist users only from a specific region. It's using a
third party GeoIP database for this purpose.

In CloudFront we can cache based on multiple things:
- Headers
- Session Cookies
- Query String parameters

The cache lives at each CloudFront Edge Location.

TTL can be from 0 seconds to 1 year. It can be set by the origin using the Cache Control header
or Expires header

You can invalidate part of the cache using the CreateInvalidation API

Viewer protocol policy - enables SSL between your client and your edge location. We can
redirect HTTP to HTTPS or enable HTTPS only.

Origin protocol policy - the HTTP(S) policy for the target. It can be HTTPS only or match
the viewer's protocol policy.

S3 bucket websites don't support HTTPS.

To distribute private content with CloudFront we can use a Signed URL / Signed Cookie. We can
attach a policy with:
- Includes URL expiration
- Includes IP ranges to access the data from
- Trusted signers (which AWS accounts can create signed URLs)

A signed URL gives access to individual files. The difference with S3 pre-signed URLs is that
CloudFront signed URLs give access to a path no matter the origin.
S3 pre-signed URLs issue a request as the person who pre-signed the URL.

Two types of signers:
- trusted key group
- aws account that contains a CloudFront key pair (not recommended because only root
accounts can be used)

Origin groups - increase high-availability and do failover. This is a group of EC2 instances.
If the primary origin fails, the second one is used.

Field level encryption

Some headers, such as Date or User-Agent, significantly reduce the cache hit ration (the proportion
of requests that are served from a CloudFront edge cache). This increases the load on your origin
and reduces performance because CloudFront must forward more requests to your origin.

=======================================================================================================
Docker in AWS (ECR, ECS)

Task Definition - metadata in JSON which defines how ECS runs a docker container or a set of docker
containers. Similar to K8S pods. Max number of containers defiend in a task definition could be 10.

Amazon ECS service discovery automatically assigns Amazon Route 53 DNS entries for tasks managed by
a service. A private namespace is created for each Amazon ECS cluster. As tasks are launched or
terminated, the namespace is updated to include entries for each task.

Task Role - optional IAM role that allows a container to talk to AWS services. Often, on the exam,
if a container is not allowed to do something it lacks a proper Task Role. It is defined in the
task definition.

ecsInstanceRole - this role gets attached to each ECS Task
ecsServiceRole - this role gets attached to each ECS Service and allows it to register and
deregister instances as ELB targets

ECS Services define how much tasks shall we run and how should they run.
They can be linked to ELBs if needed.

EC2 Instance profile - used by the ECS agent to make API calls to the ECS service and send container
logs to CloudWatch and also pull images from ECR.

We can only add a load-balancer to an ECS service at service creation. We cannot add it by editing
a service.
Only ALB load balancer supports dynamic port mapping for ECS services.

Access to ECR is controlled through IAM policies

There are two ways to authenticate against ECR
- AWS CLI v1 login command:
	$(aws ecr get-login --no-include-mail --region eu-west-1)
- AWS CLI v2 login command:
	aws ecr get-login-password --region eu-west-1 | docker login --username AWS -- password-stdin | \
	123.dkr.ecr.eu-west-1.amazonaws.com

When a task of type EC2 is launched, ECS must determine where to place it, with the constraints
of CPU, memory and available port. To define this we need to define a task placement strategy
and task placement constraints. This doesn't work on Fargate yet.

A single task definition could contain up to 10 containers.

ECS Task placement strategies:
- binpack - place task based on the least available amount of CPU or memory. This minimizes
	the number of EC2 instances in use
- random - places the task randomly
- spread - place the tasks evenly based on the specified criteria
Task placement strategies can be mixed together

ECS Task placement constraints:
- distinctInstance: place each task on a different EC2 instance
- memberOf: places task on instances that satisfy an expression

Cluster Query Language

ECS - Service auto scaling:
- target tracking
- step scaling
- scheduled scaling

The main difference in target tracking scaling and step scaling is that in target
tracking scaling we have a single threshold. In step scaling we have
multiple thresholds.

Cluster capacity provider - determines the infrastructure that the task runs on. It scales
the EC2 cluster we run ECS on.

bind mounts - sharing data between containers in a task. Can be used to share ephemeral storage
between application container and sidecar. The sidecar can be used to send metrics/logs to other
destinations (separation of concerns).

/etc/ecs/ecs.config - EC2 confing file for the ECS agent

ECS security groups work at the EC2 instance level.

ECS does provide integration with AWS CloudWatch Logs.
You need to setup logging at the task definition level.
Each container will have a different log stream.

ECS_ENABLE_TASK_IAM_ROLE - a variable defined in /etc/ecs/ecs.config that can be used for adding
additional IAM roles to an EC2 ECS instance

=======================================================================================================
Databases

-- AWS RDS / -- RDS / -- Amazon RDS

Amazon RDS manages for the developer the following things out of the box:
- database software installation and patching
- hardware provisioning
- backups
You are still responsible for managing the database settings reuired by the application.

AWS RDS supports security groups.

Enabling the Multi-AZ setup of AWS RDS enables a setup to have synchronous 

An existing Amazon RDS instance is deleted if the environment is deleted. There is no auto-retention
of the database instance. You must create a snapshot to retain the data and to restore the database.

You cannot SSH into the EC2 instances hosting AWS RDS instances. This is a managed service
RDS EC2 instance storage is backed by EBS (gp2 or io1)

RDS backups are enabled by default. There are daily full backups in a maintenance window.

Transaction logs are backed-up by RDS every 5 minutes

RDS DB snapshots differ from backups with being triggered by the user.

We can create up to 5 read replicas in the same or other AZ or other region.
Async replication is performed when we enable read replicas.
This means that the reads are "eventually consistent".

Each replica can be promoted to a stand-alone DB instance.

Applications need to update their connection string to leverage read replicas.

In AWS There's a network cost when data goes from one AZ to another.
To reduce the network cost you can have a read replica in the same AZ (if the usecase allows it).
It'll be free in such case.

A disaster recovery instance is a database "copy" that receives sync replication from the
main database. When the main database goes down the replcia starts to handle
read and write queries. The DNS record gets automatically redirected to it.

Read replicas can be used in such a way as well for disaster recovery.

AWS RDS supports IAM authentication - an IAM user could be created as a database user (it
works for MySQL and PostgreSQL).

AWS RDS supports encryption with AWS KMS - AES-256 encryption.
It needs to be defined at launch time.

If the master is not encrypted, the read replicas cannot be encrypted.

it also supports in-flight encryption using SSL.

To enforce SSL:
in postgresql: rds.force_ssl=1 in the AWS RDS console (Parameter groups)
in mysql:
GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SSL;

Snapshots of un-encrypted RDS databases are un-encrypted.
Snapshots of encrypted RDS databases are encrypted.

To encrypt an un-encrypted RDS database:
1. create a snapshot of the un-encrypted database
2. copy the snapshot and enable encryption for the snapshot
3. restore the database from the encrypted snapshot
4. migrate applications to the new database, and delete the old one

RDS databases are usually deployed within a private subnet.
RDS leverages security groups to expose the database to the internet.

Transparent Data Encryption (TDE) - available only for MS SQL and Oracle SQL.
This feature encrypts data at rest - db files both on the hard drive and backup.

If you create a database through AWS Beanstalk and delete your beanstalk
application your database will be deleted, as well.

If you would like to delete an AWS::RDS::DBInstance resource using
CloudFormation and the underlying Amazon Relational Database Service
(Amazon RDS) database instance, the recommended delition policy would be
Snapshot in order to ensure no data is lost.

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html

-- AWS Aurora

AWS Aurora is compatible with both PostgreSQL and MySQL clients.
AWS claim 5x times improvement over MySQL performance and 3x over PostgreSQL.

The Aurora size can be between 10GB and come up to 64GB.

Aurora can have up to 15 replicas.
Aurora is HA native.
Aurora costs 20% more than other RDS engines.

Aurora stores copies of your data as read replicas as twice as the AZs in which
they're placed.
E.g. 6 copies of your data across 3 AZ.
4 copies out of 6 are needed for writing.
3 copies out of 6 are needed for reads.
Aurora does some peer to peer self-healing.
Storage is striped across 100s of volumes.
One Aurora instance makes writes (master).
If the master is down, a new one is promoted in less than 30 seconds.
Aurora supports cross-region replication.
Aurora provides a "writer endpoint" - a DNS record always pointing to the master.
Aurora provides a "reader endpoint" - a DNS record pointing to all read replicas.
Aurora shared storage volumes are auto expanding from 10G to 64TB.

Aurora backtrack - a feature which allows you to restore the database to any point in time.

Aurora serverless - automated database instantiation and auto-scaling based on actual usage

Aurora global database - 1 primary region and up to 5 secondary read-only regions.

up to 16 read replicas per secondary region.

-- AWS ElastiCache

ElastiCache for Memcached doesn't support a high-availability configuration.
Only ElastiCache Redis could run for in a high-availaiblity configuration.
ElastiCache Redis supports data durability using AOF persistence (append only file).

ElastiCache Memcached - supports multi-node setup for partitioning of data (sharding)
It's non-persistent (no AOF - Append Only File).

ElastiCache Redis supports backup, high-availability
and restore with AOF, opposed to ElastiCache memcached.
Memcached has multi-threaded architecture.

Caching strategies:
- Lazy loading / Cache aside / Lazy population - update cache on retrieval if an item is missing there
- Write-through cache - update the cache after the database is updated

Both are usually combined.

Cache churn - a data is present in cache that will never be accessed

Evicting cache:
- explicitly delete the item
- item is evicted because there's not enough space in cache (LRU)
- the TTL of the item expires

If too many evictions happen due to memory, you should scale up or out

Setting a TTL is good practice, except when you're using write-through

-- Other

Amazon Redshift - managed data warehouse solution
	S3DistCp - Redshift feature that allows it to copy data from S3 to the Redshift cluster

The leader node in Amazon Redshift is used to run SQL queries.
The leader node acts as the SQL endpoint and receives queries from client applications,
parses the queries, and develops query execution plans.
The compute nodes execute the query execution plan. The leader node executes the actual
query.

Amazon Neptune - NoSQL graph database for highly connected datasets.

-- Amazon DynamoDB / -- DynamoDB / -- AWS DynamoDB

The difference between the relation databases and the dynamodb architecture is that
it keeps related data together.

Amazon DynamoDB stores data in partitions. A partition is an allocation of storage for a table, 
backed by solid state drives (SSDs) and automatically replicated across multiple Availability
Zones within an AWS Region.

In DynamoDB an inefficient design for a table would involve chosing a partition key when there
are only a few values for that partition key. E.g. chosing a status code where there are only a
few possible status codes.

Storing normalized data in a single table is the optimal way to use DynamoDB.

In DynamoDB scans are less efficient than queries.
In DynamoDB you can create local secondary indexes only when you are creating the table.
Server-side encryption provided by the DynamoDB service is a recommended way to protect data at rest
stored in DynamoDB.
Each record in DynamoDB Streams appears only once in a stream.
400KB - this is the maximum DynamoDB item size limit.
QueryTable is the DynamoDB operation used to find items based on primary key. It
also returns metadata beside the document.
UpdateTable is the DynamoDB operation used to modify the provisioned throughput settings, global
secondary indexes, or DynamoDB Streams settings for a given table.
Scan is the DynamoDB operation used to read every item in a table.
The lifetime of data in a DynamoDB stream is 24 hours. This cannot be changed.

Global tables allow DynamoDB tables to have the same table in multiple regions.

There are two types of read operations:
- strongly consistent read
- eventually consistent read

These are the following three operations used to read data in DynamoDB:
- GetItem
- Scan
- Query

Server-side encryption at rest is enabled on all DynamoDB table data and cannot be disabled.

Amazon DynamoDB Accelerator (DAX) - write-through and read-through caching service for DynamoDB.

Primary keys:
- partition key only (HASH) - must be unique for each item. it needs to be "diverse" enough so
data will be distributed
Example: user_id for a users table (hash key + range)
- partition key + sort key - the combination must be unique. Data is grouped by partition key
Sort key = range key
Example: users_games table - user_id for partition key and game_id for sort key

Only partition keys can be not null.

Throughput can be exceeded temporarily using "burst credit". If burst credit are empty, you'll get
a "ProvisionedThroughputExceededException". In such case we can use an exponentional backup strategy
A reason may be "hot keys" - a key is looked up too many times.
We can increase the WCUs and RCUs for a dynamodb table to prevent this exception in some cases.
It may not be the most optimal solution because in case of more demand we may need to increase them
again.
We can also use AWS SDK to query the database from our app because it uses a jitter-based
backoff algorithm.

The default WCUs and RCUs for a DynamoDB table are set to 5 by default upon table creation.
Before adaptive captivity, DynamoDB allocates read and write throughput evenly across partitions.

https://www.bluematador.com/docs/troubleshooting/aws-dynamo-throttling
In a DynamoDB table, items are stored across many partitions according to each itemâ€™s partition
key. Each partition has a share of the tableâ€™s provisioned RCU (read capacity units) and WCU
(write capacity units). When a request is made, it is routed to the correct partition for its data,
and that partitionâ€™s capacity is used to determine if the request is allowed, or will
be throttled (rejected). Some amount of throttling should be expected and handled by your application.

Excessive throttling is caused by:

Hot partitions: throttles are caused by a few partitions in the table 
that receive more requests than the average partition
Not enough capacity: throttles are caused by the table itself not
having enough capacity to service requests on many partitions

WCU - write capacity unit
1 WCU = 1 write per second of 1KB (1024 bytes).
How to calculate the writes of 6 objects of 4.5KB each per second:
we need 6 * 5 = 30 WCU (4.5 gets rounded to the upper KB)
How to calculate the writes of 120 objects of 2KB each per minute:
we need 120 objects / 60 seconds * 2kb = 4 WCU

There are two types of reads:
- strongly consistent read - if we read just after a write, we'll get the correct data
- eventually consistent read - if we read just after a write, it's possible we'll get unexpected
response because of replication. This is due to a read may happen on a cluster node where
the data hasn't arrived yet.

1 RCU - One strongly consistent read or two eventually consistent reads per second
for an item of 4KB.
In order to determine the number of RCUs for a number of reads per seconds we need to determine the
number of complete chunks
and then multiply the number of chunks with the count of complete reads.
10 strongly consistent reads per seconds of 4KB:
we need 10 * 4KB / 4KB = 10 RCU

16 eventually consistent reads per seconds of 12KB each:
(16 / 2) * ( 12 / 4) = 24 RCU

10 strongly consistent reads per seconds of 6KB each:
10 * 8 KB /4 = 20 RCU

DynamoDB APIs:
PutItem - can be used to add a new item to an Amazon DynamoDB table
Difference between PutItem and UpdateItem - PutItem does a full replace while UpdateItem updates
only specified fields
With UpdateItem we can use atomic counters for numeric values and increase them

local secondary index - alternative sort key for a table, local to the hash key. We can have up to
5 of them in a table. It's local be cause it's local to the partition key, e.g. 

global secondary index - enables you to use a different partition key or primary key in
addition to a different sort key

GSIs can throttle your main table

DynamoDB is an optimistic locking / concurrency database.

You can have up to 10 nodes in the DAX cluster.
DynamoDB DAX is MultiAZ (3 nodes minimum are required for production).

DynamoDB DAX stores the whole objects. If we hesitate between using ElastiCache and DynamoDB DAX
we can use ElastiCache for caching individual fields and DynamoDB DAX for caching whole objects.

DynamoDB streams - equivalent to SQL triggers. It's a changelog of events in a table.
We need to enable streams to use cross-region replication.
After we enable streams records changed prior to enabling them don't get send to a stream.

We can set up a TTL on dynamodb records. The record needs to have a Number type field with a
Unix epoch value which specifies when the record will expire.

DynamoDB CLI parameters:
--projection-expression - retrieve a subset of attributes from a table
--filter-expression - filter results

DynamoDB transactions use twice the amount of WCUs and RCUs. E.g.
If we have a 5KB item size and we want to perform 3 transactional writes per second
[5KB / 1KB per WCU] * 2(cost of transactional) * 3 writes = 30WCU
If we have a 5KB item size and we want to perform 5 transactional writes per second
[5KB / 4KB per RCU] * 2 (cost of transactional) * 5 writes
= 2 (high ceiling of 5KB to 8KB) * 2 * 5
= 20 RCU

DynamoDB can also be used to store the session state. It's scaling outomatically, opposted to
ElastiCache. Usually, questions on the exam referring to auto-scaling cache solution refer to
DynamoDB

Write sharding - in case of a small amount of partitions we can add a suffix to the partition key
so that there are more partitions. This is also called the voting pattern.

Concurrent write - race condition for writing to a table. We can mitigate it in the following ways:
- Conditional writes - specify a condition in your source code that checks if the field is
eligible for update
- Atomic writes
- Batch writes - many items at a time

There are two options to truncate a dynamodb table:
- scan and delete each entry => expensive
- drop the table and recreate => fast and cheap

Amazon DMS (Database migration sevice) - can be used to migrate from OracleSQL, MongoDB, MySQL, S3 to
DynamoDB and Aurora
AWS Schema Conversion tool

Our clients can connect to dynamodb directly using identity providers (Google, SAML, OpenID, Cognito)

BatchGetItem API call allows you to pass multiple Partition Key values in a single request.

=======================================================================================================
Route 53 and DNS

Alias - hostname mapped to an AWS resource

Route 53 supports load balancing through DNS - client load balancing
Route 53 supports health checks.

Route 53 routing policies:
- simple - use when you need to redirect to a single resource. You can't attach health checks to
	simple routing policies. If multiple values are returned a random one is chosen by the client
- failover - this one is based on health checks. We have primary and secondar failover records.
	If the health check for the AWS service behind the primary check starts to fail the DNS records
	starts pointing to the secondary one.
- geolocation - routing based on user location. E.g. traffic from the UK should go to this specific IP
- geoproximity - route traffic to your resources based on the geographic location of users and
	resources. You need traffic flow policies to use this.
	We should also define a "default" policy in case there's no match for a location
- latency - redirect to the server that has the least latency at the moment close to us
- weighted - controls the percentage of each endpoint that is going to be reached. This is helpful
	to route a % of traffic to a new applicatoin versoin. Supports health checks. Helpful to
	split traffic between regions
- multi-value - route traffic to multiple resources. Want to associate a route 53 health
	check with records. Up to 8 healthy records are returned for each Multi Value query. Multi
	value is not a substitute for having an ELB.

You pay $0.50 per month per hosted zone

Route 53 is a global service, it's not region and AZ dependant.

Route 53 TTL - the time for which the browser will cache the DNS response
High TTL - 24 hours - less traffic on DNS but more possible outdated records
Low TTL - 60 seconds - more traffic on DNS but record data on the client side will be more
up to date
TTL is mandatory for each TTL record

CNAME records vs aliases:
A CNAME points a hostname to any other hostname
They work only for non-root domains (something.mydomain.com, not mydomain.com)
Alias records point a hostname to an AWS Resource (app.mydomain.com => lb1-1234.us-east2.elb.amazonaws.com)
Aliases work for both root and subdomains.
Aliases are free of charge and include a health check.

Default Route 53 health checks are 3 in route 53 before a status changes.
Default health check interval is 30 seconds. The other option is 10s - it costs more.
Health checks cost more when they're not non-aws endpoints.

Route 53 traffic flow

=======================================================================================================
-- Encryption / -- AWS KMS / -- KMS / -- CloudHSM / -- AWS CloudHSM

The following components are required in an encryption system:
- a cryptographic algorithm
- data to encrypt
- a method to encrypt data

AWS KMS integrates with:
- AWS CloudTrail
- Amazon DynamoDB
- Amazon EBS
- Amazon EMR
- AWS Nitro Enclaves
- Amazon Redshift
- Amazon RDS
- AWS Secrets Manager
- Amazon SES (Simple Email Service)
- Amazon S3
- Amazon Systems Manager parameter store
- Amazon WorkMail
- WorkSpaces

AWS CloudHSM supports asymmetric encryption capabilities.

data key - a key used to encrypt and decrypt data at rest. Those keys need to be stored somewhere.
Usually this is KMS in the AWS cloud.
CMK - customer master key

There are two types of CMKs according to encryption:
- Symmetric (AES-256 keys - single part key)
- Asymmetric (RSA & ECC key pairs - two part key) - used to encrypt/decrypt or sign/verify
operations

Types of CMKs:
- AWS Managed Service default CMK: free - we cannot delete them
- User Keys created in KMS: 1$ / month (no free tier)
- User keys imported (must be 256-bit symmetric key): 1$ / month

The value of KMS is that the CMK can never be retrieved by the user, and the CMK can be rotated for
extra security

if data > 4KB, use envelope encryption. The main API that helps for this is GenerateDataKey

Encrypted DEK - the encrypted data encryption key we retrieve for envelope encryption. This one
is retrieved with the GenerateDataKeyWithoutPlainText API call. We can encrypt it with a CMK
that we specify.

When you exceed a request quota, you get a ThrottlingException

When using S3 with SSE-KMS we can decrease the calls to KMS with 99%.
We can use an "S3 bucket key" for this to work. We can store the key in the same s3 bucket

We can audit key usage using CloudTrail.

KMI - key management infrastructure
KMI has the following two components:
- management layer - responsible for allowing authorized users to access the stored keys
- storage layer - responsible for storing encryption keys
AWS KMS - key management service
AWS KMS uses AES-256 as its encryption algorithm.
AWS KMS provides a centralized key management dashboard.
AWS KMS provides the simplest solution with little development time to implement encryption on an
Amazon EBS volume.
Some of the services AWS KMS can integrate with are the following:
- Amazon EBS (any snapshot of an encrypted volume needs to be encrypted with the same key).
- Amazon Redshift
- Amazon S3

KMS keys are region-bound.
e.g. if you're moving an EBS volume across regions you need to encrypt it with a new key.

KMS key policies - similar to s3 bucket policies. You cannot control access without them
(main difference).

Keys starting with "aws/*" in the AWS console are AWS-managed.

Custom key stores - this AWS KMS feature allows you to use CloudHSM clusters for storing
encryption keys.

AWS CloudHSM is a cloud-based hardware security module (HSM).
It is a physical applicance.
it must be deployed into an Amazon VPC.
It is a regional service.
AWS CloudHSM supports both symetric and asymetric key encryption.
generate and use your own encryption keys on the AWS Cloud.
The following are methods for AWS to provide KMI:
- you controll the encryption method and KMI
- AWS controls the encryption method and the entire KMI
- you control the encryption method and key management, and AWS provides the storage component of
the KMI

AWS recommends deploying multiple CloudHSM instances across availability zones to achieve
the highest availability and durability of keys in your CloudHSM appliance.
Regions are geographically distinct areas of operation, and synchronization of key material
would become time consuming and costly.

Custom key stores - this feature of Amazon KMS allows you to use a cluster of AWS CloudHSM
instances for the storage of your encryption keys.

SSE-S3 - S3 feature that allows each file deployed to S3 to be encrypted by AWS. This feature is
useful when the developer doesn't want to get involved and encryption and key storing. It automatically
encrypts the files and rotates and stores keys.

On the exam, each time we see secret storing, rotation of secrets, integration with RDS the Amazon
Secrets Manager is highly likely to be referred. Secrets manager is more expensive than SSM
Parameter store but supports secret rotation using lambda functions as callbacks. It has
integration with Document DB, Redshift and RDS. KMS encryption is mandatory for the secrets
manager.

You can encrypt your CloudWatch logs with KMS keys.

Amazon ACM - AWS Certificate manager - it can generate and rotate SSL certificates for free

The AWS Encryption SDK is a client-side library designed to streamline data security operations so
that customers can follow encryption best practices. It supports the management of data keys,
encryption and decryption activities and the storage of encryped data.

AWS Encryption SDK:
- generates, encrypts and decrypts data keys
- uses the data keys to encrypt and decrypt your raw data
- stores the encrypted data keys with the corresponding encrypted data in a single object

=======================================================================================================
-- Elastic Beanstalk / -- AWS Elastic Beanstalk / -- Amazon Elastic Beanstalk

Elastic Beanstalk is based on cloud formation.
The Elastic beanstalk worker environment is useful to create long-running SQS-based logics

RDS databases aren't preserved or replicated when cloning an EB environment. A new RDS instances
with the same configuration is created.
https://aws.amazon.com/premiumsupport/knowledge-center/decouple-rds-from-beanstalk/

Elastic beanstalk can use the following to create a web server environment:
- Auto Scaling Group
- Amazon Cognito User Pool
- AWS SAM Local
- EC2

Elastic Beanstalk cannot perform automatic changes to AWS Identity and Access managemt (IAM)
policies.

Elastic beanstalk can use the following resources to create a web server environment:
- auto scaling group
- amazon cognito user pool
- AWS serverless application model (AWS SAM)
- Amazon Elastic compute cloud
We cannot use AWS Lambda in beanstalk.

All accounts involved are billed for user-accessed AWS resources allocated by AWS Elastic
Beanstalk.

In an immutable update in AWS Elastic Beanstalk, a new Auto Scaling group is created and registered
with the load balancer. Once health checks pass, the already existing Auto Scaling group is terminated.

Immutable updates are helpful when 100% uptime is required for both successful and failed
deployments.

Immutable deployments are useful when 100% of both old and new setups need to be existing
at the same time.
They are the fastest deployments for the lowest price.

After creating an Elastic Beanstalk environment, you cannot change the ELB type but only change its
configuration.
To migrate you need:
1. Create a new environment with the same configuration and different LB type (cloning won't do
in this use case).
2. Deploy your application into the new environment.
3. Swap DNS records

Elastic beanstalk creates a service role to access AWS services and instance role to access
instances.

RDS can be used with Elastic Beanstalk which is useful for dev / beta environment. However, this is
not recommended for prod, as the RDS instance lifecycle is tied to Elastic Beanstalk.
The best option for prod would be creating the RDS instance separately and referencing it from
your Elastic Beanstalk prod environment.
Steps to decouple our RDS from Elastic Beanstalk:
0. Create a snapshot of the RDS DB.
00. Go to the RDS console and protect the RDS database from deletion
1. Create a new Elastic Beanstalk environment without RDS, point your app to existing RDS.
2. Swap DNS records.
3. Terminate old environment
4. Delete CloudFormation stack (in DELETE_FAILED state)

Elastic Beanstalk automatically deletes you Amazon RDS instance when your environment is deleted
and does not automatically retain the data.

We can run our application in Docker with Elastic Beanstalk. For this we need to either:
- provide a Dockerfile or
- provide Dockerrun.aws.json(v1 or v2)- describe where *already built* docker image is. You can define here
the image, ports, volumes, logging etc. V2 supports multi container mode, while v1 only single.
Elastic Beanstalk doesn't use ECS in single docker mode, it just runs docker in EC2. For multi it
leverages ECS.
It can be run in single or multi container mode

Amazon Elastic Beanstalk deploys application code and the architecture to support an environment for
the application to run.
Elastic Beanstalk runs at no additional charge. You incur charges only for the underlying resources.
Elastic Beanstalk creates a service role to access AWS services and an instance role to access
instances.
The following accounts are billed for user-accessed AWS resources allocated by AWS Elastic Beanstalk:
- the account running the services
- the cross-account able to access the shared services
- the cross-account with the Amazon Simple Storage Service bucket holding a downloaded copy of
the code artifact

Elastic beanstalk has 3 components:
- Application
- Application version: each deployment gets a version - we can rollback to previous versions
- Environment name (test, dev, beta, stage etc) - we can deploy an app version to each stage

If you create a database through AWS Beanstalk and delete your beanstalk
application your database will be deleted, as well.

Elastic Beanstalk deployment modes:
- Single instance - great for development
- High availability with Load Balancer - good for prod. All the EC2 instances are in a
separate security group and different AZs

Elastic Beanstalk deployment options for update:
- All at once (deploy all in one go) - fastest, but instances aren't available to serve traffic
for a bit (downtime). Fastest deployment. Good for dev because of quick itterations. No
additonal cost.
- Rolling - update a few instances at a time and proceed to the next ones once the first is
healthy
- Rolling with additional batches - like rolling, but spins up new instances to move the batch
(so that the old application is still available). There are a few more EC2 instances during
deployment
- Immutable - spins up new instances in a new ASG, deploys the new version there and then
swaps the instances when the new version is healthy

Blue / Green deployments - not a "direct feature" of Elastic Beanstalk. We create a separate
environment and deploy the new version there. We can switch to it entirely using Route 53.

Canary deployments can be achieved with traffic splitting.
New application version is deployed to a temporary ASG with the same capacity.

Beanstalk can store at most 1000 application versions. When they reach 1000 a cleanup is needed
for your next deployments. We can use lifecycle policies for automatic clean up. They can be:
- based on time (old versions are removed)
- based on space (when you have too many versions)

.ebextensions/ - config files that can be added at the root of the zip in YAML or JSON
The file must end in .config

An SSL can:
- be loaded from the eb console and load balancer configuration.
- Can be added from the code: .ebextensions/securelistener-alb.config
- can be provisioned through ACM (AWS Certificate manager) or CLI
We must configure the sec group in Beanstalk to allow port 443.

Beanstalk worker environments are good for batch background tasks. We can define
period tasks in them with cron.yaml

We can use the Packer software (create your own AMIs) or Platform.yaml files (base on AWS AMIs)
to define custom AMIs to be used in
Beanstalk custom platforms

Elastic Beanstalk can use Amazon S3 buckets to store the EC2 logs and application files.

We can view the current environemnt's health using the following command:
eb health

Elastic Beanstalk can automatically deploy AWS Auto scaling groups, ELB and Amazon EC2 instances.

=======================================================================================================
Deployment as Code

AWS services for CI/CD:
AWS CodeCommit - hosted git
AWS CodeBuild
AWS CodeDeploy
AWS CodePipeline

https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials.html

Code -> Build -> Test -> Deploy -> Provision
CodeCommit -> CodeBuild (to Build and Test) -> AWS Elastic Beanstalk /
											   AWS CodeDeploy / User managed EC2 Fleet (Deploy and
											   Provision)



-- AWS CodeDeploy / -- CodeDeploy

We can configure how much revisions can CodeDeploy save back

If an AWS CodeDeploy configuration contains a file which already exists on the server,
the deployment will fail by default.
By default, CodeDeploy will not remove files it does not manage. This is maintained as a list of
files on the instance.

The CodeDeploy agent sends progress reports to the CodeDeploy service. The service does not attempt
to query instances directly, and the Amazon EC2 API does not interact with instances at the
operating system level.

If two deployment groups reference the same Auto Scaling Group, a failure of the first
group's deployment can block the second until the deployment times out. Since the
instance that failed deployment has been terminated from the Auto Scaling Group,
the AWS CodeDeploy agent is unable to provide results to the service.

If an AWS CodeDeploy deployment fails the changes are left as-is, unless they're reverted
explicitly.

appspec.yml - config file used by the CodeDeploy to be sent to the machines running the CodeDeploy
agent
CodeDeploy managed machines are grouped (dev / test / prod for example)
CodeDeploy supports AWS Lambda deployments.
It can use Blue / Green deployments when running on EC2 instances (not on-prem).
CodeDeploy only deploys your application. It doesn't provision the instances.

If you specify a hook script in the ApplicationStop lifecycle event of an AWS CodeDeploy appspec.yml
it will not run on the first deployment to an instance.

https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html
CodeDeploy deployment configurations for EC2/on-premise compute platforms:
CodeDeployDefault.AllAtOnce
CodeDeployDefault.OneAtATime
CodeDeployDefault.HalfAtATime

CodeDeploy deployment configurations for ECS compute platforms:
- Canary
- Linear
- AllAtOnce

CodeDeploy deployment configurations for EC2 blue/green deployments:
- Canary
- Linear
- AllAtOnce

CodeDeploy deployment configurations for AWS Lambda deployments:
- Canary
- Linear
- AllAtOnce

Linear deployments are useful in scenarios in cases where we like to make sure that
resources are updated on batches after equal intervals. E.g. if we have a number of
AWS Lambda functions and we want to have two functions updated each three minutes.

If we upgrade the script executed during an ApplicationStop lifecycle event it'll be executed
at the next deployment after it is updated.

-- AWS CodePipeline / -- Amazon CodePipeline / -- CodePipeline

A main difference between CodePipeline and CodeDeploy is that CodePipeline deploys to Beanstalk
and CodeDeploy deploys to an EC2 fleet or on-premise machines that run the CodeDeploy
agent

When a CodePipeline revision manual approval task gets rejected the revision
is treated as failed.

The minimum number of stages required by a pipeline in AWS CodePipeline is 2.
CodePipeline stages can have multiple action groups.

CodePipeline concepts (to print):
https://docs.aws.amazon.com/codepipeline/latest/userguide/concepts.html

The minimum number of stages required by a pipeline in AWS CodePipeline is 2.
In the Invoke pipeline action we can execute Lambda functions.

When you configure a deploy event in CodePipeline to deploy to OpsWorks Stacks, you must select the
stack, the application to deploy, and the layers into which to deploy the application.
You cannot modify OpsWorks cookbooks and run lists in OpsWorks with CodePipeline.

an action is part of the sequence in a stage of a pipeline.
It is a task performed on the artifact in that stage.
Pipeline actions occur in a specified order, in sequence or in parallel,
as determined in the configuration of the stage.

AWS CodePipeline actions:
- Source - we can use CodeCommit, S3, ECR and CodeStarSourceConnection as sources
- Approval - If the action is approved, the pipeline execution resumes. 
	If the action is rejectedâ€”or if no one approves or rejects the action within seven days of the 
	pipeline reaching the action and stoppingâ€”the result is the same as an action failing,
	and the pipeline execution does not continue.
	The approval action integrates with an SNS approval topic. From then on, SNS can send the
	approval request to an HTTP/HTTPS endpoint, an AWS Lambda function, SQS queues,
	email or SMS recipients
- Build - integrates with CodeBuild, CloudBees CI/CD, Jenkins, TeamCity
- Deploy - integrates with S3, AWS AppConfig, AWS CloudFormation,
	AWS CloudFormation StackSets, Amazon ECS, Elastic Beanstalk, AWS OpsWorks,
	AWS Service Catalog, Amazon Alexa, CodeDeploy deploy actions,
	XebiaLabs deploy actions
- Test - integrates with CodeBuild, AWS Device Farm, BlazeMeter, Ghost Inspector,
	Micro Focus StormRunner Load test, Nouvola, Runscope
- Invoke - we can invoke AWS Lambda functions with this action. It also
	integrates with Snyk and Step Functions
https://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html

When multiple resources are configured for the same pipeline, the pipeline
will be triggered when any source is updated.

-- AWS CodeBuild / -- CodeBuild

buildspec.yml - defines how CodeBuild compiles a project
SSM Parameter store - can be used in buildspec.yml to add secrets to the build

If we need to use a large file that changes rarely in CodeBuild we can create
a custom build container that includes it and use it any time we need to change the files

AWS S3 cache bucket (optional) - can be used in CodeBuild for faster builds

By default, any code build containers are launched outside of your VPC and this is why they
cannot access resources in your VPC. We can add access to it by specifying the following:
- VPCID
- Subnet IDs
- Security group IDs

We can output artifact files from AWS CodeBuild to AWS CodePipeline by specifying
the output files in the buildspec.yml config file.

CodeBuild runs outside of a VPC by default. We can specify a VPC configuration for
CodeBuild.
We can use SSM Parameter store with CodeBuild

In case we need to work with large binary files that change rarely the best way to work with them
in builds is to create a custom build container and include them there.

Environment variables in CodeBuild projects are not encrypted and are visible using the CodeBuild
API. Thus, if you need to pass sensitive information to build containers, use Systems
Manager Parameter Store.

-- AWS CodeCommit / -- CodeCommit

We can send notifications to SNS in CodeCommit on the following events:
- Comments on commits
- Comments on pull requests
- PR created
- PR Updated
- PR merged
- PR status changed
- Branch created
- Branch deleted
- Branch updated

CodeCommit repositories are automatically encrypted at rest using KMS. IAM user roles and policies
are used for authenticaiton and authorization.
It uses SSH or HTTPS for encryption in transit.
We can also use Amazon STS to grant temporary access to external users.

You can connect to AWS CodeCommit without git credentials using AWS CodeCommit credential
helper. You can use it to convert an IAM access key and secret access key to valid Git
credentials for SSH and HTTPS authentication.

You can trigger notifications in CodeCommit using:
- AWS SNS - deletion of branches, pushes on master, notify external build system
- AWS Lambda - for code analysis
- AWS CloudWatch Event Rules - trigger for PR updates (created, updated, deleted, commented)

You can connect to an AWS CodeCommit repository without git credentials using the credentials helper

Storing large binary objects in a git repository can incur massive storage requirements.
Any time a binary object is modified in a repository, a new copy is saved. Comparing this to
storing the binary file in an S3 bucket, the CodeCommit approach is more expensive.

CodeCommit repositories are by default encrypted at rest.

=======================================================================================================
Infrastructure as Code

-- AWS CloudFormation / -- CloudFormation

The benefits of using AWS CloudFormation over AWS CLI / AWS SDK are the following:
- Reduced IAM permissions requirements
- Versionable infrastructure
- Repeatable Infrastructure
- Reduction of human error

The reason for a cloudformation stack to reach the CreationPolicy's timeout value may be the following:
- the user data script fails before reaching the cfn-signal step
- the user data script does not include a call to cfn-signal
- the instance cannot connect to the AWS CloudFormation endpoint when calling cfn-signal

The CreationPolicy allows us to specify a required number of signals to mark a resource as
CREATE_COMPLETE.

We can use CloudTrail to track which AWS resource a CloudFormation stack updated. We need to search
CloudTrail for events where the ClientRequestToken corresponds to the token for this stack action.

Migration of database instances between stacks is a common workflow. To migrate an Amazon RDS
layer, you must remove it from the first layer before you add it to the second.
Here's the flow to execute such a migration:
- supply the connection information to the second stack as custom JSON to ensure that the instances
can connect
- remove the Amazon RDS layer from the first stack
- add the Amazon RDS layer to the second stack
- Remove the connection custom JSON

To prevent any potential errors, you need to declare explicitly that two or more resources
depend on each other. Otherwise, if we use arbitrary strings AWS CloudFormation might not
recognize this as dependency between resources and might not guarantee that they'll be deployed
in the right order.

AWS CloudFormation supports templates of 51200 bytes in size when passing the template body as
part of the CreateStack request. If we need to pass a larger template we can upload it to Amazon S3 and
reference its location in the CreateStack request.

If you have a template that contains more than the limit of 200 resources, we can work around
this limit if needed by creating nested stacks.

You can access a property of a resource created in a nested stack by defining it as stack
output. Then, in the parent stack use Fn::GetAtt and pass in two parameters, the child stack
logical ID and Outputs.NestedStackOutputName.
If we export the resource in the child stack and then in the parent import the exported value
this won't work. It'll create a circular reference.

Only the Resources section of a CloudFormation template is required.

We cannot create a dynamic amount of resources. Everything in CloudFormation has to be declared
and you cannot perform dynamic code generation.

Is every AWS Service supported?
Almost. Only a select few niches are not there yet
You can work around that using AWS Lambda Custom Resources

Check the "Parameters Settings" slide to see how CloudFormation parameters can be controlled

pseudo parameters

CloudFormatiom mappings - syntax construct that allows you to use hash map-like values

CloudFormation outputs - syntax construct that allows us to export our definitions and reference
them elsewhere
You can delete a referenced stack until all the references are removed.

CloudFormation conditions

CloudFormation OS helper scripts:
- cfn-init - reads template metadata from the AWS::CloudFormation::Init key and acts accordingly to:
	- Fetch and parse metadata from CloudFormation
	- Install packages
	- Write files to disk
	- Enable/disable and start/stop services
- cfn-signal -  signals CloudFormation to indicate whether
Amazon EC2 instances have been successfully created or updated
- cfn-get-metadata - fetch a metadata block from CloudFormation and print it to standard out
- cfn-hup - a daemon script that detects changes in resource metadata and runs
user-specified actions when a change is detected. This allows you to make
configuration updates on your running Amazon EC2 instances through the UpdateStack API action.

--retain-resources - CLI argument to retain resources you don't want to delete when using the
"aws cloudformation delete-stack" command

Must know Intrisic Functions:
- Ref - get the value of a parameter in an AWS CloudFormation template
- Fn::GetAtt - get the attributes of a resource in a template
- Fn::Split - used to split a string into multiple substrings based on a
common delimiter
- Fn::FindInMap
- Fn::ImportValue
- Fn::Join
- Fn::Sub
- Conditionals - If, Not, Equals...

The return value of the Ref intrisic function for an AWS::ElasticLoadBalancing::LoadBalancer
resource is the load balancer name, which is not valid in a URL.

Stack set - create, update or delete stacks across multiple accounts and regions with a
single operation
Administrator accounts can create stack sets.
When you update a stack set, all associated stack sets across regions get updated.

UPDATE_COMPLETE_CLEANUP_IN_PROGRESS - this status means CloudFormation has successfully updated and
removing old resources.

AWS::CloudFormation::Interface - this metadata option does not apply to stacks using the AWS CLI
or AWS SDK. In it parameters can be organized into groups and labels could be changed.

AWS::CloudFormation::CustomResource - Custom resources provide a way for you to write custom
provisioning logic in CloudFormation template and have CloudFormation run it during a stack operation,
such as when you create, update or delete a stack.
A service token in a custom resource represents an AWS SNS or AWS Lambda ARN that receives the request

The UPDATE_IN_PROGRESS state could be followed by the following three states:
- UPDATE_FAILED
- UPDATE_COMPLETE
- UPDATE_COMPLETE_CLEANUP_IN_PROGRESS

By default, the AWS CloudFormation stack will perform the operations with the permissions of the user
performing the operation.

Update behaviors of stack resources:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html
Update with No Interruption
	AWS CloudFormation updates the resource without disrupting operation of that resource and without 
	changing the resource's physical ID. For example, if you update certain properties on an
	AWS::CloudTrail::Trail resource, AWS CloudFormation updates the trail without disruption.

Updates with Some Interruption
	AWS CloudFormation updates the resource with some interruption. For example, if you update certain
	properties on an AWS::EC2::Instance resource, the instance might have some interruption while
	AWS CloudFormation and Amazon EC2 reconfigure the instance.

Replacement
	AWS CloudFormation recreates the resource during an update, which also generates a new physical ID.
	AWS CloudFormation usually creates the replacement resource first, changes references from other
	dependent resources to point to the replacement resource, and then deletes the old resource.
	For example, if you update the AvailabilityZone property of an AWS::EC2::Instance resource type,
	AWS CloudFormation creates a new resource and replaces the current EC2 Instance
	resource with the new one.
	The following occur during a replacing update:
	- A new resource is created
	- The original resource is deleted during the cleanup phase
	- the resource becomes unavailable
	- The resource physical ID changes

Before a stack that exports an output can be deleted any stacks importing the exported value must
remove the import.

=======================================================================================================
Configuration as Code

-- AWS OpsWorks / -- OpsWorks / -- Amazon OpsWorks

AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.

An AWS OpsWorks stacks configure lifecycle event runs when a single instance comes online
or goes offline.

AWS OpsWorks Stacks lets you manage applications and servers on AWS and on-premises.
With OpsWorks Stacks, you can model your application as a stack containing different layers,
such as load balancing, database, and application server.

AWS OpsWorks Stacks does not include a central Chef Server.
For OpsWocks stacks a custom cookbook repository location is configured for a stack. When instances
in the stack are first launched, they will download cookbooks from this location and run them as part
of lifecycle events.

AWS OpsWorks Stacks is an application and server management service. With OpsWorks Stacks, you can
model your application as a stack containing different layers, such as load balancing,
database, and application server.

Amazon EC2 is not a valid storage location for regular recipe code updates.

In AWS OpsWorks Stacks a custom cookbook repository location is configured for a stack. When
instances in the stack are first launched, they will download cookbooks from this location and run
them as part of lifecycle events.

AWS OpsWorks Stacks instance types:
- 24/7 - you need to start them and stop them manually; time-based and load-based
instances get started automatically
- On demand
- Load-based - start additional instances when traffic is high and
stop instances when traffic is low
- Time-based - start and stop on specific time
https://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autoscaling.html
Both time-based and load-based instances are supported for Linux stacks, while
only time-based instances are supported for Windows stacks.

We can configure the following OpsWorks limits:
- maximum layers per stack
- maximum instances per layer
- maximum apps per stack
- maximum apps per account, per region

OpsWorks stacks support the following:
- RDS
- EBS
- Elastic IPs

AWS OpsWorks doesn't have a concept of cookbook caching.
OpsWorks lifecycle events don't allow you to specify cookbook versions.
After modifying cookbooks you must first run the Update Custom Cookbooks command.

A single OpsWorks task definition can describe up to 10 containers to launch at a time.
Task definitions should group containers by similar purpose, lifecycle, or resource requirements.

OpsWorks stacks instances are registered when they come online and deregistered when they are
moved to a different state.

Instances in a single AWS OpsWorks Stacks layer should have the same functionality and purpose
because all instances in a layer run the same recipes.

When you configure a deploy event in CodePipeline to deploy to OpsWorks Stacks, you must select the
stack the application to deploy, and the layers into which to deploy the application.

After updating a custom cookbook repository, any currently online instances will not automatically
receive the updated cookbooks. To upload the modified cookbooks to the instances, you must first
run the Update Custom Cookbooks command.

-- AWS Config

The following steps are a part of setting up AWS Config:
- specifying the resource types that you want AWS Config to record
- Granting AWS Config the permissions it needs to access the Amazon S3 bucket and the Amazon
Simple Notification Service (SNS) topic
- setting up an Amazon S3 bucket to receive a configuration snapshot on request
and configuration history

=======================================================================================================
Authentication and Authorization

The AWS SDK relies on access keys, not passwords. The best practice is to use AWS Identity and Access
Management (IAM) credentials and not the AWS account credentials. Comparing IAM users or IAM roles,
only IAM users can have long-term security credentials.

We can use identity federation and IAM roles to establish a trust relationship between an
external Active Directory and AWS.

MFa minimizes the impact of lost or compromised credentials.

You cannot connect to multiple AD domains with AD connector, only a single one.

When you use identity federation to assume a role, The AWS security token service (AWS STS)
generates the access key ID, secret access key and session token.

The IAM trust policy defines the principals who can request role credentials from the AWS STS.
Access policies define what API actions can be performed with the credentials from the role.

The long-term credentials are not limited to a single AWS Region. IAM is a global service, and IAM
user credentials are valid across different AWS Regions.

IAM policies are global in scope, so you do not need a custom one per AWS Region.

You can add IAM users to IAM groups but not IAM roles. Instead, roles must be assumed for short-term
sessions.

The purpose of an idP is to answer the question "Who are you?". Based on
this answer policies are assigned.

DynamoDBReadOnlyAccess policy - built-in policy that applies to the resource * wildcard, which means
that it applies to any and all DynamoDB tables accessible from the account regardless of when those
tables were created.

AWS Security Token Service (AWS STS) supports a number of different tokens:
- AssumeRole - short lived; 60 minutes by default, can be extended to 720 minutes
- GetSessionToken - this is the call being used with MFA auth
- GetFederationToken - returns a set of temporary security credentials (consisting of an access key
ID, a secret access key and a security token) for a federated user. You call the GetFederationToken
action using the long-term security credentials of an IAM user. This is appropriate in contexts
where those creds can be safely stored, usually in a server-side app.
https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html

AWS AD Connector - AD Connector is a directory gateway with which you can redirect directory requests
to your on-premises Microsoft Active Directory without caching any information in the cloud.
For example, you can integrate your RADIUS-based MFA infrastructure that you already have on-premise.
It gives you the ability to configure changes to Active Directory on your existing Acitve
Directory console.

Using AWS as an identity provider (IdP) to access non-AWS resources allows you to use AWS
CloudTrail to audit who is using the service.

To use AWS Single Sign-On (AWS SSO), you must set up AWS Organizations Service and enable
all the features.
AWS SSO uses MS AD (either AWS Managed Microsoft Acitve Directory or Active Directory
Connector [AD Connector] but not Simple Active Directory).
AWS SSO does not support Amazon Cognito.
AWS SSO does not use SAML.

-- Amazon STS / -- AWS STS / -- STS

These are the Amazon STS actions:
- AssumeRole - 
- AssumeRoleWithSAML - this action provides access key ID, secret access key and
	a security tokens
- AssumeRoleWithWebIdentity
- DecodeAuthorizationMessage
- GetAccessKeyInfo
- GetCallerIdentity
- GetFederationToken
- GetSessionToken

https://docs.aws.amazon.com/STS/latest/APIReference/API_Operations.html

to use SAML idP with Amazon STS you need a SAML 2.0 based provider, 1.0 will not work.

-- Cognito

Amazon Cognito supports device remembering and tracking
Amazon Cognito supports MFA authentication.
Amaozn Cognito supports also SMS-based MFA authentication.
Amazon Cognito supports Google SSO and MS AD and custom identity providers.You need SAML 2.0 based
identity provider to integrate with Amazon Cognito.

Cognito Identity pools could be used to provide single sign on capabilities for
your application.

SAML - security assertion markup language

The following are valid Cognito identity providers (idP) for Amazon Cognito:
- Google
- MS AD
- your own identity store

you can view the cognito API calls in Amazon CloudTrail.
AWS can act as an idP for non-AWS services.
Using AWS as an idP allows you to use AWS CloudTrail to audit who is using the service.

Amazon CloudWatch logs are generated if you are using Amazon Cognito to control access to AWS
resources.

OpenID token types:
- Refresh token - allows you to require a new access token when the previous one expires
- Access token - controls which APIs can be executed
- ID token - establishes identity

=======================================================================================================
Integration

-- Amazon SQS / -- SQS

Amazon SNS supports the same attributes and parameters as Amazon SQS.

The number of in-flight messages is the number of messages that have been received by a consumer and
not yet deleted. These messages are invisible until the visibility timeout interval passes. During
this interval, they are in flight.

One of the widest usages of SQS is for decoupling applications.

Occasionally, messages can be delivered twice.

The max size of a message is 256 kb.
They can be send through the SendMessage API.
A message is persisted in SQS until a consumer deletes it (DeleteMessage API).

A consumer can receive up to 10 messages at a time.

After a message is polled by the consumer, it becomes invisible to other consumers.
This is when the visibility timeout begins. It's 30 seconds by default. During those 30 seconds the
message has to be processed. During that period of another consumer makes an attempt to retrieve the
message it'll not be returned.
After the visibility timeout passes and the message has not been deleted, it's put back into the
queue and another consumer can do a ReceiveMessage API call.

A consumer can make a ChangeMessageVisibility API call if it needs more than 30 seconds to
process the message.

If visibility timeout is too low we may get duplicates.

We can set up an ASG to scale a target group instance cound depending on an SQS queue length
(e.g. if those 10 queues at a time aren't sufficient) using a CloudWatch Alarm.

A message will stay in a queue for 4 days by default. This retention period can extend to 14.
It can range from 1 minute to 14 days.

To send a message larger than 256KB, you use Amazon SQS to save the file in Amazon S3 and then
send a link to the file on Amazon SQS. In this way you can send files larger than 256KB as S3 links
through SQS.

Amazon SQS queues do not support subscriptions.

Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages
that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your
application or messaging system because they let you isolate problematic messages to determine
why their processing doesn't succeed.

Messages get moved to the dead-letter queue if they have met the Maximum Receives parameter
(the number of times that a message can be received before being sent to a dead-letter
queue) and have not been deleted.

If a queue has no consumers and the maximum retention period passes, the messages in it don't
get moved to the dead-letter queue. They get deleted.

If a message doesn't get processed and goes back to the queue a specific number of times without
being processed it gets moved to a dead-letter queue. We can set this by changing the
MaximumReceives threshold.

The default amount of times a message goes back for reprocessing is 10. The limit is from 1
to 1000. You need to setup a separate queue and specify it as dead-letter queue.

A queue without consumers deletes all the messages in it after the retention period has passed.

SQS queue access policies:
- cross account access
- publish S3 Event notifications to SQS queue

We can delay a message (consumers don't see it immediately) up to 15 minutes. Default is
0 seconds. We can set a default at queue level. The producer can override the default
when sending the message with the DelaySeconds parameter.

Long polling - a consumer can optionally wait for messages to arrive if there are none in the queue.
This is useful when we want to make less SQS requests.
Long polling can take from 1 to 20 seconds.
We can set it at the queue level or specify it with ReceiveMessageWaitTimeSeconds parameter.

SQS extended client - we can use this Java library to handle messages larger than 256kb.
The larger message actually ends up in S3 and the SQS message contains only metadata
about the S3 object.

popular API calls on the exam:
CreateQueue(MessageRetentionPeriod), DeleteQueue
PurgeQueue - delete all messages in the queue
SendMessage(DelaySeconds), ReceiveMessage, DeleteMessage
MaxNumberOfMessages( default 1, max 10 for ReceiveMessage API)
Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility helps decrease your
costs

FIFO is useful when we want the consumer to receive the messages in the exact
same order.
When we create a FIFO queue it's name needs to end in ".fifo"

you can have as many consumers as GroupID for your FIFO queues

de-duplication methods for FIFO queues:
- explicitly provide a message deduplication ID
- Content - based: make a SHA-256 hash of the message body

We can specify the MessageGroupID in FIFO queues to group messages together and
all of them will have only 1 consumer.

The producer should provide message deduplication ID values for each message in the following scenarios:
- Messages sent with identical message bodies that Amazon SQS must treat as unique.
- Messages sent with identical content but different message attributes that Amazon SQS must treat as unique.
- Messages sent with different content (for example, retry counts included in the message body) that Amazon SQS must treat as duplicates.

ReceiveMessageWaitTimeSeconds - length of time, in seconds, for which a ReceiveMessage action waits for a
message to arive.
DelaySeconds - the length of time, in seconds, for which to delay a specific message.
VisibilityTimeout - the duration, in seconds, that the received messages are hidden from
subsequent retrieve requests after being retrieved by a ReceiveMessage request
MessageRetentionPeriod - the period for which SQS retains a message

-- Amazon SNS / -- SNS

Amazon SNS supports the same attributes and parameters as Amazon SQS.
The "event producer" only sends message to one SNS topic.
You may have as many event receivers as you wish.

Subscribers can be:
- SQS
- HTTP/HTTPS
- email/email as JSON
- Lambda
- SMS
- Mobile notifications
- Amazon Kinesis Data Firehose

Many different services can send data to SNS for notifications:
- CloudWatch alarms
- ASG
- Amazon S3
- CloudFormation

SNS + SQS Fan out pattern:
Push once in SNS, receive in all SQS queues that are subscribers. This is a fully decoupled
model with no data loss. SQS here allows for data persistence, delayed processing and
retries of work.

If you want to send the same SNS notification to multiple SQS queues use fan-out pattern.

SNS also supports FIFO - FIFO topic. It can only have SQS queues as subscribers. To create
a FIFO topic its name needs to end in ".fifo"

Message filtering - JSON policy to filter messages sent to SNS topics.

You want to design an application that sends a status email every morning to the system
administrators. For this purpose you can create an SNS topic, subscribe all the
administrators to this topic, set up an Amazon CloudWatch event to send a message on a
daily cron schedule to this topic.

-- Amazon Kinesis

Amazon Kinesis Data Streams - capture, process and store big data streams. Data in
Data streams is made up of multiple shards. Billing is per shard provisioned, you can have as
many as you want.

Meant for real-time big data, analytics and ETL

Each Kinesis data stream shard supports a read throughput of 2MB per seconds.
This is how we can calculate the number of shards per throughput and consumers:
Supose the throughput into the stream is 10MB and we have three separate applications
consuming them.
You have three applications, so you need to read at least 30MB per second of read
throughput to read this data three times, which will require atleast 15 shards.

Each Kinesis data stream shard supports a write throughput of 1MB per second.
E.g. if you expect about 5MB of message you'll have to provision at least 5 shards.
It would be also a good idea to provision 6 shards for some margin for fluctuation.

Producers - they send data to Kinesis data streams
Data retention is set to 1 day by default. It can be extended up to 365 days.
Amazon Kinesis Data Streams is not a durable data store. It's purpose is just to stream
data
Once data is inserted in Kinesis, it can't be deleted (immutability)
Data that shares the same partition goes to the same shard (ordering)

Amazon Kinesis Data Streams is a service for ingesting large amounts of data in real time and
for performing real-time analytics on the data.

Messages written to Kinesis Data Streams are deleted after the message retention period
passes. The maximum message retention period is 7 days.

There's no limit for the number of consumers in Kinesis data stream, as long as
they stay within the capacity of the stream, which is based on the number of shards.

For a single shard, the capacity is 2MB of read or five transactions per second.

A data record consists of:
- sequence number (unique per partition-key within shard)
- partition key (must specify while put records into stream)
- data blob (up to 1MB)
PutRecord API

Service limits:
https://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html

"hot partition" - highly used partition

provisionedThroughputExceeded
Solution:
- use highly distributed partition key
- retries with exponential backoff
- increase shards (scaling)
- use AWS SDK (it automatically retries failed requests using jitter based backoff algorithm)

Consumer patterns:
- Shared (Classic) Fan-out consumer (GetRecords) - pull model. Good for low number
of consumers - max. 5 GetRecords API calls/second
- Enhanced Fan-out consumer (SubscribeToShard)

Some of the Kinesis producer options
- Amazon Kinesis agent - a stand-alone Java agent The agent continuously monitors a set of files
and sends new data to your Kinesis Data Firehose delivery stream.
- Amazon Kinesis Producer Library (KPL) -
	https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html
- Amazon Kinesis Data Streams API
- open source tools built with Amazon Kinesis Data Streams API

Kinesis consumer options:
- AWS SDK
- Kinesis client library - each shard can be read by only one KCL instance, e.g.
	4 shards = max. 4 KCL instances
	6 shards = max. 6 KCL instances
- Lambda, Kinesis Data Firehose, Kinesis Data Analytics

Each Kinesis shard supports a write throughput of 1MB per second.

In Kinesis there is no limit in the number of consumers as long as they stay within the capacity
of the stream, which is based on the number of shards. For a single shard, the capacity is 2MB of
read or five transactions per second.

Shard splitting - used to divide a "hot shard". This is a pattern we can use when a shard usage gets
increased. There's no "auto scalling", we need to do this manually as developers. We can't split into
more than 2 shards in a single operation

Merging shards - opposite operation of shard splitting. Decreases capacity and saves cost. Can be used
to group 2 shards with low traffic

Amazon Kinesis Data Firehose - load data streams into AWS data stores. It can use a lambda function
to transform the data. CloudWatch, AWS IoT and Kinesis data streams can be used as sources.
Possible destinations are:
- Amazon S3
- Amazon Redshift (Copy through s3)
- Amazon Elastic Search
- 3rd party providers - Splunk, New Relic, DataDog, mongodb
- custom HTTP endpoint

you only pay for the data coming through firehose
It supports automatic scaling
It's near real time (buffer time is atleast 60s)

Data can be stored from 1 to 365 days.

Amazon Kinesis Data Analytics - analyze data streams with SQL or Apache Flink. After we process the
data it can go to a data sink. It can be a new Kinesis Data Stream or go to Kinesis Data Firehose
Use cases:
- time-series analytics
- real-time dashboards
- real-time metrics

Amazon Kinesis Video Streams - capture, store and process video streams (not on the exam)

=======================================================================================================
Serverless Compute

Using a serverless approach means not having to manage servers and not incuring compute costs when
there is no user traffic. This is achieved while still offering instant scale to meet high demand, such
as a flash sale on an ecommerce site or a social media mention that drives a sudden wave of traffic.

-- AWS Lambda / -- Lambda / -- Amazon Lambda

In AWS Lambda you pay per request and compute time.

Services that Lambda reads events from
- Amazon DynamoDB
- Amazon Kinesis
- Amazon MQ
- Amazon Managed Streaming for Apache Kafka
- self-managed Apache Kafka
- Amazon Simple Queue Service

The total size of environment variables shouldn't exceed 4KB.

We can create a separate environment in Lambda with new versions of the functions using
aliases. E.g. we can switch between those environments by moving a PROD alias between them.

Lambda automatically retries failed executions for asynchronous invocations. You can also configure 
Lambda to forward payloads that were not processed to a dead-letter queue, which can be an
Amazon SQS queue or Amazon SNS topic.

There are two types of policies with Lambda: a function policy and an execution policy, or
AWS role. A function policy defines which AWS resources are alowed to invoke your function. The
execution role defines which AWS resources your function can access.

The default timeout value for an AWS Lambda function is 3 seconds.
The default limit for concurrent executions with Lambda is set to 1000. This is a soft limit that
can be raised. To do this, you must open a case through the AWS support center page and send a Server
Limit increase request.
The maximum execution time for a Lambda function is 300 seconds (5 minutes).
The minimum execution time for a Lambda function is 1 second.

The AWS Lambda concurrent executions limit is 1000.
To raise this number one needs to contact the AWS support.

You can use AWS SAM CLI to test and develop Lambda functions locally.

Developing lambda functions locally:
https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-using-debugging.html

AWS Lambda natively supports Java, Go, PowerShell, Node. js, C#, Python, and Ruby code, and
provides a Runtime API which allows you to use any additional programming languages to author your
functions.

AWS Lambda  bills based on memory, the amount of compute time spent on a function in 100ms
increments, and the number of times that you execute or trigger a function.

There are two types of policies with Lambda:
- function policy - defines which AWS resources are allowed to invoke your function
- execution policy - defines which AWS resources your function can access.

We can send event payloads to an Amazon SQS queue or Amazon SNS topic if the lambda function fails
by enabling the dead-letter queue configuration option for the lambda.

The maximum amount of memory you can assign to an AWS Lambda function is 3008MB.
We can scale from 128MB to 3008 with a 64MB increment.
The more ram you add the more vCPU u get.
At 1792MB the funciton has the equivalent of one full vCPU.
After 1792MB you have more than 1 vCPU and need to use multi-threading to get the
benefit from this.

The default timeout for a lambda is 3 seconds. The maximum is 900 seconds (15 minutes).
We can have up to 4KB of environment variables.

Execution context - temp runtime environment that initializes external dependencies

It is best practice to initialize a db connection outside of the handler and before it.
Anything that takes time to initialize should be put out the function handler.

You can use the /tmp directory to write and persist files in lambda functions.
Max size is 512mb. This can be used transiently inbetween multiple invocations.
If you need persistent storage instead, use S3.

We can also run Lambda functions in containers. The docker container must implement the
Lambda Runtime API.

Lambda Runtime Interface Emulator - can be used locally to test docker images prepared for
AWS Lambda

Main lambda integrations mentioned on the exam:
- S3
- DynamoDb
- Kinesis
- API Gateway
- CloudFront
- CloudWatch Events / EventBridge
- SNS
- SQS
- Cognito

Lambdas can run as Serverless cron jobs. We can create a CloudWatch Event / EventBridge rule
that will be triggered at a certain time and spawn an AWS Lambda.

Invocations
- Synchronous: CLI, SDK, API Gateway, Application Load balancer - direct invocation
for which you wait the result
Any error must be handled client-side
- Asynchronous invocations - events are placed in an event queue. Lambda attempts to retry
the invocations on error. We need to make sure our lambda code is idempotent in case of retries
(the result is the same in case of retry). In case of retry we may see duplicate cloud watch logs.
We can define a dead-letter queue in SNS or SQS for failed processing.
- Event Source mapping - applies for Kinesis, SQS and SQS FIFO queue and DynamoDB streams. In this
case records need to be polled from the source. Lambda needs to ask some of these services. The
lambda function gets invoked synchronously and iterates the streams.

Lambda@Edge - another synchronous usage of Lambda. It uses S3 and cloudfront.
A Lambda function is deployed not in a region but at an edge location.
We can use Lambda@Edge to process CloudFront requests and responses.

Async invocations:
- SNS
- SQS
- CloudWatch Events / EventBridge
(not on the exam)
- CodeCommit, CodePipeline, SES, CloudFormation, CloudWatch logs, Config, IoT, IoT Events

Lambda destination - define receiver for an asynchronous invocation. We can send to
- SQS
- SNS
- Lambda
- EventBridge bus
Destinations are newer than Lambda DLQs and more recommended. They allow more targets.

When you use an event source mapping to invoke your function, Lambda uses the execution role
to read event data

By default, lambda functions are lanuched in another VPC. Therefore, by default it
cannot access resources in your VPC. You can allow access to a specific VPC and subnet but
after you do so, it won't by default have access to the internet.
Deploying a lambda function to a public subnet does not give it access to the Internet.
To solve this, you need to deploy it to a private subnet with NAT Gateway / Instnace.
You can also use VPC endpoints to privately access AWS services without a NAT.

In case you have back-end web service EC2 instances that you want to block inbout
public traffic from you can do the following:
- launch the instances in a private subnet and rely on a NAT gateway in a public subnet
to forward outbound internet requests
- modify the inbound security group rules for the instance to allow only inbound requests
from your webservers

Lambda supports up to 1000 concurrent exectuions. This limit amounts for all the lambda
functions in your account.
Each invocation over the concurrency limit will
cause a throttle.
ThrottleError  - 429 - returned in case of synchronous execution
It'll retry automatically and then go to DLQ.

Cold start - when you create a new instance code is loaded and code outside the handler needs
to run. This happens at the first request to your instance and this request may take longer.
To solve this we can use provisioned concurrency - it allocates lambda instances before
they're invocked.

you can reference dependencies - up to 50mb can be zipped and uploaded to lambda.
For more - you need s3.

You cannot add dependencies when adding inline lambda code in CloudFormation
You can also use S3 and CloudFormation for bigger lambda functions that include dependencies

Lambda layers - allow the usage of third-party languages and to externalize dependencies to
reuse them

We've discovered that the documented limit of 50 MB seems to be true when uploading a Lambda
function's package directly.  when using the S3 method to upload a Lambda function's code, we can upload
up to 250 MB. The limit in that case is the 250 MB uncompressed code/dependency size limit.

We can create an AWS lambda function with the following command:
aws lambda create-function

-- Amazon Step Functions / -- Step Functions

Amazon Step functions states:
- Pass
- Task
- Choice
- Wait
- Succeed
- Fail
- Parallel
- Map

Pipeline actions can be ordered to be completed in stage and in parallel.

All the error handling in step function flows should happen outside the lambda functions used in the
flow - in the step functions themselves.

In Parallel states, the state completes after all the state machines inside it have completed.

The Activity feature of step functions allows us to ask for approval when a certain
step is reached.

Task state can be an activity or an AWS Lambda function.
The same activity can be used in multiple state machines.

Difference between AWS Step function state machines and AWS Step function activities:
- AWS Step Function state machines are made especially for AWS Lambda Workflows
- AWS Step function activities are made for flows where the step is represented by
a worker which could be hosted in ECS, EC2, mobile devices or elsewhere by calling the
step functions API

-- AppSync - managed service that uses GraphQL.

It has integrations with DynamoDB, Aurora, ElasticSearch and public HTTP endpoints.
Lambda can be used for custom processing to make GraphQL resolvers
It supports also Websockets and MQTT on websocket
It provides local data access & data synchronization for mobile apps. It's about to replace
Cognito sync.

There are 4 ways you can authorize applications to communicate with GraphQL:
- API_KEY
- AWS_IAM
- OPENID_CONNECT
- AMAZON_COGNITO_USER_POOLS

For custom domains and HTTPS we can integrate AppSync with cloudfront

AWS Lambda does not support in-place deployments. This is because after a function version is
published it cannot be updated.

Function versions cannot be modified after they have been pushed.
Function versoin numbers cannot be changed.
Aliases can be used to point to different function versions; however, the alias itself cannot be
overwriten (it is a point to a function version).

=======================================================================================================
Serverless Applications

Serverless applications contain both static and dynamic data

-- SAM / -- AWS SAM / -- Amazon SAM

https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-resources-and-properties.html

AWS SAM (Serverless application model) - it's based on Amazon Cloudformation and is optimized for
deploying serverless resources.
Properties for AWS Serverless Application Model:
- context - context object for a Lambda function
- handler - function handler for a Lambda function
- events - the events property allows you to assign a Lambda to an event source in Amazon API Gateway
- runtime - the language in which your AWS Lambda runs as

CLI:
sam packge is shorthand for aws cloudformation package

Sam templates always need to contain:
- Transform
- Resource

The event property in SAM is used to connect an AWS Lambda function to the Amazon API gateway
We can generate a lambda function with the following command:
sam local generate-event

After you deploy an AWS SAM template to AWS CloudFormation you cannot view the
original template. After the template is submitted and the AWS::Serverless transform
is executed, an AWS CloudFormation-supported template is generated.

We can create an Amazon API Gateway with SAM in the following ways:
1. By defining an AWS::ApiGateway::RestApi resource and any associated AWS::ApiGateway::Method
resources
2. One will be created automatically whenever AWS::Serverless::Function resources are declared with
one or more events.
3. By defining an AWS::Serverless:Api and providing an inline or external Swagger definition

-- API Gateway / -- AWS API Gateway / -- Amazon API Gateway

API Gateway supports the following HTTP methods:
GET, POST, PUT, PATCH, DELETE, OPTIONS, HEAD
It doesn't support trace

Amazon API gateway only supports HTTPS endpoints.
Without enabling CORS we're not going to be able to call the Amazon API Gateway service from another
domain. You use a stage to deploy your API, and a resource is typed object that is part of your API's
domain. Each resource may have an associated data model and relationships to other resourses and can
respond to different methods.

Integration types:
- mock - useful for testing
- HTTP / AWS Lambda - useful for mapping directly to an SQS queue
- AWS_PROXY (Lambda Proxy)
- HTTP_PROXY - proxy to ALB or EC2

If we want to use API keys for usage plans we need to associate them with a specific stage for this
to work.

AWS_IAM authentication is using SigV4

=======================================================================================================
Serverless Application Patterns

You can add a maximum of 20 nodes to an Amazon ElastiCache Redis cluster.

=======================================================================================================
Monitoring and Troubleshooting

-- AWS CloudTrail / -- CloudTrail

Provide governance, compliance and audit for your AWS Account
It's enabled by default

A trail can be applied to all regions (default) or a single Region.

If a resource is deleted in AWS, investigate CloudTrail first!

We have 3 types of events in CloudTrail:
- management events - operations performed on resources in your AWS account
	e.g. AttachRolePolicy - emitted when somebody attaches a policy
	CreateSubnet
	CreateTrail
	by default, trails are configured to log management events
- data events - by default not logged, huge volume of information
	e.g. GetObject, PutObject, DeleteObject from S3
- CloudTrail insight events
	traces unusual behaviour in your account - e.g. inaccurate resource provisioning,
	hitting service limits, bursts of AWS IAM actions

Events are stored for 90 days in CloudTrail
To keep them more than this period, store them in S3.

The following are considered management events by AWS CloudTrail:
- modifying an Amazon S3 bucket policy
- creating an Amazon RDS instance

CloudTrail tracks the origin identity of a request and the response sent.

We can access AWS CloudTrail logs using:
- AWS CloudTrail API
- AWS CLI
- AWS Management Console

AWS CloudTrail events contain the following information:
- what request is being made
- when the request was made
- which resource was acted on
- who made the request

CloudTrail stores event history for 90 days back. If you would like to store this information
permanently, you can create a CloudTrail trail, which stores the logs in Amazon S3.

Here's how you can configure notifications for each time the AWS root user logs in:
- Configure streaming of AWS CloudTrail logs to Amazon CloudWatch; configure a metric filter and
metric for console sign-in events; create an alarm for this metric; and configure an Amazon SNS
action for the alarm

The following are provided in an AWS CloudTrail event:
- the response of the request
- the AWS IAM user or role making the request

-- AWS CloudWatch / -- CloudWatch / -- Amazon CloudWatch

You can have up to 10 dimensions in a metric.

CloudWatch does not aggregate data across regions.

The following are available CloudWatch alarm actions:
- EC2 Auto Scaling actions
- AWS Lambda functions
- Amazon SNS topics

We can stream our custom logs from our applications to CloudWatch by using the cloudwatch
agent. It doesn't take any additional configurations besides installing, configuring and
starting the AWS CloudWatch agent.

In Amazon CloudWatch data points with a period of 300 seconds are stored for 63 days.
Logging to Amazon CloudWatch from a EC2 instance requires installing the CloudWatch Logs agent.

CloudWatch alarms support triggering actions in Amazon EC2, EC2 Auto Scaling, and Amazon SNS. We can
trigger AWS Lambda functions from an alarm, but only by first sending the alarm notification to an
Amazon SNS topic.

AWS CloudWatch EC2 detailed monitoring:
EC2 instance metrics have metrics "every 5 minutes" by default. With detailed monitoring you get
metrics every 1 minute for an additional cost.

The following EC2 metrics are available in CloudWatch:
- cpu utilization
- disk I/O
- network traffic - in/out
RAM cannot be tracked by CloudWatch because memory is allocated in a single block to an instance
and is managed by the guest OS, the underlying EC2 instance host doesn't have visibility
into consumption. It can be sent as a custom metric.

Higher metric resolutions cost more money. The highest resolution is 1 second
StorageResolution API parameter

CloudWatch metrics support dashboards
The LookupEvents API action can be used to query event data.

CloudWatch alarm states:
- OK
- INSUFFICIENT DATA
- ALARM

An Amazon CloudWatch alarm enters INSUFFICIENT_DATA state when:
- there's not enough data available for the metric to determine the alarm state
- the metric is not available
- the alarm was just created

We can stream cloudwatch logs to Elastic Search

By default no metrics are comming from EC2 instances. We need to install a CloudWatch
Logs agent for this purpose. We can set it up on-premise too.

CloudWatch Logs agent is the legacy agent. The Unified agent is the up-to-date one
and it can stream both logs and metrics. We can use SSM parameter store with the Unified one.

-- AWS CloudWatch metrics and Amazon EventBridge

Amazon EventBridge is the preferred way to manage your events. CloudWatch Events and EventBridge are
the same underlying service and API, but EventBridge provides more features. Changes you make in
either CloudWatch or EventBridge will appear in each console.

Default event bus: generated by AWS services (definved by CloudWatch events)
Partner event bus: - receive events from SaaS services - Zendesk, DataDog, Segment, Auth0 etc
We can create also our own event buses

Schema Registry - EventBus feature - generates code that can parse event data

Soon the CloudWatch Events name will be replaced with EventBridge

-- Amazon X-Ray / -- X-ray / -- AWS XRay / -- Amazon XRay

System for microservice tracing
Uses IAM for authorization and KMS for encryption at rest

X-Ray tracks only a statistically meaningful number of requests to a system

How to use X-Ray
- Your code must leverage the AWS SDK. It'll then capture HTTP, DB, SQS, AWS service calls
- Install the X-Ray daemon. Then X-Ray agent will work as a low level UDP packet interceptor.
AWS Lambda has X-Ray daemon preinstalled

If X-Ray is not working on EC2:
- Ensure the EC2 instance IAM role has the proper permissions
- Ensure the EC2 instance is running the X-Ray daemon

To Enable on AWS Lambda:
- ensure it has an IAM execution role with proper policy
(AWSX-RayWriteOnlyAccess)

X-Ray instrumentation - the measure of product's performance, diagnostics of errors, and to write
trace information.
We need the X-Ray SDK to do this.

X-Ray concepts:
- segments
- subsegments
- trace - segments collected together to form an end-to-end trace
- sampling - decrease the amount of requests sent to X-Ray to save cost
- annotations - key/value pairs used to index traces and use in filters
- metadata - key/value pairs not indexed and not used in search filters

The X-Ray daemon can be set to send cross acount traces. We need IAM tweaks for this to work

reservoir
rate

X-Ray daemon APIs:
- X-Ray Write APIs
PutTraceSegments - uploads segment documents to AWS X-Ray
PutTelemetryRecords - used by the AWS X-Ray daemon to upload telemetry
GetSamplingRules - retrieve all sampling rules
- X-Ray Read APIs
GetServiceGraph - get main graph
BatchGetTraces - retrieve a list of traces specified by id
GetTraceSummaries
GetTraceGraph

There are three options to run X-Ray with ECS:
- run a separate container in each EC2 instance within the cluster as a Daemon container (EC2)
- X-Ray daemon running in a separate container within a task ("side-car") (EC2/Fargate)

-- Amazon Systems Manager

With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon EKS clusters,
Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and
troubleshooting, implement pre-approved change work flows, and audit operational changes
for your groups of resources.

-- AWS IoT Core service

When you build a solution with IoT Core that sends statuses to IoT devices and want to handle the times
when those devices don't have internet connectivity, you might want to post status updates to the
Device Shadow service.

The AWS IoT Device Shadow service adds shadows to AWS IoT thing objects. Shadows can make a device's
state available to apps and other services whether the device is connected to AWS IoT or not. AWS IoT
thing objects can have multiple named shadows so that your IoT solution has more options for connecting
your devices to other apps and services.

=======================================================================================================
Optimization

Multi availability zone is used for fault tolerance.
In case of slow reads from AWS RDS we can create a read replica and routing calls to it.

Just stopping an EC2 instance might not be the best practice for cost optimization. You may be
charged for storage. Terminating it would prevent from being charged for storage.

In AWS autoscaling group the instance user data is the space where you can place optional
scripts that can run after bootstrapping the application. This is used to perform.

By launching instances in multiple Availability Zones in the same region, you help protect your
applications from a single point of failure.

Provisioned IOPS volumes in the Amazon EBS are designed to deliver the expected performance only
when they are attached to an Amazon EBS optimized instance.
For maximum consistency, a Provisioned IOPS volume must maintain an average queue depth
(rounded to the nearest whole number) of one for every 1,000 Provisioned IOPS in a minute

In AWS Cloudfront, some headers such as Date or User-Agent, significantly reduce the cache hit ration
(the proportion of requests that are served from a CloudFront edge cache).

In AWS S3, a multipart upload is supported and we can use it to upload big objects to S3.

One of the recommended best practices for AWS Lambda is to avoid using recursive code.

If you use SSE-KMS in S3 you may be impacted by the KMS limits.

S3 multipart upload is recommended for files >100MB.
It must be used for files >5GB.

S3 Transfer acceleration - increase transfer speed by transferring file to an AWS edge location
which will forward the data to the S3 bucket in the target region.
It's compatible with multipart upload

S3 byte-range fetches - parallelize GETs by requesting specific byte ranges

=======================================================================================================
Requirements:
https://aws.amazon.com/certification/certified-developer-associate/

Resources:
https://github.com/mostafac0des/certified-aws-developer-associate-notes
https://aws.amazon.com/blogs/aws/new-elastic-network-interfaces-in-the-virtual-private-cloud/
https://instances.vantage.sh/
https://aws.amazon.com/ec2/instance-types/
https://aws.amazon.com/caching/best-practices/
https://aws.amazon.com/builders-library/caching-challenges-and-strategies/

QWikLabs:
https://amazon.qwiklabs.com/catalog?keywords=introduction%20to&ransack=true&per_page=50

QWikLabs AWS developer associate labs:
https://www.qwiklabs.com/quests/20

AWS Policy generator:
https://awspolicygen.s3.amazonaws.com/policygen.html
https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html
https://aws.amazon.com/cloudfront/features/

AWS ECS hands-on lab:
https://ecsworkshop.com/introduction/

https://github.com/lukpep/aws-sam-workshop

Introduction to DevOps in AWS:
https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/welcome.html

Notes:
https://github.com/itsmostafa/certified-aws-developer-associate-notes
https://github.com/KarlCF/aws-developer-associate-notes
https://github.com/shrinivas93/AWS-Certified-Developer-Associate-Notes
https://github.com/maludecks/aws-developer-associate-notes
https://gist.github.com/dboyd13/cff5169f25b4f35e1816edd85943dec5
https://github.com/minhntm/aws-developer-associate-certificate
https://github.com/antoniolofiego/CloudCertNotes/blob/master/AWSCertifiedDeveloperAssociate.md

topic - it's like a table in a database (without all the constraints)
	topic replication factor - this way if a broker is down another broker can serve the data
data stream - the sequence of messages in a topic is called a data stream
partition - topics are split into partitions. Messages within each partition are going to be ordered.
	Kafka topics are immutable. Once data is written to a partition, it cannot be changed
	Data is kept for a limited time.
	A Kafka partitioner is a code logic that takes a record and determines to which partition to send it to.
	Key hashing is the process of determining the mapping of a key to a partition
	Partition rebalance - moving partitions between consumers
	Reassignment of partitions happen when a consumer leaves or joins a group
	it also happens if an administrator adds new partitions into a topic
	Cooperative rebalance (incremental rebalance) - reassigning a small subset of the partitions from one consumer to another. Other
		consumers that don't have reassigned partitions can still process uninterrupted. Can go through several iterations to find a
		"stable" assignment (hence "incremental")
producers - producers can choose to send a key with the message (string, number, binary, etc...)
	if the key is not set data is sent round robin (partition 0, then 1, then 2...)
	if the key is set then all messages for that key will always go to the same partition (hashing)
	A key is typically set if you need message ordering for a specific field
	Producer acknowledgements - producers can choose to receive acknowledgement of data writes
	acks=0 - producer won't wait for acknowledgement (possible data loss)
	acks=1 - producer will wait for leader acknowledgement (limited data loss)
	acks=all / acks=-1 - Leader + replicas acknowledgement (no data loss)
offset - index of a message within a partition
	Consumer offsets can be reset
	No consumer should be running in order to reset
consumer - consumers automatically know which broker to read from
	consumers are grouped in consumer groups
	if you have more consumers than partitions, some consumers will be inactive
	In kafka, it is possible to have multiple consumer groups on the same topic
	To create distinct consumer groups, use the consumer property group.id
	Kafka stores the offsets at which a consumer group has been reading in a topic named __consumer_offsets
	When a consumer in a group has processed data received from Kafka, it should be periodically committing the
	offsets (the Kafka broker will write to __consumer_offsets).
	If a consumer dies, it will be able to read back from where it left off thanks to the committed consumer offsets
	Since Kafka 2.4, it is possible to configure consumers to read from the closest replica
	This may help improve latency, and also decrease network costs if using the cloud

	partition.assignment.strategy:
	- RangeAssignor
	- RoundRobin
	- StickyAssignor
	- CooperativeStickyAssignor
	- default: RangeAssignor, CooperativeStickyAssignor

	static group membership - if we specify a group.instance.id it makes the consumer a static member (it gets assigned
		the same partition after disappearing and reappearing)
brokers - A Kafka cluster is composed of multiple brokers (servers). Each broker has its own id.
	Each broker contains certain topic partitions
	After connecting to any broker (called a bootstrap broker), you will be connected to the entire cluster (Kafka
	clients have smart mechanics for that)
	Every kafka broker is also called a bootstrap server
	At any time only one broker can be a leader for a given partition
	Producers can only send data to the broker that is the leader for a partition

Kafka messages anatomy:
- key - binary (can be null - zero or more bytes)
- value - binary (can be null - zero or more bytes)
- compression type - none, gzip, snappy, lz4, zstd
- headers (optional) - they contain keys and values
- partition + offset
- timestamp - set by the system or by the user

Message serialization means transforming objects / data into bytes. We can serialize both keys and values
Common serializers:
- String (incl. JSON)
- int, float
- Avro
- Protobuf

Message deserialization means transforming bytes into objects / data. We can deserialize both keys and values

Zookeeper:
- manages brokers (keeps a list of them)
- helps in performing leader election for partitions
- sends notifications to Kafka in case of changes (e.g. new topic, broker dies, broker comes up, delete topics, etc ...)
- Kafka 2.x cannot work without Zookeeper
- Kafka 3.x can work without Zookeper (KIP-500) - using Kafka Raft instead
- Kafka 4.x will not have Zookeeper
- by design operates with an odd number of servers (1, 3, 5...)
- zookeeper has a leader (writes) the rest of the servers are followers (reads)
- (Zookeeper does not store consumer offsets with Kafka >v0.1)
